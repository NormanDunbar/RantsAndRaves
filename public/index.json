[{"content":"In a previous post I mentioned that the Arduino IDE needed to be setup in order to allow the Sketch-\u0026gt;Optimize for debugging menu option to have any effect. The IDE \u0026ldquo;knows\u0026rdquo; that ATmega328P microcontrollers, as used on the Uno R3, Duemilanove, Nano and so on, do not have actual hardware debugging, so the option has no effect at all.\nHowever, with a little sneaky configuration, we can change things so that we can optimise our code for debugging with avr-gdb and simulAVR as described here.\nClose the IDE and navigate to $HOME/.arduino15/packages/arduino/hardware/avr/1.8.6 and open the file named platform.local.txt in your favourite text editor. If there is no file of that name there, create the file\u0026mdash;the name is all lower case\u0026mdash;and open it.\nAdd the text in the Listing below to the file. Comments are prefixed by the \u0026lsquo;#\u0026rsquo; character and if you wish, just ignore them. All non-comment lines are a single line each\u0026mdash;they have wrapped around here to fit the page.\n## See https://github.com/arduino/ArduinoCore-avr/issues/554 # Fallback property definition for compatibility with # development tools that don\u0026#39;t have the \u0026#34;Optimize for # Debugging\u0026#34; control. compiler.optimization_flags=-Os -g # See: https://arduino.github.io/arduino-cli/latest/platform- # specification/#optimization-level-for-debugging. compiler.optimization_flags.release=-Os -g compiler.optimization_flags.debug=-O0 -g compiler.c.flags=-c {compiler.optimization_flags} {compiler.warning_flags} -std=gnu11 -ffunction-sections -fdata-sections -MMD -flto -fno-fat-lto-objects compiler.c.elf.flags={compiler.optimization_flags} {compiler.warning_flags} -flto -fuse-linker-plugin -Wl,--gc-sections compiler.cpp.flags=-c {compiler.warning_flags} {compiler.optimization_flags} -std=gnu++11 -fpermissive -fno-exceptions -ffunction-sections -fdata-sections -fno-threadsafe-statics -Wno-error=narrowing -MMD -flto NOTE: By creating (or using) platform.local.txt you will avoid having to redo all this configuration when you next update the IDE to a newer version. Had we simply added the code to platform.txt our changes would have been overwritten on every update.\nSave the platform.local.txt file and open the IDE again; open a sketch you have compiled previously; click Sketch-\u0026gt;Optimize for Debugging; then compile the sketch in verbose mode. You should see that the compiler and linker options now display something similar to the following:\nCompiling sketch... .../avr-gcc/7.3.0-atmel3.6.1-arduino7/bin/avr-g++ -c -O0 -g ... ... Linking everything together... .../avr-gcc/7.3.0-atmel3.6.1-arduino7/bin/avr-gcc -O0 -g ... NOTE: You can ignore the warning messages that advise about #warning \u0026quot;Compiler optimizations disabled; functions from \u0026lt;util/delay.h\u0026gt; won't work as designed as you will not be uploading the debug version of the sketch to your board.\nFrom now on, always untick the Optimize for Debugging option in the IDE before compiling and uploading to your board. The option is only required for debugging in the simulator.\nDebugging creates much larger output files and it\u0026rsquo;s possible that a sketch you have previously uploaded quite happily will no longer upload if you forget about this option. One of my sketches compiles to 10,570 bytes with debugging enabled but only 2,410 bytes without debugging.\n","description":"Configuring the Arduino IDE to Allow AVR Debugging","id":0,"section":"posts","tags":null,"title":"Configuring the Arduino IDE to Allow AVR Debugging","uri":"http://localhost:1313/RantsAndRaves/posts/2024/04/configure-arduino-ide-to-allow-avr-debugging/"},{"content":"When writing AVR code, it can be difficult to debug problems, especially when adding Serial.printf() calls means that the bug vanishes! Actually, only Serial.begin() is required to make the bugs vanish.\nWARNING: The simulator hangs if you let the code run and there\u0026rsquo;s a delay() call in the code. This may be an artefact of running under gdb \u0026ndash; I have yet to test what happens if I just run the code. Might be worth bearing this in mind.\nGDB Tutorial See https://www.gdbtutorial.com/gdb_commands for a useful gdb tutorial\nConfiguration and Execution In order to use the option Sketch-\u0026gt;Optimize for Debugging in the Arduino IDE, you need to do a little configuration of the IDE to allow that option to have any effect on Arduino boards using the ATmega328P microcontroller. See this post from the future for details. Once the IDE is correctly configured, you may proceed.\nIn the Arduino IDE, Sketch-\u0026gt;Optimize for Debugging. Then rebuild the sketch with verbose compilation enabled in preferences. In a bash session cd /tmp/arduino/sketches/LONG_HEX_NUMBER which you can see in the compilation output from the previous step. simulavr -g -d atmega328 -f SKETCH.ino.elf to start the simulator. You do not need the Arduino board to be running, we are simulation it. Your terminal will hang, so open another [tab] and run: 1 2 3 4 5 cd /tmp/arduino/sketches/LONG_HEX_NUMBER avr-gdb file SKETCH.ino.elf target remote localhost:1212 load You are now running the sketch in the simulator, and using gdb to set breakpoints etc in the normal manner.\nUseful gdb commands See also https://www.gdbtutorial.com/gdb_commands\nb location - sets a breakpoint at location. ir - displays all 32 registers. ir r18 r25 - displays registers r18 and r25 only. p variable - displays the value of a variable. p (short)variable - displays the value of a variable as a short data type. continue, cont or c - resumes execution after a breakpoint stop. step - steps into the next executable instruction. next - same as step but will treat a function call as a single instruction and will stop when the function returns, unlike step. x/FMT ADDRESS - displays a memory dump of ADDRESS in the specified FMT FMT Specifiers The FMT specifier for the x command is in three parts:\nAn optional counter to specify how many \u0026ldquo;data types\u0026rdquo; will be displayed. The format of the output: o = octal d = decimal x = hexadecimal u = unsigned int s = string t = binary The data type: b = bytes h = half words (16 bit) w = words (32 bit) g = double word (64 bits). ","description":"How do I use Simulavr and gdb to debug AVR code?","id":1,"section":"posts","tags":null,"title":"Debugging AVR Code with Simulavr","uri":"http://localhost:1313/RantsAndRaves/posts/2023/10/debugging-avr-code-with-simulavr/"},{"content":"When writing AVR assembly code, there are protocols that must be followed when using and abusing the various registers available in the ATmega328P device, which is what I\u0026rsquo;m using, built in to many Arduino boards.\nThe assembly language written can be compiled in the Arduino IDE using the GCC/G++ compiler controller and this, in turn, will call the avr-as assembler to do the actual work. All you need to do is add a new file to your Arduino sketch, and name it *.S where the name can be anything, but the extension must be an upper case \u0026lsquo;S\u0026rsquo;.\nSpecial Registers R0: Temporary Register R0 is the temporary register used by the C++ compiler generated code. To this end, it must be preserved by the assembly code if it is used there. Obviously, it should also be restored before calling or returning to C++.\nR1: Zero Register R1 is the zero register. The code generated by the C++ compiler expects the register toalways hold the value zero. If your assembly code uses this register for any reason, it must be cleared back to zero before calling or returning to C++.\nCall Saved Registers R2-R17 plus R28 and R29 are the Call Saved Registers. What this means is that these must be saved if they are used in your assembly code, and restored before calling or returning to C++.\nCall Clobbered Registers R18-R27 plus R30 and R31 are the Call Used/Clobbered Registers. What this means is that these can be used freely in your assembly code, however, they are clobbered by the C++ compiler\u0026rsquo;s generated code, so if you call a C++ function, you must preserve them if you have values in them which you will use on return from the C++ call.\nSummary Register Category Special Considerations R0 Temporary Register Must be preserved if used by assembly code, and restored before calling or returning to C++ code. R1 Zero Register Must be cleared to zero before calling or returning to C++. R2 to R17 Call Saved Registers If used in assembly code, must have their values preserved before use, and restored before calling or returning to C++. R18 to R27 Call Clobbered Registers If used in assembly code, must have their values preserved before calling C++ functions, and restored on return. R28 and R29 Call Saved Registers If used in assembly code, must have their values preserved before use, and restored before calling or returning to C++. R30 and R31 Call Clobbered Registers If used in assembly code, must have their values preserved before calling C++ functions, and restored on return. ","description":"Which registers do I need to preserve when calling C++ code from assembly, and vice versa?","id":2,"section":"posts","tags":null,"title":"AVR-GCC Register Usage","uri":"http://localhost:1313/RantsAndRaves/posts/2023/10/avr-gcc-register-usage/"},{"content":"My Linux Mint installation on my laptop has a wee problem with the extreme resolution of the laptop display. This is 3072 by 1920 (16:10) and when an application is opened on the laptop screen, it is tiny. I need to fix it, so this is what I do when I install a new release.\nDisplay Properties LM -\u0026gt; Preferences -\u0026gt; Display. Settings tab: Make sure \u0026ldquo;Enable fractional scaling controls\u0026rdquo; is off. Layout Tab: Make sure \u0026ldquo;User Interface Scale\u0026rdquo; is 100%. Repeat for all monitors. I have two.\nFonts LM -\u0026gt; Preferences -\u0026gt; System Settings Font Selection: Choose your preferred font and sizes. Mine are the default, Ubuntu Regular 10 point for most of the options. Font Settings: Set \u0026ldquo;Text scaling factor\u0026rdquo; to 1.3, 1.4 or 1.5 as you prefer. Mine is 1.4. Desktop Panel Next, the desktop panel (the task bar if you want the Windows terminology!) might need adjusting. My primary monitor is the UHD laptop, so the panel is a wee bit thin for my eyes these days!\nRight-click the panel. Choose \u0026ldquo;Panel Settings\u0026rdquo; Click \u0026ldquo;Left Zone\u0026rdquo; Adjust the panel height, mine is 44 pixies tall. Set the \u0026ldquo;Font size\u0026rdquo; to \u0026ldquo;Allow theme to determine font size\u0026rdquo;. Set \u0026ldquo;Coloured icon size\u0026rdquo; to \u0026ldquo;Scale to panel size optimally\u0026rdquo;, or, \u0026ldquo;Scale to panel size exactly\u0026rdquo; if you prefer. Click \u0026ldquo;Right Zone\u0026rdquo; Set the \u0026ldquo;Font size\u0026rdquo; to \u0026ldquo;Allow theme to determine font size\u0026rdquo;. Set \u0026ldquo;Coloured icon -\u0026gt; Preferences -\u0026gt; System Settings size\u0026rdquo; to Scale to panel size optimally, or, Scale to panel size exactly if you prefer. Adjust \u0026ldquo;Symbolic icon size\u0026rdquo; to 32 pixies or to match any coloured icons you have on the right side. I have HPLIP over on the right so that sizes automagically, but the symbolic icons (monocolour) look tiny next to it. I don\u0026rsquo;t have anything to worry about in the \u0026ldquo;Centre Zone\u0026rdquo; but if I ever do, I can adjust accordingly.\nMouse The mouse pointer looks ridiculously tiny on the UHD monitor, so:\nLM -\u0026gt; Preferences -\u0026gt; Mouse and Touchpad Adjust the pointer size on the UHD monitor until it looks a bit more visible. I ended up at a position 65% of the way along the slider. Reboot, Now! Reboot the laptop even though it appears to have all been changed correctly, you get \u0026ldquo;funnies\u0026rdquo; if you don\u0026rsquo;t do a reboot.\nSecond Monitor As my second monitor is only a 1920 by 1080 (16:9 HD) one, the fonts appear to be quite large when an application is moved onto that monitor. Nothing too onerous, but if the standard settings don\u0026rsquo;t look right, we need to go back to scaling.\nThis saves mucking about with the scaling, however, if the two monitors were too far apart in terms of size, it might be a better plan to:\nSet the font scaling back to 1.0. Turn on the frational scaling again. Set each monitor\u0026rsquo;s scaling individually. I did try this, but wasn\u0026rsquo;t too happy with 125% for the laptop and 100% for the HD monitor. Your mileage, as they say. may vary.\n","description":"How to setup the display and fonts for best results on a UHD laptop screen.","id":3,"section":"posts","tags":null,"title":"Configuring Linux Mint to use my UHD Laptop Display","uri":"http://localhost:1313/RantsAndRaves/posts/2023/10/configure-fonts-desktop-linux-mint/"},{"content":"As you might know, if you have read my two previous posts, I have a Raspberry Pi set up as a Music player using Volumio and while this is excellent when I\u0026rsquo;m sat at my laptop \u0026ndash; probably writing another Arduino book! \u0026ndash; I can\u0026rsquo;t play my music from my tablet or phone while elsewhere in the house. I still want to install MStream so I\u0026rsquo;m no setting up a second Raspberry Pi to install MStream on, ad hopefully, get it to read all the music from the Volumio server.\nI have already installed Bubble UPNP, from the Google Play Store, onto my phone and tablet. This app comes highly recommended. Unfortunately, in the free version, playlists are limited to 16 tracks only, and many of my albums have more tracks that this. So, unless I upgrade and pay (!) or mess around with playlists consisting of the first 16 tracks then the remainder, I\u0026rsquo;m not sure this is the best option for me, especially when the MStream Android app is so good \u0026ndash; I\u0026rsquo;ve been running it connected to their own demo server, and it\u0026rsquo;s brilliant!\nUseful SAMBA Links I found the information on the following links to be extremely helpful in setting up my SAMBA shares.\nConfiguring Samba Mounting SAMBA shares at boot time Configuring the Volumio Server The first task is to get onto the Volumio server to set up a new, read only, share that other devices can mount and see all the USB attached thumb drives I store my music on. In theory, this should also allow them to see any new drives that I add.\nTo configure SAMBA, I need to be root, but I\u0026rsquo;m logged in as volumio, no worries:\n1 2 sudo -i cs /etc/samba Now, create a backup before editing the configuration file, smb.conf:\n1 2 cp smb.conf smb.conf.backup.ND nano smb.conf In the editor, we can see that there are three SAMBA mounts defined:\nInternal Storage on /data/internal, we are not interested in this one. NAS on /mnt/NAS, again, we are not interested. USB on /mnt/USB, and this one, we are most definitely interested in! These are configured as read-write, but I don\u0026rsquo;t want to allow everyone and their dog to write all over my music collection, so I\u0026rsquo;ll be setting up a new share which will be read only. At the bottom of the file, add the following text:\n1 2 3 4 5 6 7 8 [MSTREAM] # This allows anonymous (guest) access # without authentication. # Only USB mounts are accessible, read only access. path = /mnt/USB read only = yes guest ok = yes guest only = yes After saving the file with CTRL-X, Y, ENTER we need to restart the SAMBA daemon, smbd:\n1 systemctl restart smbd Testing the New SAMBA Share On a different device \u0026ndash; laptop or Raspberry Pi \u0026ndash; I can test if the new share is available as follows.\nFirst, we need our own user\u0026rsquo;s user and group ids:\n1 2 3 id uid=1000(norman) gid=1000(norman) groups=..... You will need to substitute your own user and group ids as necessary. Now we can run the test:\n1 2 3 sudo -i ## If necessary, we need to be root. mkdir -p /mnt/mstream mount -t cifs -o guest -o uid=1000 -o gid=1000 //192.168.1.41/MSTREAM /mnt/mstream Assuming that the mount command, above, comes back with no errors, then we should be able to see my mounted USB devices on the remote Volumio server:\n1 2 3 ls mnt/mstream 0959-0DF7 30E5-1614 And, check within one of them:\n1 2 3 4 5 6 7 ls mnt/mstream/0959-0DF7 \u0026#39;Alice Cooper\u0026#39; Journey \u0026#39;Amy MacDonald\u0026#39; \u0026#39;Kate Bush\u0026#39; \u0026#39;Anderson Bruford Wakeman Howe\u0026#39; \u0026#39;Katie Melua\u0026#39; \u0026#39;Andrea Bocelli\u0026#39; \u0026#39;Marillion\u0026#39; ... ... So, it all seems to be working. Unmount the test and, if necessary, remove the mount point if it is no longer needed. In my case, it will hopefully be used when I install MStream, so I\u0026rsquo;m leaving it, for now at least. We should still be logged into root through the sudo -i command earlier.\n1 2 sudo umount /mnt/mstream sudo rmdir /mnt/mstream ## If mount is test only and no longer required. Configure the Client Device We would like the client device, where MStream is to be installed, to mount the SAMBA share at boot time. To do this we need to edit /etc/fstab on the client machine:\n1 sudo vi /etc/fstab Add a single line of text to the file, at the end is probably best:\n1 //192.168.1.41/MSTREAM /mnt/mstream cifs uid=1000,gid=1000,guest 0 0 We can test this with:\n1 2 3 4 sudo mount -a ls mnt/mstream 0959-0DF7 30E5-1614 ","description":"How to mount SAMBA shares from the Volumio server, on other devices.","id":4,"section":"posts","tags":null,"title":"Messing About with Samba Shares on my Volumio Server","uri":"http://localhost:1313/RantsAndRaves/posts/2023/04/messing-about-with-volumio-samba/"},{"content":"I have a Raspberry Pi set up as a Music player using Volumio and while this is excellent when I\u0026rsquo;m sat at my laptop \u0026ndash; probably writing another Arduino book! \u0026ndash; I can\u0026rsquo;t play my music from my tablet ot phone while elsewhere in the house. My original plan was to install MStream but thanks to the way that Volumio takes over the Pi, I was unable to install the MStream dependencies, nodejs and npm to that was a bummer straight away. However, this turned out to be quite simple to fix, although I had to kisss MStream goodbye at this point.\nI first installed Bubble UPNP, from the Google Play Store, onto my phone and tablet. This app comes highly recommended. There is a free \u0026ndash; with ads \u0026ndash; version and a Â£3.99 pay for version. Being Scottish and living in Yorkshire, I have two reputations for meanness to live down to, so I went for the free version. This is especially handy as I\u0026rsquo;m using Pi Hole to block ads anyway.\nAfter installing BubBle UPNP, I opened it, and opened the Local and Cloud option and set up a pair of servers, Volumio One and Volumio Two \u0026ndash; meaningful names eh? These were configured as SMB (Samba) server types and mapped to the two large USB thumb drives I\u0026rsquo;m using on the Raspberry Pi to hold my music.\nNow I can stream music from my Volumio server to my devices. Easy!\nI could have just set up a single Volumio server and left the optional folder blank, but then I\u0026rsquo;d have had to navigate to USB then to one or other of the devices \u0026ndash; and they have very unmeaningful names \u0026ndash; so I preferred to do it as described above. I simply open one or other server and pick a folder to play. Job done.\n","description":"How to stream music from my Volumio server to my other devices.","id":5,"section":"posts","tags":null,"title":"Streaming Music from my Volumio Server","uri":"http://localhost:1313/RantsAndRaves/posts/2023/04/streaming-from-volumio/"},{"content":"The Raspberry Pi is a great little device. I use one to play my music as my CD player died a death a while back \u0026ndash; the CDs kept slipping \u0026ndash; and spares are very hard to come by as my \u0026ldquo;stack\u0026rdquo; is over 30 years old, but still sounds great. I have ripped my CDs, or most of them, to FLAC format, and have some of them stored on a couple of larger sized USB thumb drives attached to a Raspberry Pi. The Pi has a HiFiBerry DAC Plus attached \u0026ndash; which I was lucky enough to win in a MagPi Magazine giveaway \u0026ndash; so the sound is excellent played through my Sony \u0026ldquo;stack\u0026rdquo;.\nI recently had to update Volumio to version 3, and while I\u0026rsquo;m still happy with it, I do now have a couple of foibles, and bugs which seemingly were fixed still appear to be not fixed. I couldn\u0026rsquo;t change the theme, for example, until I logged onto the Raspberry Pi and executed the command touch /data/manifestUI.\nAs it turned out, changing the theme was a nightmare as there is (now) a transparency applied so that background shows through the text. This is garbage! Especially on the \u0026ldquo;classic\u0026rdquo; theme, the one I actually like, but there no longer seems to be an option to select a better background, and to lose the transparency.\nAnyway, I also wanted to stream my music via MStream, so I needed to install and enable SSH, so that I could login and install software remotely. This turned out to be interesting as the Pi uses a different name for the SSH daemon, it\u0026rsquo;s ssh rather than sshd which is on every other Linux installation that I know of! Also, because of how the Volumio installation takes over the Pi, I was unable to install nodejs and npm which are requirements of MStream. Bummer! I still needed remote access to the Volumio server though.\nTo install the SSH server, if not already installed:\n1 2 3 sudo apt update sudo apt upgrade sudo apt install openssh-server If it needs to be installed, it will be, otherwise you will be told that it\u0026rsquo;s already there. Handy.\nOnce installed, you need to start it:\n1 sudo systemctl start ssh ### Beware, \u0026#39;SSH\u0026#39; not \u0026#39;SSHD\u0026#39;! Test it by logging into your Pi from a different system. If its ok, then enable it to automatically start at boot time:\n1 sudo systemctl enable ssh Job done. After a reboot \u0026ndash; to test the automatic starting of the server \u0026ndash; you should be able to login.\nIf necessary, generate a new key file with:\n1 ssh-keygen -t ecdsa -b 521 This will generate a 521 bit ECDSA key pair. These days, the DSA key type is best ignored and RSA is probably going to be broken at some point soon.\nThe key files, for there are two, generated will be in your $HOME/.ssh directory:\n1 2 3 4 5 6 ls ~/.ssh/id* ... /home/norman/.ssh/id_ecdsa /home/norman/.ssh/id_ecdsa.pub ... The new key file can be copied to the server now:\n1 ssh-copy-id -i ~/.ssh/id_ecdsa volumio@wvolumio Obviously, substituting your key file name and logon details, you will be prompted for a password at this stage. Once copied over, test that it worked:\n1 ssh volumio@wvolumio And this time, if all was well, you will get logged straight in without needing a password.\n","description":"How to Enable SSH on a Raspberry Pi from the Commandline.","id":6,"section":"posts","tags":null,"title":"Enabling SSH on a Raspberry Pi from the Commandline","uri":"http://localhost:1313/RantsAndRaves/posts/2023/04/enabling-ssh-on-a-raspberry-pi-from-the-commandline/"},{"content":"While writing my third Arduino book, Arduino Assembly Language, I needed to determine if interrupts can be used when mixing C++ and Assembly using the Arduino IDE. It turns out that it can be done.\nThe Arduino IDE uses avr-g++ to compile the C++ sources (the *.ino files) and avr-gcc with the -assembler-with-cpp option to run avr-gas on any Assembly Language sources.\nAll that is required to mix interrupt handlers written in Assembly Language, with code written in C++, is:\nUse the C++ interrupt handler name for the one written in the Assembly source file. For example INT0_vect is the handler for the INT0 external interrupt; Mark the interrupt handler as .global. That\u0026rsquo;s it!\nExample Sketch Here\u0026rsquo;s a very simple example where the INT0 interrupt is used to toggle an LED \u0026ndash; what else? It uses a switch on pin D2 which is pulled up either internally, externally with a 10K resistor or externally with the MC14490P Hex debouncer. When the switch is pressed D2 goes low and the interrupt fires as it is configured as a falling interrupt trigger.\nThe obligatory LED on pin D13, the built in LED for an UNO board, does its usual thing by blinking every second \u0026ndash; also under control of an Assembly routine.\nBe aware that the switch needs to be debounced somehow, as otherwise, the LED will not always follow the switch presses. I use an MC14490P hex debounce chip, but two 10K resistors and a 100nF ceramic capacitor will also work well.\nSketch: INT0.ino 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 //============================================================= // Demonstrates using interrupt handlers with avr-gas as the // assembler. In this example, INT0 interrupts toggle an LED // attached to D12 while the LED on D13 blinks every second. //============================================================= //------------------------------------------------------------- // Define the setup() and toggle() functions to be external // and written in Assembler. //------------------------------------------------------------- extern \u0026#34;C\u0026#34; { void setup(); void toggle(); } //------------------------------------------------------------- // I\u0026#39;m only using loop() to toggle the D13 LED and to delay. // It\u0026#39;s easier than writing my own delay() function in // Assembler -- for now anyway. // // Actually, we don\u0026#39;t even need this, but it shows how the // interrupt handler can even interrupt a delay() call. Try // that without using interrupts! //------------------------------------------------------------- void loop() { toggle(); delay(1000); } Nothing much to see here, we declare the setup() and toggle() functions to use the C calling convention and that they are defined elsewhere. In the loop() function, we simply toggle the LED on pin D13 every second. Standard \u0026ldquo;Blink\u0026rdquo; behaviour.\nSketch: INT0.S This file must have an extension of an uppercase \u0026lsquo;S\u0026rsquo;. If not, it won\u0026rsquo;t be compiled by the avr-gcc compiler and avr-gas, the assembler, without errors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 ;-------------------------------------------------------------- ; We must do this to __SFR_OFFSET or the various I/O registers ; use their data address and not their I/O address. It\u0026#39;s a C++ ; thing that we have to undo for Assembly. ;-------------------------------------------------------------- #define __SFR_OFFSET 0 ;-------------------------------------------------------------- ; Get the register and bit names for the board in use. ;-------------------------------------------------------------- #include \u0026lt;avr/io.h\u0026gt; ;-------------------------------------------------------------- ; Make sure the following code goes into the code, aka text, ; section. ;-------------------------------------------------------------- .section .text ;-------------------------------------------------------------- ; Our setup() and toggle() functions are global so that they ; can be called from C/C++. ;-------------------------------------------------------------- .global setup .global toggle ;-------------------------------------------------------------- ; The interrupt handler must have the same name as in C/C++ and ; must also be global -- even though it is called by the CPU ; and not explicitly by the C++ code. ;-------------------------------------------------------------- .global INT0_vect ;-------------------------------------------------------------- ; In setup we set D12 and D13 as OUTPUT pins for our LEDs. D2, ; where the switch is, is defined as INPUT with an optional ; PULLUP if external pullups are not used. ;-------------------------------------------------------------- setup: sbi DDRB,DDB5 ; D13 is output sbi DDRB,DDB4 ; D12 is output cbi DDRD,DDD2 ; D2 is input for INT0. ; sbi PORTD,PORTD2 ; Internal pullup for D2, if required. ;-------------------------------------------------------------- ; Configure the INT0 interrupt as FALLING. ;-------------------------------------------------------------- ldi r16,(1\u0026lt;\u0026lt;ISC01)|(1\u0026lt;\u0026lt;ISC00) ; INT0 FALLING sts EICRA,r16 ;-------------------------------------------------------------- ; Enable INT0 and clear any pending INT0 flag notifications to ; avoid spurious triggering at power up. We want the LED off to ; begin with. ;-------------------------------------------------------------- sbi EIMSK,INT0 ; Enable INT0 sbi EIFR,INTF0 ; Clear INT0 flag ret ;-------------------------------------------------------------- ; Toggle the D13 LED. Writing a 1 to a bit in the PIN register ; for an output pin will toggle the pin -- useful! ; ; D13 is PINB bit 5. ;-------------------------------------------------------------- toggle: sbi PINB,PINB5 ret ;-------------------------------------------------------------- ; The interrupt handler for the INT0 interrupt. All we do is ; toggle the D12 bit in the PIN register. Note the use of the ; RETI instruction to return from an interrupt handler. Also ; using a handler will automatically clear the INT0 flag so we ; don\u0026#39;t have to. ; ; D12 is PINB bit 4. ;-------------------------------------------------------------- ; Toggle D12 on INT0 interrupt (on D2). INT0_vect: sbi PINB,PINB4 reti All that this code does is initialise the two output pins, D12 and D13; as well as the input pin, D2. D2 defaults to INPUT as I\u0026rsquo;m using external circuitry to pull the switch high. If your circuit doesn\u0026rsquo;t, uncomment the line 42 to enable internal pullups on D2.\nThe INT0 interrupt is configured to trigger when pin D2 falls from high to low, it is then enabled and any pending interrupts are cleared. An interrupt will not fire until one instruction has been executed after enabling it, or after returning from an ISR, so the flag is cleared before the pending INT0 gets a chance to trigger.\nThe toggle() function is called from loop() to toggle the LED on pin D13. It does this by writing to the PIN register for the pin \u0026ndash; the Atmel engineers decided to make use of the input registers, PINx, which would otherwise be unused for output pins.\nThe interrupt handler does a similar toggle for the LED on pin D12.\nDebouncing the Switch I used an MC14490P hex debouncer as shown below. It can debounce up to 6 separate switches in pullup or pulldown modes. The switch is pulled up by the MC1440P in this circuit.\nIf, on the other hand, you don\u0026rsquo;t have a MC14490P handy, a couple of 10K resistors and a 100nF capacitor will do the job, as per the image below. The switch is configured in pullup mode in this circuit.\nOf course, if you want to live dangerously, you could remove R3 and C1 from the breadboard layout above, and reposition the orange wire down four holes until it connects to the top of R4. This way, you get a normal pulled up switch, but with no debouncing. Try it and see how well it all works!\n","description":"How to use AVR assembly language, avr-gas, and interrupts.","id":7,"section":"posts","tags":null,"title":"Using Avr Assembly With Interrupts","uri":"http://localhost:1313/RantsAndRaves/posts/2023/03/using-avr-assembly-with-interrupts/"},{"content":"My new laptop has a 4K monitor and my tool of choice for writing my Arduino books, Lyx, always displayed the toolbar icons as \u0026ldquo;so very tiny, it is hard to see the damned things!\u0026rdquo;\nAfter much experimenting with the monitor\u0026rsquo;s scaling, fractional scaling and such like, I found QT_AUTO_SCREEN_SCALE_FACTOR which should fix things.\nI tested:\n1 2 export QT_AUTO_SCREEN_SCALE_FACTOR=1 lyx \u0026amp; and yes, the icons were now usable. Hooray. I just need to make this permanent.\nThere is a QT Configuration application installed, but that doesn\u0026rsquo;t have any way, that is obvious, on setting environment variables. A failure!\nI added the export of QT_AUTO_SCREEN_SCALE_FACTOR to my own .bashrc startup script, which worked as long as I executed Lyx via the command line, but not from the menu. Another partial failure.\nFinally, I added a file named QT5_HIDPI_SETTING.sh to /etc/profile.d, as root, and edited the contents thusly:\n1 export QT_AUTO_SCREEN_SCALE_FACTOR=1 On a reboot, and obviously, afer deleting the setting from my own .bashrc file, Running Lyx from the menu gave me the desired, usable, toolbar icons and buttons.\nCheers,\nNorm.\n","description":"","id":8,"section":"posts","tags":null,"title":"Fixing Lyx for HiDPI Monitor","uri":"http://localhost:1313/RantsAndRaves/posts/2023/03/fixing-lyx-for-hidpi-monitor/"},{"content":"So, after many years of happy blogging \u0026ndash; some years more often than others \u0026ndash; I\u0026rsquo;ve finally run out of hosting space with my cheap and cheerful hosting provider. I thought about just paying for more storage, but with retirement and the closing of my \u0026ndash; and by \u0026ldquo;my\u0026rdquo; I mean, \u0026ldquo;mine and MrsD\u0026rsquo;s\u0026rdquo; IT Company \u0026ndash; I decided that a full Wordpress website wasn\u0026rsquo;t really necessary, what with its need for a database such as MySQL and PHP. In the end I found a static site generator which is named Hugo.\nHugo Hugo is, as briefly mentioned, a static website generator, apparently, the fastest in the world. I write a topic in Markdown and Hugo builds my website from that. I have chosen as a theme \u0026ndash; by default, Hugo doesn\u0026rsquo;t install a theme \u0026ndash; the ZZO theme. It took a lot of experimenting and playing around, but I finally found out how to make it do pretty mush all I wanted.\nThe ZZO theme has excellent documentation.\nI have to say that one of Hugo\u0026rsquo;s drawbacks is the fact that there are numerous themes available, some free, some paid for. A strange thing to say? (or type?) but, the problem is pretty much that no two themes do exactly the same things for the same outcome! Theme Sortcodes are excellent tools, but start using them in your blog source, and try changing themes \u0026ndash; in the majority of cases, it will require some amendment.\nHowever, I\u0026rsquo;m not planning on changing my theme, I hope!\nMigration Exporting from Wordpress was easy enough, I logged in to the admin panel, and exported my entire site as XML and downloaded the generated file.\nNext, I installed a set of tools from Lone Korean on GitHub and after extracting the Zip file, I followed the instructions in the readme to install them:\n1 npx wordpress-export-to-markdown And after running the code, I had my old blog site converted to a pile of individual Markdown files. I chose to configure the conversion where each blog post was put into a directory for the year and a sub-directory for the month. Under this, each post for that particular month gets it\u0026rsquo;s own sub-directory and a file named index.md. Everything was contained in a top level directory named posts. It looks something like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 content | +----posts | | | +----2023 | | | | | +----migrating-from-wordpress | | | | | | | +----index.md | +----2022 | +----2021 | +----2020 | +----2019 etc Once the utility had completed, I simply lifted the posts folder and pasted it into the Hugo\u0026rsquo;s content folder. Running up a development HTML server with:\n1 hugo server -D I connected my browser to http://localhost:1313 and I had a blog!\nI\u0026rsquo;ll be looking to get it online ASAP, I may stick with my current host, but I\u0026rsquo;m researching Netlify as a potential host as:\nIt\u0026rsquo;s free! It comes highly recommended by many. I can update my blog simply by pushing changes to a GitHub repository. I can use my current URL for the blog \u0026ndash; I think! Comments, if I choose to enable them, are handled by Disqus through the HUgo ZZO theme. There are obviously limits, but I don\u0026rsquo;\u0026rsquo;t see myself making too many blog postings over the years, and certainly I won\u0026rsquo;t be initiating builds every 5 minutes or so!'\nI wonder how it will all go?\nEditing Posts After the conversion, I had a look at the generated Markdown. It was mostly ok, strangely enough. However, a few things needed editing:\nSome special characters had been escaped, and this is not needed in Hugo: \\_ changed to _. \\* changed to *. \\- changed to -. \\= changed to =. \\[ changed to [. \\] changed to ]. \\# changed to #. Some bash root prompts, the # had to be changed to something that didn\u0026rsquo;t force a heading! The # is Markdown\u0026rsquo;s top level heading marker. A lot of code sections, from the original Wordpress blog posts were not exported as code by Wordpress. These had to be found and \u0026ldquo;codified\u0026rdquo; using Markdown techniques. I also took the liberty of adding proper code highlighting depending upon the code language in question. Some postings had to be reverted to previous versions and re-converted. It seems that my blog got somewhat hacked over the years and lots of my stuff was deleted and replace by spam postings offering dubious services and fad diets, oh yes, and a plumbing service in Oklahoma, USA. Images. The default on Wordpress is that when an image is clicked, it opens a full sized version of the thumbnail used in the post. The conversion converted the thumbnail image\u0026rsquo;s details, but left the link to my Wordpress blog. This had to be edited, but was simple. I just replaced the http://qdosmsq.dunbar-it.co.uk/blog/ part of the link, with /posts/, and it \u0026ldquo;just\u0026rdquo; worked \u0026ndash; because I chose to give my post a year and month directory during the conversion\u0026quot; Cheers,\nNorm.\n","description":"","id":9,"section":"posts","tags":null,"title":"Migrating from Wordpress","uri":"http://localhost:1313/RantsAndRaves/posts/2023/02/migrating-from-wordpress/"},{"content":"I have a lot of eBooks on my tablet and phone. I also have a backup on my Linux laptop. In the past I have often just plugged the tablet into the laptop, opened the appropriate MTP folder, selected everything and copied them all to the backup, overwriting the previous backup. There has to be a better way, surely? Step up rsync.\nThe problem is the MTP mount, however, I found a way to make it work. Those mounts, mainly from Android devices, usually open the the appropriate file explorer application in the laptop. But, they are mounted \u0026ndash; somewhere \u0026ndash; and with a bit of digging, we can find the mount point, and from that, run rsync to do the backup of the files that are new or updated only.\nFinding the Mount Point The first problem, is where on earth has the Android device been mounted?\n$ gio mount -li | grep -i activation_root |\\ cut --delimiter== --fields=2 |\\ cut --delimiter=\u0026#34;/\u0026#34; --fields=3 SAMSUNG_SAMSUNG_Android_R52M70B1KWE/ So, that\u0026rsquo;s the mount point sorted. Well, part of it. I need to know where it\u0026rsquo;s mounted. This turned out to be:\n/run/user/1000/gvfs/mtp:host=SAMSUNG_SAMSUNG_Android_R52M70B1KWE/\nwhere 1000 is my user id. That colon will need escaping too, I suspect! Now, how to work that out in code in case it ever changes?\nFinding my User Id This is easy enough, the file /etc/passwd holds details of my user id in the third field, fields are separated by colons, so, cut to the rescue!\n$ grep ^$USER /etc/passwd |\\ cut --delimiter=: --fields=3 1000 That was simple. So, this should work then? I\u0026rsquo;m using the short form of the cut parameters here by the way. Less typing! I\u0026rsquo;m also using the $(...) form of extracting a result from a command, Wordpress has stopped me using backticks for some reason!\n$ ## Where the backups live. $ cd ~/Backups/Tablet/ $ ## Get my user id, $ USER_ID=$(`grep ^$USER /etc/passwd | cut -d: -f3`) $ ## Get the MTP mounted device. $ DEVICE=$(gio mount -li | grep -i activation_root | cut -d= -f2 | cut -d/ -f3) $ ## Get the actual mount point for the MTP device. $ MOUNT=/run/user/${USER_ID}/gvfs/mtp\\\\:host\\\\=${DEVICE}/Tablet/My_Books $ ## Run the backup from MTP to Backups/Tablet/My_Books/ $ ## Trailing slash on \u0026#34;$MOUNT\u0026#34; is important here. $ rsync $MOUNT/ --archive --recursive --verbose My_Books/ And, indeed it does. I did have a problem though. My_Books used to be named My Books. Neither the ls command, nor rsync could see it on an MTP mount point, for some unknown reason. The file manager could, and allowed manual copying. However, a quick rename and all was well.\nSo, now I\u0026rsquo;ve built the above into a shell script and saved it in $HOME/bin/backup_mybooks.sh for future use.\n","description":"","id":10,"section":"posts","tags":null,"title":"Backing Up My Books","uri":"http://localhost:1313/RantsAndRaves/posts/2022/08/backing-up-my-books/"},{"content":"The code below, somewhere, shows how to measure the temperature of the ATmega328 microcontroller built in to numerous Arduino boards. You can find all the gory details in my new book Arduino Software Internals available from Apress, Amazon and good bookshops everywhere.\nA complete guide to how the Arduino Language works, and how it makes the hardware work.\nApress.com: https://www.apress.com/gb/book/9781484257890\nAmazon.co.uk: https://www.amazon.co.uk/Arduino-Software-Internals-Complete-Language/dp/1484257898/\nThe Warning Do not use this code if your Arduino has the AREF pin connected to any voltage source. When you enable the 1.1v bandgap voltage as the ADC reference, it goes Bzzzt! And all the magic blue smoke gets out! The only thing connected to AREF should be a 100nF capacitor, to ground. (Already built in on Arduino boards.)\nThe Listing /* * ARDIUNO - measure the internal temperature of the * AVR ATmega328P using the ADC internal temperature * input. See the data sheet for details. * * (C) Norman Dunbar, July 21 2018. * * This code is in the public domain - use it and abuse * it as you wish! It is based, but not copied, from * an Atmel example found at: * * https://microchipdeveloper.com/8avr:avradc * */ void setup() { // Initialise the ADC to use the // internal 1.1V reference voltage. ADMUX = (1 \u0026lt;\u0026lt; REFS0) | (1 \u0026lt;\u0026lt; REFS1); // Use the ADC multiplexer input // number 8, the temperature sensor. ADMUX |= (1 \u0026lt;\u0026lt; MUX3); // Slow the ADC clock down to 125 KHz // by dividing by 128. Assumes that the // standard Arduino 16 MHz clock is in use. ADCSRA = (1 \u0026lt;\u0026lt; ADPS2) | (1 \u0026lt;\u0026lt; ADPS1) | (1 \u0026lt;\u0026lt; ADPS0); // Non-standard 8MHz clock in use. //ADCSRA = (1 \u0026lt;\u0026lt; ADPS2) | (1 \u0026lt;\u0026lt; ADPS1) | (0 \u0026lt;\u0026lt; ADPS0); // Enable the ADC and discard the first reading as // it is always 351 on my device. ADCSRA |= (1 \u0026lt;\u0026lt; ADEN) | (1 \u0026lt;\u0026lt; ADSC); (void)readADC(); // Use the Serial monitor for output. Serial.begin(9600); Serial.println(\u0026#34;Arduino Internal Temperature\u0026#34;); } // Read the ADC result from the most recent conversion and // start another before ruturning the current reading. uint16_t readADC() { // Make sure the most recent ADC read is complete. while ((ADCSRA \u0026amp; (1\u0026lt;\u0026lt;ADSC))) { ; // Just wait for ADC to finish. } uint16_t result = ADCW; // Initiate another reading. ADCSRA |= (1 \u0026lt;\u0026lt; ADSC); return result; } //----------------------------------------------------------------- // There are many ways, it seems, to calculate the degrees C from // the ADC. Read the chapter on the ADC to find out where they come // from. Here are some: // // ADC - some random offset; // (ADC - 247)/1.22; // ADC - 273; // (((ADC - (273 - 100 - TS_OFFSET)) * 128) / TS_GAIN) + 25. // (ADC - 324.31) / 1.22 // // I\u0026#39;m using the last one, as it\u0026#39;s the one closest to my actual // temperature measurements. //----------------------------------------------------------------- void loop() { // Running average of the ADC Readings for // better accuracy. uint32_t ADCTotal = 0; float ADCAverage = 0.0; uint16_t ADCReading = readADC(); for (uint8_t x = 1; x \u0026lt; 101; x++) { ADCTotal += ADCReading; ADCAverage = (float)ADCTotal / (float)x; // Uncomment if you want a running commentary! /* Serial.print(\u0026#34;ADC = \u0026#34;); Serial.print(ADCReading); Serial.print(\u0026#34; \u0026#34;); Serial.print(\u0026#34;ADCTotal = \u0026#34;); Serial.print(ADCTotal); Serial.print(\u0026#34; \u0026#34;); Serial.print(\u0026#34;ADCAverage = \u0026#34;); Serial.println(ADCAverage); */ ADCReading = readADC(); } // Print the ADC temperature. float degreesC = (ADCAverage - 324.31) / 1.22; Serial.print(degreesC); Serial.print(\u0026#34;C, \u0026#34;); // Convert to Fahrenheit. C * 1.8 + 32. Serial.print(degreesC * 1.8 + 32); Serial.println(\u0026#34;F.\u0026#34;); // Delay a second more between readings. delay(1000); } ","description":"","id":11,"section":"posts","tags":null,"title":"Arduino Internal Temperature Measuring","uri":"http://localhost:1313/RantsAndRaves/posts/2020/05/arduino-internal-temperature-measuring/"},{"content":"I tend to compile with gcc, in a bash session, on Windows 7. I use Code::Blocks as my IDE of choice and one of my projects, well, quite a few, use the excellent OCILIB library for accessing Oracle databases, by Vincent Rogier. I can\u0026rsquo;t recommend this library highly enough.\nHowever, it comes with a Code::Blocks project file to build 32 bit libraries, but I need 64 bits. Here\u0026rsquo;s how I do it.\nNone of the following is needed if you use something like Visual C as your compiler as OCILIB supplied 32 and 64 bit libraries for Visual C/C++. I don\u0026rsquo;t use Visual C/C+ so I need to build my own libraries!\nDownload the source code Got to https://github.com/vrogier/ocilib/releases and grab the latest release. Watch out for version 4.6.0 as it will not compile on a system with an Oracle 12.1 client - but I have a fix. Version 4.6.1 (when it\u0026rsquo;s available) has the fix built in.\nYou need the file ocilib-x.x.x-windows.zip, so download it to wherever you keep your source files, and unzip it.\nAmend the Supplied Project File There is a project file called proj\\mingw\\ocilib_static_lib_mingw.cbp, so open that in Code::Blocks. You will notice that it has two options - \u0026lsquo;Release - ANSI\u0026rsquo; and \u0026lsquo;Release - UNICODE\u0026rsquo;. I use the ANSI version, but the following gives instructions for both.\nThe project as supplied uses the built in compiler which is a 32 bit only, version of gcc. My system has a separately installed version that compiles 32 and 64 bit applications. So I need to change the existing targets to use my compiler instead of the built in one.\nChange the Default Compiler Go to Project -\u0026gt; Build Options.\nSelect the top level ocilib_static_lib_mingw option on the left. Do not select either of the two release targets at this stage.\nThe default, built in 32 bit, compiler is named \u0026lsquo;GNU GCC Compiler\u0026rsquo; I need to change this to \u0026lsquo;GNU GCC Compiler 32/64bit (TDM-GCC-64)\u0026rsquo; which is the name my separately installed compiler is called. If you are prompted to save the settings, choose \u0026lsquo;Yes\u0026rsquo; then \u0026lsquo;OK\u0026rsquo; on all the remaining dialogues that pop-up. Finally, \u0026lsquo;OK\u0026rsquo; your way back to the main screen.\nThe compiler has now been changed for all targets currently defined. Now, to add the new 64 bit targets.\nAdd 64bit Targets. Go to Project -\u0026gt; Properties and select the \u0026lsquo;Build targets\u0026rsquo; tab.\nSelect \u0026lsquo;Release - ANSI\u0026rsquo; on the left and click the \u0026lsquo;Duplicate\u0026rsquo; button, change the name to \u0026lsquo;Release 64bit - ANSI\u0026rsquo; and \u0026lsquo;OK\u0026rsquo;.\nSelect the new target, if not already selected.\nUnder the \u0026lsquo;Output filename\u0026rsquo; option, towards the right side, change the \u0026rsquo;lib32\u0026rsquo; to \u0026rsquo;lib64\u0026rsquo;. Everything else remains the same.\nNow repeat the above to create the \u0026lsquo;Release 64bit - UNICODE\u0026rsquo; target, and \u0026lsquo;OK\u0026rsquo; back to the main screen.\nThat now gives us 4 targets, but they are all 32 bit at present. We now need to change the compiler options.\nChange the Compiler Options Go to Project -\u0026gt; Build Options and select the very top of the tree - ocilib_static_lib_mingw. Do not select any of the sub-targets, yet.\nOn the \u0026lsquo;Compiler settings\u0026rsquo; tab, scroll down the options list and make sure that both \u0026lsquo;Target X86 (32bit)\u0026rsquo; and \u0026lsquo;Target X86 (64bit)\u0026rsquo; are unselected.\nClick \u0026lsquo;Release - ANSI\u0026rsquo; on the left, and select \u0026lsquo;Target X86 (32bit)\u0026rsquo;.\nClick \u0026lsquo;Release - UNICODE\u0026rsquo; on the left, and select \u0026lsquo;Target X86 (32bit)\u0026rsquo;. Select \u0026lsquo;yes\u0026rsquo; if prompted to save changes.\nClick \u0026lsquo;Release 64bit - ANSI\u0026rsquo; on the left, and select \u0026lsquo;Target X86 (64bit)\u0026rsquo;. Select \u0026lsquo;yes\u0026rsquo; if prompted to save changes.\nClick \u0026lsquo;Release 64bit - UNICODE\u0026rsquo; on the left, and select \u0026lsquo;Target X86 (64bit)\u0026rsquo;. Select \u0026lsquo;yes\u0026rsquo; if prompted to save changes.\nNow \u0026lsquo;OK\u0026rsquo; back to the main screen.\nBuild the Libraries Now that we have 4 targets, it\u0026rsquo;s time to build the different libraries. In the main screen, select each of the 4 targets in turn, and then select Build -\u0026gt; Rebuild to make sure that any existing versions are rebuilt with the new compiler and options.\nWhen done, there should be two new files in each of the lib32 and lib64 folders, these will be named libociliba.a and libocilibw.a.\nFix 4.6.0 Errors If you get an error about OCI_ATTR_COL_PROPERTY_IS_CONID then you need to apply the following fix if release 4.6.1 is not yet available.\nEdit the file src/column.c and change the code to the following at lines 260 onwards - there\u0026rsquo;s only two lines to add.\n259 if (value \u0026amp; OCI_ATTR_COL_PROPERTY_IS_GEN_BY_DEF_ON_NULL) 260 { 261 col-\u0026gt;props |= OCI_CPF_IS_GEN_BY_DEFAULT_ON_NULL; 262 } 263 264 #if OCI_VERSION_COMPILE \u0026gt;= OCI_18_1 265 266 if (value \u0026amp; OCI_ATTR_COL_PROPERTY_IS_LPART) 267 { 268 col-\u0026gt;props |= OCI_CPF_IS_LPART; 269 } 270 271 if (value \u0026amp; OCI_ATTR_COL_PROPERTY_IS_CONID) 272 { 273 col-\u0026gt;props |= OCI_CPF_IS_CONID; 274 } 275 #endif 276 277 } 278 } All that is required is to add the two lines numbered 264 and 275 above, but not the line numbers please! You should now be able to compile the code.\nJob done, I can now build 32 and 64 bit versions of my applications to access the databases.\nUndefined References? If, when compiling an application that uses OCILIB, you see lots of error messages stating something like:\nUndefined reference to \u0026lsquo;OCI_Initialize\u0026rsquo;\nUndefined reference to \u0026lsquo;OCI_Cleanup\u0026rsquo;\nUndefined reference to \u0026lsquo;OCI_ \u0026hellip;\u0026rsquo;\nThen you have a wee bit of a problem. Although the 32 and 64 bit OCILIB compilations have obvioulsy worked ok, your Oracle Client software is the wrong bit size for the application\u0026rsquo;s bitness.\nI\u0026rsquo;ve had this problem where a system happily compiled OCILIB in 32 and 64 bit mode, but when I came to compile an application in 64 and 32 bit mode, it failed on the latter as my Oracle Client is only 64 bits. Bummer - I can\u0026rsquo;t compile 32 bit applications on that particular system.\nTo get around this, you must compile the application (and, maybe also OCILIB) with the correct bit sized Oracle Client on the path\n","description":"","id":12,"section":"posts","tags":null,"title":"Build 64bit OCILIB Libraries for CodeBlocks","uri":"http://localhost:1313/RantsAndRaves/posts/2019/02/build-64bit-ocilib-libraries-for-codeblocks/"},{"content":"I had an urgent need (!) to build a breadboard version of an Arduino board which I needed to run without the 16MHz crystal and the two 22pF capacitors used by most Arduino boards.\nThe following steps are what I had to do, as my brand new ATmega328P micro-controller came supplied with an Arduino Uno boot-loader installed. I didn\u0026rsquo;t want that one because it depends on having the 16MHz crystal and that takes up two of the I/O pins, which I might have a good use for, plus I can run it at 3.5V rather than 5V and get extra long life when running off of batteries.\nI had to first download the 8MHz configuration files for the Arduino IDE. These were obtained as a zip file from https://www.arduino.cc/en/uploads/Tutorial/breadboard-1-6-x.zip. (My Arduino IDE version is 1.8.6 but this download worked fine.)\nThe file was copied into my default \u0026lsquo;sketchbook\u0026rsquo; location, which can be found within the Arduino IDE\u0026rsquo;s preferences dialogue. Mine was /home/norman/Arduino.\nI then had to create a folder named hardware within my default sketchbook location, and copy the downloaded zip file into that new folder.\nFinally, a quick shutdown and restart of the Arduino IDE and I had, on the tools-\u0026gt;boards option, a new Arduino Board, right at the bottom for a bread-boarded 8MHz Arduino with an ATMega328P micro-controller. So far so good.\nI built up the circuit on my breadboard, and as the ATMega328P was already configured, and fused, for a 16 MHz crystal, I had to fit one, plus capacitors to get it to work.\nThe next step should have been simply to select the board as the new one, and burn a boot-loader with the USBTiny programmer. However, that refused to work and gave me messages that the device signature was incorrect and it certainly looked that way as it came back all zeros - not good.\nAt this point, you should imagine me trying all sorts of fiddles, some blindfold, to get it working - all to no avail. So, eventually, I went back to basics.\nI changed the board back to an Uno, and attempted to burn the bootloader - that worked. Hooray!\nI uploaded the famous \u0026lsquo;blink\u0026rsquo; sketch to the pseudo Uno. That worked fine.\nThen, I chose the new board setting again, and tried once more to burn the boot-loader, and, surprise, surprise, that worked now! Now we are cooking!\nSo, if it was running at 8MHz, I should be able to pull out the crystal and its two load capacitor friends, and it should still work (It was running the ubiquitous \u0026lsquo;blink\u0026rsquo; sketch at this point) \u0026hellip; drum roll please \u0026hellip;.\nIt worked! I now have a perfectly working 8MHz ATMega328P running on a breadboard, for now, without losing the two pins taken up by the crystal. Job done!\nAnyone Interested in the schematics? Here you are:\nNormDuino v1.0.0\nA few words about the above:\nThe AREF pin is not directly connected to VCC or any other external reference voltage. It goes through a jumper so that it can be connected and disconnected to and from VCC. You do not want a voltage on AREF if you decide to use the internally generated 1.1V reference voltage for the ADC and/or Analogue Comparator. If you do, it lets the magic blue smoke out and the device stops working. The 100nF capacitor between AREF and GND is, however, always required, regardless of whether or not AREF itself is connected to any external reference voltage. There also a jumper on the main voltage supply. This is so that it can be powered from batteries, or, from an FTDI Programmer which can supply the 5V required to power the AVR while being programmed, or afterwards. C4, C5 and XTAL1 are there in case you want to upgrade to an Arduino Uno again and run the device at 5V and 16MHz instead of 3.5V and 8MHz. However, in the configuration I have, they are not required and can be omitted - after you burn the 8MHz boot-loader of course. As you can see, I created the schematics with Fritzing. I quite like it for a quick and dirty one-off design, but it does drive me mad when I start routing PCBs or trying to get components adjusted to fit the bread-board layout. (It also, sometimes moves captions around - take note of R1\u0026rsquo;s caption - it\u0026rsquo;s not where it was in the original schematic!) ","description":"","id":13,"section":"posts","tags":null,"title":"ATMega328P 8MHz on a Breadboard","uri":"http://localhost:1313/RantsAndRaves/posts/2019/02/atmega328p-8mhz-on-a-breadboard/"},{"content":"There\u0026rsquo;s a new utility to assist in diagnosing the underlying cause of Oracle deadlocks. Interested?\nYou can download source code as well as binaries for Linux and Windows at https://github.com/NormanDunbar/DeadlockAnalysys/releases.\nAll you have to do is execute the DeadlockAnalysis utility with a list of Oracle trace files on the command line. Each trace file will get its own report, in HTML format, in the same directory as the trace file.\nThe report files use a CSS style sheet to format the output. This file can be edited to use your own installation style, if necessary, and if the style sheet exists in the output directory when the utility is executed, it will not be overwritten.\nThat\u0026rsquo;s about it really! There\u0026rsquo;s not much more to say except, enjoy! (Ok, one last thing, yes, I do wish I had managed to spell \u0026ldquo;analysis\u0026rdquo; correctly when setting up the github repository. Sigh.)\n","description":"","id":14,"section":"posts","tags":null,"title":"Oracle Deadlock Analysis","uri":"http://localhost:1313/RantsAndRaves/posts/2019/01/oracle-deadlock-analysis/"},{"content":"proctree The proctree command acts like the Linux pstree and displays the hierarchy of processes from a given starting process id.\nView Large Files If, when you view a large file you get an errors about it being too big, try the following:\necho \u0026#34;set ll=3501720 dir=/tmp\u0026#34; \u0026gt;\u0026gt; ~/.exrc That will allow you to read up toÂ 3501720 lines in a single text file.\n","description":"","id":15,"section":"posts","tags":null,"title":"Useful AIX Unix Commands","uri":"http://localhost:1313/RantsAndRaves/posts/2018/09/useful-aix-unix-commands/"},{"content":"Thanks toÂ http://www.dbaglobe.com/2010/08/drop-temporary-tablespace-hang-with-enq.html it was a simple matter to resolve the above enqueue wait on an attempt to drop a previously default temporary tablespace.\nThe session causing the problem was a DBSNMP session being run by the OEM agent on the server. The following script, from the above blog, allowed me to identify the session and sort out getting it \u0026lsquo;removed\u0026rsquo; to allow the drop to continue.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SELECT se.username username, se.SID sid, se.serial# serial#, se.status status, --se.sql_hash_value, --se.prev_hash_value, se.machine machine, su.TABLESPACE tablespace, --su.segtype, --su.CONTENTS CONTENTS FROM v$session se, v$sort_usage su WHERE se.saddr=su.session_addr; I\u0026rsquo;ve commented out a few of the columns that I\u0026rsquo;m not interested in at this point, but maybe another time\u0026hellip;.\nUSERNAME SID SERIAL# STATUS MACHINE TABLESPACE -------- --- ------- ------- ------------------ ---------- DBSNMP 595 8273 INACTIVE myserver.mydomain NORMS_TEMP The sid and serial# were then used to remove the session and allow the tablespace to be dropped.\nThe script above is a general purpose \u0026ldquo;who is using temp space at the moment\u0026rdquo; query, and has been added to my arsenal.\n","description":"","id":16,"section":"posts","tags":null,"title":"ENQ: TS - Contention","uri":"http://localhost:1313/RantsAndRaves/posts/2018/08/enq-ts-contention/"},{"content":"Have you ever seen the error RMAN-20033: control file SEQUENCE# too lowÂ and wondered what could be causing it?Â If you look on MOS, you will probably see that the error is caused by the control file in use is older than the one that was most recently used to synchronise the RMAN catalogue and that you should either recreate the database control file(s), or, delete the database from the catalog and add it in again.\nThink again! The problemÂ could be caused by the fact that there are two backups attempting to run at the same time, and both need to synchronise with the control file.\nThis is especially true if you find that while the error is reproducible, it is intermittent in nature - it doesn\u0026rsquo;t fail every time, which it would if the control file wasÂ really to blame.\n","description":"","id":17,"section":"posts","tags":null,"title":"RMAN-20033: control file SEQUENCE# too low","uri":"http://localhost:1313/RantsAndRaves/posts/2018/08/rman-20033-control-file-sequence-too-low/"},{"content":"Sometimes, just occasionally, you find yourself as a DBA on a site where, for some strange and unknown reason, you don\u0026rsquo;t have an Entity Relationship Diagram (ERD) for the database that you are working on. You could use a tool such as Toad, or SQL*Plus (or even, SQL Developer - if you must) to generate a list of referential integrity constraints. There has to be a better way.\nThe problem with lists is, they are just words. And they do say that a picture is worth a thousands words, so lets do pictures.\nGraphviz Graphviz is a set of tools for visualising graphs (that\u0026rsquo;s directed or undirected graphs as opposed to Cartesian graphs by the way) and the source files for the utility are simple text files. So, can we generate an ERD in text format and have graphviz convert it to an image? Of course we can. But first, get thee hence to https://graphviz.gitlab.io/download/ and download the utility for your particular system - it runs cross platform and is free.\nGenerating Source The following query will generate a list of parent -\u0026gt; child lines in the output, that show the relationship between any pair of tables, given a suitable starting owner and table_name.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 \\------------------------------------------------------------- -- Generate a simple \u0026#34;dot\u0026#34; file to be processed with GraphViz -- to create an image showing the referential integrity -- around a single table. -- -- Norman Dunbar ------------------------------------------------------------- set lines 2000 trimspool on trimout on set pages 2000 set echo off set feed off set verify off set timing off set head off accept owner_name prompt \u0026#34;Enter owner: \u0026#34; accept table_name prompt \u0026#34;Enter table name: \u0026#34; spool refint.dot select \u0026#39;// dot -Tpdf -o this_file.pdf this_file.dot\u0026#39; || chr(10) || chr(10) || \u0026#39;digraph refInt {\u0026#39; || chr(10) || \u0026#39; splines=ortho\u0026#39; || chr(10) || \u0026#39; size=8.25\u0026#39; || chr(10) || \u0026#39; label=\u0026#34;Referential Integrity around \u0026amp;\u0026amp;TABLE_NAME table.\u0026#34;;\u0026#39; || chr(10) || \u0026#39; rankdir=LR;\u0026#39; || chr(10) || \u0026#39; edge \\[color=blue4, arrowhead=crow\\];\u0026#39; || chr(10) || chr(10) || \u0026#39; // \u0026amp;\u0026amp;TABLE_NAME is the starting table.\u0026#39; || chr(10) || \u0026#39; \u0026#34;\u0026amp;\u0026amp;TABLE_NAME\u0026#34; \\[shape=box, style=filled, color=blue4 fillcolor=cornflowerblue\\];\u0026#39; || chr(10) || chr(10) || \u0026#39; // The remaining nodes are this style.\u0026#39; || chr(10) || \u0026#39; node \\[shape=box, style=filled, color=dodgerblue2, fillcolor=aliceblue\\];\u0026#39; || chr(10) || chr(10) || \u0026#39; // These are the parent -\u0026gt; child edges.\u0026#39; || chr(10) from dual; with refint as ( select constraint_name, table_name, r_constraint_name from dba_constraints where constraint_type = \u0026#39;R\u0026#39; and owner = upper(\u0026#39;\u0026amp;\u0026amp;owner_name\u0026#39;) ), primekey as ( select constraint_name, table_name from dba_constraints where constraint_type in ( \u0026#39;P\u0026#39;, \u0026#39;U\u0026#39;) and owner = upper(\u0026#39;\u0026amp;\u0026amp;owner_name\u0026#39;) ), links (child_table, f_key, parent_table, p_key) as ( select refint.table_name, refint.constraint_name, primekey.table_name, refint.r_constraint_name from primekey join refint on primekey.constraint_name = refint.r_constraint_name ) select distinct chr(9) || \u0026#39;\u0026#34;\u0026#39; || links.parent_table || \u0026#39;\u0026#34; -\u0026gt; \u0026#34;\u0026#39; || links.child_table || \u0026#39;\u0026#34;;\u0026#39; as dot --select distinct level, links.\\* from links start with links.parent_table = upper(\u0026#39;\u0026amp;\u0026amp;TABLE_NAME\u0026#39;) connect by nocycle links.child_table = prior links.parent_table order by 1 --order by parent_table, child_table, f_key ; select \u0026#39;}\u0026#39; from dual; spool off The output looks remarkably similar to the following example:\n// dot -Tpdf -o this_file.pdf this_file.dot digraph refInt { splines=ortho size=8.25 label=\u0026#34;Referential Integrity around ORDER_ITEM table.\u0026#34;; rankdir=LR; edge \\[color=blue4, arrowhead=crow\\]; // ORDER_ITEM is the starting table. \u0026#34;ORDER_ITEM\u0026#34; \\[shape=box, style=filled, color=blue4 fillcolor=cornflowerblue\\]; // The remaining nodes are this style. node \\[shape=box, style=filled, color=dodgerblue2, fillcolor=aliceblue\\]; // These are the parent -\u0026gt; child edges. \u0026#34;ADDRRESS\u0026#34; -\u0026gt; \u0026#34;CUSTOMER\u0026#34;; \u0026#34;COLLECTION_METHOD\u0026#34; -\u0026gt; \u0026#34;ORDER_ITEM\u0026#34;; \u0026#34;COMPENSATION_LEVEL\u0026#34; -\u0026gt; \u0026#34;ORDER_ITEM\u0026#34;; \u0026#34;CUSTOMER\u0026#34; -\u0026gt; \u0026#34;CUSTOMER_ORDER\u0026#34;; \u0026#34;CUSTOMER\u0026#34; -\u0026gt; \u0026#34;ORDER_ITEM\u0026#34;; \u0026#34;CUSTOMER_GRP\u0026#34; -\u0026gt; \u0026#34;CUSTOMER\u0026#34;; \u0026#34;CUSTOMER_GRP\u0026#34; -\u0026gt; \u0026#34;PRICING_BAND\u0026#34;; \u0026#34;CUSTOMER_ORDER\u0026#34; -\u0026gt; \u0026#34;DISCOUNT_USAGE\u0026#34;; \u0026#34;CUSTOMER_ORDER\u0026#34; -\u0026gt; \u0026#34;ORDER_ITEM\u0026#34;; \u0026#34;DISCOUNT_PLAN\u0026#34; -\u0026gt; \u0026#34;DISCOUNT_USAGE\u0026#34;; \u0026#34;DISCOUNT_USAGE\u0026#34; -\u0026gt; \u0026#34;CUSTOMER\u0026#34;; \u0026#34;ORDER_ORIGIN\u0026#34; -\u0026gt; \u0026#34;ORDER_ITEM\u0026#34;; \u0026#34;ORDER_ITEM\u0026#34; -\u0026gt; \u0026#34;EXTRAS\u0026#34;; \u0026#34;ORDER_ITEM\u0026#34; -\u0026gt; \u0026#34;ADJUSTMENTS\u0026#34;; \u0026#34;ORDER_ITEM\u0026#34; -\u0026gt; \u0026#34;ORDER_ITEM\u0026#34;; \u0026#34;ORDER_ITEM\u0026#34; -\u0026gt; \u0026#34;REFUND_PAYMENTS\u0026#34;; \u0026#34;ORDER_ITEM_STAT\u0026#34; -\u0026gt; \u0026#34;ORDER_ITEM\u0026#34;; \u0026#34;ORDER_STATUS\u0026#34; -\u0026gt; \u0026#34;CUSTOMER_ORDER\u0026#34;; \u0026#34;PRICING_BAND\u0026#34; -\u0026gt; \u0026#34;WEIGHT_RANGE\u0026#34;; \u0026#34;WEIGHT_RANGE\u0026#34; -\u0026gt; \u0026#34;ORDER_ITEM\u0026#34;; } Generating the Image The first line of the output file, refint.dot, shows the command line required to create a PDF version of the ERD. You can specify PNG, JPG, etc as desired. SVG is good for images that need to be scalable (and is usually the better quality output). To generate an SVG image, run the following command line:\n1 dot -Tsvg -o refint.svg refint.dot A file by the name of refint.svg will be created. And it looks like the following, for this particular example.\nCaveats The diagram is only as good as the referential integrity in your target database. This much should be obvious - if there are no referential integrity constraints, then all you will get is a single entity. If that\u0026rsquo;s the case, I\u0026rsquo;d be looking for another job as I suspect that all the required checking is being done in the application, rather than in the database - best avoided!\nAnd finally, you will not generate a full schema ERD with this code, but it\u0026rsquo;s handy for stuff around and about a particular table.\nEnjoy.\n","description":"","id":18,"section":"posts","tags":null,"title":"Generate Entity Relationship Diagrams from a SQL Script.","uri":"http://localhost:1313/RantsAndRaves/posts/2018/07/generate-entity-relationship-diagrams-from-a-sql-script/"},{"content":"(Snorkelling is not quite as in depth as a \u0026ldquo;deep dive\u0026rdquo;!)\nAttempting to parse a listener.log will probably bend your brain, but I needed to do it recently to determine which unique servers and/or desktops and/or application servers were still connecting to a database prior to that database going down for maintenance. This was an exercise in confirming that the documentation we have, is correct.\nAccording to the Net Services Administrator\u0026rsquo;s Guide, there are a number of different message types that can appear in a listener.log:\nA client connection request. A RELOAD, START, STOP, STATUS or SERVICES command, issued by lsnrctl. However, I\u0026rsquo;ve found that a tnsping also logs a message - probably because it\u0026rsquo;s a connection request, sort of, plus, regular \u0026ldquo;service update\u0026rdquo; messages also appear, and finally, error messages.\nEach entry in the file consists of up to 6 different fields, as follows:\nTimestamp. Connect Data. Protocol Information (optional). Event. SID or SERVICE (optional). Result code. The fields are, as mentioned, separated by asterisks. It\u0026rsquo;s very nice of Oracle to mention this in the documentation, but, actually scanning the file has shown that there can be more, or less! More on that later.\nIn the following, servers, databases and IP addresses have been obfuscated to protect the innocent, me! Even the dates and times are somewhat fictitious.\nConnection Requests Connection requests come in two types, successful and failed. Both have, as far as my listener logs are concerned, the full complement of 6 fields:\n15-MAY-2018 10:34:44 * (CONNECT_DATA=(CID=(PROGRAM=)(HOST=__jdbc__)(USER=))(SERVICE_NAME=ORCL_RW)) * (ADDRESS=(PROTOCOL=tcp)(HOST=192.168.1.20)(PORT=35405)) * establish * ORCL_RW * 0 A failed connection request is normally followed by an error message.\n14-JUN-2018 10:07:37 * (CONNECT_DATA=(CID=(PROGRAM=JDBC Thin Client)(HOST=__jdbc__)(USER=root))(SERVICE_NAME=ORCL_RO)) * (ADDRESS=(PROTOCOL=tcp)(HOST=192.168.1.20)(PORT=34082)) * establish * ORCL_RO * 12514 TNS-12514: TNS:listener does not currently know of service requested in connect descriptor Lsnrctl Commands These have a reduced complement of fields, only 4.\n14-JUN-2018 10:14:34 * (CONNECT_DATA=(CID=(PROGRAM=)(HOST=my_server)(USER=oracle))(COMMAND=status)(ARGUMENTS=64)(SERVICE=LISTENER)(VERSION=202375680)) * status * 0 Tnsping Requests These have 3 fields, separated by asterisks.\n15-MAY-2018 10:36:43 * ping * 0 Service Updates These also have 3 fields, separated by asterisks.\n15-MAY-2018 10:34:50 * service_update * pnet01p1 * 0 Reload Requests These have 4 fields, and the following is copied directly from the docs as I\u0026rsquo;m not allowed to carry out a reload on my listeners!\n14-MAY-2009 00:29:54 * (connect_data=(cid=(program=)(host=sales-server)(user=jdoe))(command=reload) (arguments=64)(service=listener)(version=135290880)) * reload * 0 Service Registration These have 3 fields, and the following is copied directly from the docs as all my databases registered before the start of the listener log!\n14-MAY-2009 15:28:43 * service_register * sales * 0 Service Died These have 3 fields, and the following is copied directly from the docs as none of my services have died, yet!\n14-MAY-2009 15:51:26 * service_died * sales * 12537 Error Messages These normally follow on from a failed connection request, or command, and only have a single field:\nTNS-12514: TNS:listener does not currently know of service requested in connect descriptor Timestamp Messages It appears also that there can be numerous Timestamp lines in the listener.log, these too consist of a single field.\nFri Jun 01 14:15:30 2018 Parsing the Listener Log Enough background, moving on\u0026hellip;\nI\u0026rsquo;m using awk to process the log files, it works, I can tell it to use an asterisk as the field separator and so on. I\u0026rsquo;m not even using any of the special GNU gawk extensions as I don\u0026rsquo;t have gawk on this server.\nIn this exercise, I\u0026rsquo;m really only interested in connection attempts, and they have (or should have) 6 fields separated by \u0026lsquo;*\u0026rsquo; characters. However, being a suspicious type, I better check:\n1 2 3 4 5 6 7 8 9 awk -F* \u0026#39;{print NF;}\u0026#39; /u01/listener.full.log | sort -n -u 0 1 3 4 6 8 10 Hmm, looks slightly unpromising. Lets extract all those different line types to separate files,. If your listener.log is as big as mine, that exercise took a while! Don\u0026rsquo;t bother with the zeros, they are the blank lines.\n1 2 3 4 for x in 1 3 4 6 8 10 do awk -F* -v XXX=${x} \u0026#39;{if (NF == XXX){print $0;}}\u0026#39; /u01/listener.full.log \u0026gt; /u01/listener.${x}.log done 1 2 3 4 5 6 7 8 9 ls -l /u01/listener* 65013791 Jun 14 11:06 /u01/listener.1.log 739825 Jun 14 11:07 /u01/listener.3.log 45683981 Jun 14 11:07 /u01/listener.4.log 2288202297 Jun 14 11:08 /u01/listener.6.log 342 Jun 14 11:08 /u01/listener.8.log 382 Jun 14 11:08 /u01/listener.10.log 2399640823 Jun 14 10:23 /u01/listener.full.log The listing above is slightly edited for ordering and space purposes. It shows just the size in bytes, the date/time and the file name.\n/u01/listener.1.log is the full list of error messages and timestamp records. Nothing to see here! /u01/listener.3.log is the list of tnsping type messages, version messages etc. Nothing to see here either! /u01/listener.4.log is a list of \u0026ldquo;service update\u0026rdquo; events, plus the lsnrctl status or lsnrctl services commands. /u01/listener.6.log is a list of the connection requests, successful or failed. This is what I\u0026rsquo;m interested in. /u01/listener.8.log is a list of corruptions, possibly caused by the listener being briefly unavailable. The format of this file seems to be two entries amalgamated into one. Best avoided! /u01/listener.10.log is a similar problem to listener.8.log above. Another set of corruptions. In order to whittle down the amount of data I \u0026rsquo;m scanning, I have decided to extract only those rows with 6 fields, and which have a zero response code. These are the successful connection attempts. This is what I\u0026rsquo;m trying to gather figures for.\nI will use the following script to extract the data from my own listener.6.log file, which is the extract of the 6 field records from the full listener log. However, the script still checks - in case I wish to use it on the listener.log itself.\nI\u0026rsquo;m only keeping the Connect Data, Protocol Data and the SID/Service as I don\u0026rsquo;t need the rest.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #! /usr/bin/awk -f # # Parses the listener.log file (or whatever comes in on stdin) to # find any line with 6 fields. These will be: # # $1 Date and time. Unwanted. # $2 Connect Data. # $3 Address data including host from where the request came from. # $4 Usually \u0026#34;establish\u0026#34;. Unwanted. # $5 The service name connecting to. Kept, just in case. # $6 The response code. 0 is good. We only want zero. # # The output will be only those fields we want from any connection request that worked. # # # USAGE: # # cat listener.log | awk -f extract_listener.awk \u0026gt; listener.temp.log # # OR: # # awk -f extract_listener.awk \u0026lt; listener.log \u0026gt; listener.temp.log # # OR: # # ./extract_listener.awk \u0026lt; listener.log \u0026gt; listener.temp.log # # Norman Dunbar # 07/04/2018. # # This happens before the start of the file. BEGIN{ # Set the incoming field separator. FS=\u0026#34;*\u0026#34;; # Set the output fields separator too. OFS=\u0026#34;*\u0026#34;; } # This happens for every record in the file. { # Only interested in records with 6 fields... if (NF == 6) { # And of those, only successful connection requests. if ($6 == \u0026#34;0\u0026#34;) { print $2, $3, $5; } } } I\u0026rsquo;m running this as follows:\n1 ./extract_listener.awk \u0026lt; /u01/listener.6.log \u0026gt; /u01/listener.temp.log The output file looks vaguely like this:\n(CONNECT_DATA=(CID=(PROGRAM=)(HOST=__jdbc__)(USER=))(SERVER=DEDICATED)(SERVICE_NAME=ORCL_RW)) * (ADDRESS=(PROTOCOL=tcp)(HOST=192.168.1.20)(PORT=43511)) * ORCL_RW (CONNECT_DATA=(CID=(PROGRAM=)(HOST=__jdbc__)(USER=))(SERVICE_NAME=ORCL_RW)) * (ADDRESS=(PROTOCOL=tcp)(HOST=192.168.1.20)(PORT=47111)) * ORCL_RW (CONNECT_DATA=(CID=(PROGRAM=)(HOST=__jdbc__)(USER=))(SERVER=DEDICATED)(SERVICE_NAME=ORCL_RW)) * (ADDRESS=(PROTOCOL=tcp)(HOST=192.168.1.20)(PORT=48619)) * ORCL_RW (CONNECT_DATA=(CID=(PROGRAM=JDBC Thin Client)(HOST=__jdbc__)(USER=norman))(SERVICE_NAME=ORCL_RW)) * (ADDRESS=(PROTOCOL=tcp)(HOST=192.168.1.20)(PORT=40722)) * ORCL_RW (CONNECT_DATA=(CID=(PROGRAM=)(HOST=__jdbc__)(USER=))(SERVICE_NAME=ORCL_RW)) * (ADDRESS=(PROTOCOL=tcp)(HOST=192.168.1.20)(PORT=47113)) * ORCL_RW (CONNECT_DATA=(CID=(PROGRAM=)(HOST=__jdbc__)(USER=))(SERVICE_NAME=ORCL_RW)) * (ADDRESS=(PROTOCOL=tcp)(HOST=192.168.1.20)(PORT=47112)) * ORCL_RW What I\u0026rsquo;m after is a list of unique programs, hosts and users from the connect data field, plus the host they came from on the protocol information field. I tried a couple of different script methods before finding a reasonably good workable one. The processing is:\nUse the \u0026lsquo;(\u0026rsquo; as a field separator for the input file. Use the \u0026lsquo;=\u0026rsquo; as a field separator for the output. Look for the text \u0026ldquo;CID=\u0026rdquo; in all fields. If \u0026ldquo;CID=\u0026rdquo; is found, then the desired program is in the next field, host in the one after and user in the one after that. The host (from the protocol data) is in the second last field. Here\u0026rsquo;s the script, which has the extremely meaningful name of a.awk!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 #! /usr/bin/awk -f # # Parses whatever comes in on stdin (from extract_listener.awk) to extract # the PROGRAM, HOST and USER from the CONNECT DATA plus the HOST from the PROTOCOL # DATA of a listener log file. # # We need to find \u0026#34;CID=\u0026#34; in any field, then output the following three fields - # PROGRAM, HOST and USER, plus the second to last field - HOST. # # The input file fields are separated by \u0026#39;(\u0026#39; which will need double escapes. # The output fields will be separated by \u0026#39;=\u0026#39;. # # It appears that on AIX, printing \u0026#34;expression fields\u0026#34; doesn\u0026#39;t work. :-( # # USAGE: # # cat listener.temp.log | awk -f a.awk \u0026gt; listener.a.log # # OR: # # awk -f a.awk \u0026lt; listener.temp.log \u0026gt; listener.a.log # # OR: # # ./a.awk \u0026lt; listener.temp.log \u0026gt; listener.a.log # # Norman Dunbar # 07/04/2018. # BEGIN { FS=\u0026#34;\\\\\\\\(\u0026#34;; OFS=\u0026#34;=\u0026#34;; } { # Count fields minus 1. myNF = NF - 1; # Find CID= ... for (x = 1; x \u0026lt; NF; x++) { if ($x == \u0026#34;CID=\u0026#34;) break; } if (x != NF) { program = x+1; host = x+2; user=x+3; print $program, $host, $user, $myNF; } else { print \u0026#34;CID= Not Found\\\\n\u0026#34;; print $0; } } I did try to print $x+1, $x+2, $x+3, $NF-1 but all I got was those digits! I\u0026rsquo;m using an AIX version of awk so that might account for my difficulties! The output looks like this:\nPROGRAM=)=HOST=__jdbc__)=USER=))=HOST=172.20.238.35) PROGRAM=JDBC Thin Client)=HOST=__jdbc__)=USER=norman))=HOST=192.168.1.20) PROGRAM=JDBC Thin Client)=HOST=__jdbc__)=USER=briansmith))=HOST=192.168.10.40) PROGRAM=JDBC Thin Client)=HOST=__jdbc__)=USER=webonline))=HOST=192.168.10.62) Next, I need to process this and extract a csv file consisting on just the 4 data items I need - program, host, user and from host. The excellently named b.awk does this for me.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 #! /usr/bin/awk -f # # Parses whatever comes in on stdin (from a.awk) to convert the input # PROGRAM, HOST, USER and HOST (another host) into a CSV file. # # The input file fields are separated by \u0026#39;=\u0026#39;. # The output fields will be separated by \u0026#39;,\u0026#39;. (It\u0026#39;s CSV after all!) # # USAGE: # # cat listener.a.log | awk -f b.awk \u0026gt; listener.b.csv # # OR: # # awk -f b.awk \u0026lt; listener.a.log \u0026gt; listener.b.csv # # OR: # # ./b.awk \u0026lt; listener.a.log \u0026gt; listener.b.csv # # Norman Dunbar # 07/04/2018. # BEGIN { FS=\u0026#34;=\u0026#34;; OFS=\u0026#34;,\u0026#34;; } # We get $2 = PROGRAM + \u0026#39;)\u0026#39; # $4 = HOST + \u0026#39;)\u0026#39; # $6 = USER + \u0026#39;))\u0026#39; # $8 = Calling Host + \u0026#39;)\u0026#39; # # Need to slice and dice. Oh, wrap the program in quotes in case \u0026#34;+ASM\u0026#34; comes up # as this foxes Excel when imported! It thinks it\u0026#39;s a formula because of the leading \u0026#39;+\u0026#39;. { print \u0026#34;\\\\\u0026#34;\u0026#34; substr($2,1,length($2) - 1) \u0026#34;\\\\\u0026#34;\u0026#34;, substr($4, 1, length($4) - 1), substr($6, 1, length($6) - 2), substr($8, 1, length($8) - 1); } That gives me this:\n\u0026#34;\u0026#34;,__jdbc__,,172.20.238.35 \u0026#34;JDBC Thin Client\u0026#34;,__jdbc__,norman,192.168.1.20 \u0026#34;JDBC Thin Client\u0026#34;,__jdbc__,briansmith,192.168.10.40 \u0026#34;JDBC Thin Client\u0026#34;,__jdbc__,webonline,192.168.10.62 Now all I need to do is replace the empty programs with \u0026ldquo;Unknown Program\u0026rdquo; and any empty users with \u0026ldquo;Unknown User\u0026rdquo; and sort into a unique output, so here\u0026rsquo;s the sed file I\u0026rsquo;m using to make the edits. It goes by the name of sed.file (I do like meaningful names!):\ns/^\u0026#34;\u0026#34;,/\u0026#34;Unknown Program\u0026#34;,/ s/,,/,Unknown User,/ s/))) // The last line is for those weird entries which have a very strange username! This minor edit gives me the following:\n\u0026#34;Unknown Program\u0026#34;,__jdbc__,Unknown User,172.20.238.35 \u0026#34;JDBC Thin Client\u0026#34;,__jdbc__,norman,192.168.1.20 \u0026#34;JDBC Thin Client\u0026#34;,__jdbc__,briansmith,192.168.10.40 \u0026#34;JDBC Thin Client\u0026#34;,__jdbc__,webonline,192.168.10.62 So, finally, I can string it all together and sort the output into unique records, add a few headings and we are done:\n1 2 3 4 5 ./extract_listener.awk \u0026lt; listener.log |\\\\ ./a.awk |\\\\ ./b.awk |\\\\ sed -f sed.file |\\\\ sort -u \u0026gt; temp.csv I have a headings file as follows:\nPROGRAM,HOST,USERNAME,CALLING_HOST Which I prepend to the above output, to get the final, desired csv file:\ncat headings temp.csv \u0026gt; listener.csv\nI now have the required listing of all the different programs in use to connect to the database, who connected and from which server. It makes for interesting reading, and surprisingly, matches the documentation!\nSummary There\u0026rsquo;s lots of interesting information in the listener log files, some of it useful, some of it not so useful. Awk is a pretty decent way of mining the file for the information you need, and, even better, you don\u0026rsquo;t have to write perl scripts either! At least with awk, you can read it again after 6 months, and still understand it.\nEnjoy.\n","description":"","id":19,"section":"posts","tags":null,"title":"Snorkelling in the Oracle Listener Logs.","uri":"http://localhost:1313/RantsAndRaves/posts/2018/06/snorkelling-in-the-oracle-listener-logs/"},{"content":"You know the score, you are running an impdp and it looks to have hung up. You\u0026rsquo;ve watched the log file (or on screen messages) and it\u0026rsquo;s sitting at something like:\nProcessing object type TABLE_EXPORT/TABLE/INDEX/INDEX But hasn\u0026rsquo;t moved from there for what seems hours. The alert log for the database is of no help, as there are no errors or warnings logged there. What\u0026rsquo;s going on?\nIs the import actually running? Check DBA_DATAPUMP_JOBS to find out:\n1 2 3 select owner_name, job_name, operation, job_mode from dba_datapump_jobs where state=\u0026#39;EXECUTING\u0026#39; ; Which gives something like:\nOWNER_NAME JOB_NAME OPERATION JOB_MODE ---------- ------------------ --------- -------- DUNBARNOR SYS_IMPORT_FULL_01 IMPORT FULL So, we see at least one full import job is running. Good news. Do we have any sessions running though?\n1 2 select owner_name, job_name, session_type from dba_datapump_sessions; And we see this:\nOWNER_NAME JOB_NAME SESSION_TYPE ---------- ------------------ ------------- DUNBARNOR SYS_IMPORT_FULL_01 DBMS_DATAPUMP DUNBARNOR SYS_IMPORT_FULL_01 MASTER DUNBARNOR SYS_IMPORT_FULL_01 WORKER DUNBARNOR SYS_IMPORT_FULL_01 WORKER DUNBARNOR SYS_IMPORT_FULL_01 WORKER DUNBARNOR SYS_IMPORT_FULL_01 WORKER So, we have the master session, and a few workers. My job is running with parallel=4 in the parameter file, so that\u0026rsquo;s why there are 4 workers. Are they actually doing anything?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 select v.status, v.sid,v.serial#,io.block_changes,event from v$sess_io io, v$session v where io.sid = v.sid and v.saddr in ( select saddr from dba_datapump_sessions ) order by sid; STATUS SID SERIAL# BLOCK_CHANGES EVENT ------ ---- ------- ------------- -------------------------------------------- ACTIVE 45 27 197679 PX Deq: Execute Reply ACTIVE 324 431 5484 wait for unread message on broadcast channel ACTIVE 614 89 15406 wait for unread message on broadcast channel ACTIVE 757 105 50130 wait for unread message on broadcast channel ACTIVE 891 169 77216 wait for unread message on broadcast channel ACTIVE 1035 59 76471 wait for unread message on broadcast channel Hmm, looks like nothing is working at all. Every session appears to be waiting for something to be broadcast. The number of BLOCK_CHANGES should be increasing if the job was working correctly, shouldn\u0026rsquo;t it?\nThe job has definitely hung.\nOr has it?\nWell, in this particular case, the clue is on screen, in the log file, and noted above.\nThe job is Processing object type TABLE_EXPORT/TABLE/INDEX/INDEX so, I\u0026rsquo;m hoping it\u0026rsquo;s creating indexes. Can we check? Of course!\nFor reasons of space across the page, I\u0026rsquo;ve had to substr() a couple of columns in the following SQL statement. You should refrain from doing so to get the full picture.\n1 2 3 4 5 6 7 8 select s.sid, s.module, s.state, substr(s.event, 1, 21) as event, s.seconds_in_wait as secs, substr(sql.sql_text, 1, 30) as sql_text from v$session s join v$sql sql on sql.sql_id = s.sql_id where s.module like \u0026#39;Data Pump%\u0026#39; order by s.module, s.sid; And now we can see what\u0026rsquo;s going on, The SQL_TEXT column shows that a number of parallel sessions are indeed creating indexes:\nSID MODULE STATE EVENT SECS SQL_TEXT ---- ---------------- ------- --------------------- ---- ------------------------------ 614 Data Pump Master WAITING wait for unread messa 0 BEGIN :1 := sys.kupc$que_int.r 45 Data Pump Worker WAITING PX Deq: Execute Reply 64 CREATE INDEX \u0026#34;DUNBARNOR\u0026#34;.\u0026#34;XIE1 192 Data Pump Worker WAITING direct path read temp 0 CREATE INDEX \u0026#34;DUNBARNOR\u0026#34;.\u0026#34;XIE1 757 Data Pump Worker WAITING wait for unread messa 1 BEGIN :1 := sys.kupc$que_int.t 757 Data Pump Worker WAITING wait for unread messa 1 BEGIN :1 := sys.kupc$que_int.t 757 Data Pump Worker WAITING wait for unread messa 1 BEGIN :1 := sys.kupc$que_int.t 857 Data Pump Worker WAITING direct path read temp 0 CREATE INDEX \u0026#34;DUNBARNOR\u0026#34;.\u0026#34;XIE1 891 Data Pump Worker WAITING wait for unread messa 1 BEGIN :1 := sys.kupc$que_int.t 891 Data Pump Worker WAITING wait for unread messa 1 BEGIN :1 := sys.kupc$que_int.t 891 Data Pump Worker WAITING wait for unread messa 1 BEGIN :1 := sys.kupc$que_int.t 1019 Data Pump Worker WAITING PX Deq: Execution Msg 64 CREATE INDEX \u0026#34;DUNBARNOR\u0026#34;.\u0026#34;XIE1 1035 Data Pump Worker WAITING wait for unread messa 1 BEGIN :1 := sys.kupc$que_int.t 1035 Data Pump Worker WAITING wait for unread messa 1 BEGIN :1 := sys.kupc$que_int.t 1035 Data Pump Worker WAITING wait for unread messa 1 BEGIN :1 := sys.kupc$que_int.t 1324 Data Pump Worker WAITING PX Deq: Execution Msg 64 CREATE INDEX \u0026#34;DUNBARNOR\u0026#34;.\u0026#34;XIE1 1749 Data Pump Worker WAITING direct path read temp 0 CREATE INDEX \u0026#34;DUNBARNOR\u0026#34;.\u0026#34;XIE1 2034 Data Pump Worker WAITING PX Deq: Execution Msg 67 CREATE INDEX \u0026#34;DUNBARNOR\u0026#34;.\u0026#34;XIE1 2153 Data Pump Worker WAITING direct path read temp 0 CREATE INDEX \u0026#34;DUNBARNOR\u0026#34;.\u0026#34;XIE1 2177 Data Pump Worker WAITING PX Deq: Execution Msg 66 CREATE INDEX \u0026#34;DUNBARNOR\u0026#34;.\u0026#34;XIE1 So, the impdp job is still running and is still working, it\u0026rsquo;s just not importing at the moment, it is building indexes.\nWhy does it appear hung? This is an exceedingly large table, with far too many indexes for comfort. They all need to be recreated, so this takes time. Repeatedly executing the above query (the non-substr()\u0026rsquo;d version I mean) will show the names of the indexes changing every time it completes one index and moves on to the next. You will also see it moving on to a different table name when it has built all the indexes on the currently displayed table.\nYou will, I hope, also notice a number of SIDs in the above output which are never mentioned in the preceding query results. This is why, I suspect, that there are a lot of hits on the web about the wait event wait for unread message on broadcast channel related to impdp (or expdp) but so far, none of those hits seem to go into any details about the sessions you don\u0026rsquo;t find in DBA_DATAPUMP_JOBS or DBA_DATAPUMP_SESSIONS, perhaps a quick look in V$SESSION is more helpful when trying to track down suspected hangs in these utilities?\nSo, it turns out the job was not hung after all. At least, not in this case.\nEnjoy.\n","description":"","id":20,"section":"posts","tags":null,"title":"IMPDP Hangs, or Appears to Hang - But Has it?","uri":"http://localhost:1313/RantsAndRaves/posts/2018/06/impdp-hangs-or-appears-to-hang-but-has-it/"},{"content":"My Arduino Nano started to refuse to upload sketches after I upgraded the boards library to version 1.6.21 from 1.6.20. All I got was this:\n... avrdude: stk500\\_recv(): programmer is not responding avrdude: stk500\\_getsync() attempt 1 of 10: not in sync: resp=0x00 avrdude: stk500\\_recv(): programmer is not responding avrdude: stk500\\_getsync() attempt 2 of 10: not in sync: resp=0x00 ... Repeat 100 times!\nAfter reverting back to the previous boards library, version 1.6.20, everything worked. I upgraded and everything stopped working.\nThe solution, from the arcduino.cc forums, is simple:\nTools -\u0026gt; Processor -\u0026gt; ATmega328P (Old Bootloader)\nRead the forum thread at https://forum.arduino.cc/index.php?topic=532983.0 if you wish. I\u0026rsquo;m just noting this here for my own reference and definitely not claiming any credit.\n","description":"","id":21,"section":"posts","tags":null,"title":"Arduino Nano - Cannot Upload Sketches after Board Upgrade to 1.6.21","uri":"http://localhost:1313/RantsAndRaves/posts/2018/05/arduino-nano-cannot-upload-sketches-after-board-upgrade-to-1-6-21/"},{"content":"I have ripped all my music, well most of it, to FLAC for the quality aspect. Sometimes though, I need to convert to MP3 for some of the lesser audio players out there that I might have to use from time to time.\nI have recently come across a pretty nifty (Linux) way to do this, without having to cope with having duplicate files in FLAC and MP3 formats on my hard drives.\nThe utility I\u0026rsquo;ve discovered is called mp3fs and is a FUSE file system whereby a normal, non-root user, can mount the FLAC folder and see the contents as MP3 files.\nOnce mounted in this way, the MP3 files can be played by, or copied to, a less well enabled device and will be converted to MP3 on the fly. I don\u0026rsquo;t then have to have MP3 files clogging up my music hard drives.\nInstallation On my Linux Mint 18.2 setup, it\u0026rsquo;s a simple one liner:\n1 sudo apt-get install mp3fs Usage First, create the folder where my FLAC files will appear as MP3 files. I\u0026rsquo;m calling mine mp3:\n1 mkdir ./mp3 Then mount the FLAC folder on to the new mp3 folder. The FLAC files live in /media/norman/USB_MUSIC and sub-folders below this mount point:\n1 mp3fs -b 192 /media/norman/USB\\_MUSIC ./mp3 The -b 192 part sets the bit rate for the MP3 output files. Other values are available.\nNow, if I do a quick check, I see the following:\n1 2 ls ./mp3 Benny Andersson Carole\\_King Fleetwood\\_Mac Tangerine Dream Zero Project 1 2 ls ./mp3/Tangerine\\\\ Dream/ Quantum\\_Gate 1 2 3 4 5 6 7 8 9 10 ls ./mp3/Tangerine\\\\ Dream/Quantum\\_Gate/ 01 - Sensing\\_Elements.mp3 02 - Roll\\_the\\_Seven\\_Twice.mp3 03 - Granular\\_Blankets.mp3 04 - It\\_is\\_Time\\_to\\_Leave\\_When\\_Everyone\\_is\\_Dancing.mp3 05 - Identify\\_Proven\\_Matrix.mp3 06 - Non-Locality\\_Destination.mp3 07 - Proton\\_Bonfire.mp3 08 - Tear\\_Down\\_the\\_Grey\\_Skies.mp3 09 - Genesis\\_of\\_Precious\\_Thoughts.mp3 It\u0026rsquo;s looking good. Now I can copy my wife\u0026rsquo;s new CDs from the folders above to the device she wants to play them on, as MP3 files. My FLAC ripped files will be converted to MP3 on the fly as the copy progresses.\nOnce completed, I can unmount the mp3 folder as follows:\n1 fusermount -u ./mp3 There are numerous options that can be supplied to the mp3fs command, including one to automatically unmount the folder after the file operation has completed. I prefer to manual unmount things as I might want to do other stuff later.\n","description":"","id":22,"section":"posts","tags":null,"title":"FLAC to MP3 as Easy as Pie!","uri":"http://localhost:1313/RantsAndRaves/posts/2017/10/flac-to-mp3-as-easy-as-pie/"},{"content":"I have recently come across a pretty nifty Linux utility that allows me to mount a remote filesystem on an SSH server, locally and without requiring root privileges to do so. The remote filesystem happens to be where my backups are located, so that\u0026rsquo;s going to be useful for making and restoring backups!\nThe utility I\u0026rsquo;ve discovered is called sshfs and is a FUSE file system whereby a normal, non-root user, can mount the remote folder and see the contents as if they were actually in a local folder.\nOnce mounted in this way, the remote files can be copied to, from, deleted etc in the normal manner.\nInstallation On my Linux Mint 18.2 setup, it\u0026rsquo;s a simple one liner:\n1 sudo apt-get install sshfs Usage First, create the folder where my remote files will appear. I\u0026rsquo;m calling mine sshfiles:\n1 mkdir ./sshfiles Then mount the remote folder on to the new sshfiles folder. The backup files live in the norman/backups folder, on a server named wd and the user account I need to login to is my own, norman:\n1 sshfs norman@wd:norman/backups ./sshfiles Now, if I do a quick check, I see the following:\n1 2 3 4 ls ./sshfiles Backup\\_scripts Downloads Records data Calibre Home SourceCode It\u0026rsquo;s looking good. Now I can copy my local folders to the backup device by copying them locally to the sshfiles folder. The sshfs utility will do the needful in copying them across the network to the correct server.\nOnce my backups (or restores) are completed, I can unmount the sshfiles folder as follows:\n1 fusermount -u ./sshfiles ","description":"","id":23,"section":"posts","tags":null,"title":"Using FUSE to Mount an SSH Folder Locally","uri":"http://localhost:1313/RantsAndRaves/posts/2017/10/using-fuse-to-mount-an-ssh-folder-locally/"},{"content":"I wanted to compile the sqlite3 shell on Windows, using my Free Embarcadero C compiler, but it didn\u0026rsquo;t work. It was quite easy to fix, but if you are affected, read on.\nFirst Attempt After unzipping the amalgamated source files a default compilation was attempted with the following command line. The -tCM option simply says to create a console based non-windows application.\nbcc32c -o sqlite3.exe shell.c sqlite3.c -tCM Which results in the following compiler warnings and linker errors:\nshell.c:158:3: warning: implicit declaration of function \u0026#39;_setmode\u0026#39; is invalid in C99... shell.c:7018:26: warning: implicit declaration of function \u0026#39;_isatty\u0026#39; is invalid in C99... ... Error: Unresolved external \u0026#39;__isatty\u0026#39; referenced from C:\\\\SQLITE3\\\\SHELL-264807.O Error: Unresolved external \u0026#39;__setmode\u0026#39; referenced from C:\\\\SQLITE3\\\\SHELL-264807.O The compiler warnings gave me the impression that something called _isatty and _setmode do not exist, or something is not setting up their function definitions correctly. That\u0026rsquo;s usually the case with the warnings shown anyway.\nBecause all the warnings are in the file shell.c, that\u0026rsquo;s what we need to look at.\nFixing the shell.c Source File Open shell.c in your favourite text editor. Alternatively, use Notepad!\nSearch for _isatty. You should find it around line 104.\nReplace this code:\n1 2 3 ... # define isatty(h) _isatty(h) ... With the following:\n1 2 3 4 5 ... # if !defined (__BORLANDC__) # define isatty(h) _isatty(h) # endif ... Embarcadero aka Borland C doesn\u0026rsquo;t have anything named _isatty but it does have isatty and it is the function we need to be calling. Moving on\u0026hellip;\nSearch for _setmode. You should find it around line 158.\nReplace this code:\n1 2 3 4 5 ... _setmode(_fileno(file), _O_BINARY); ... _setmode(_fileno(file), _O_TEXT); ... With the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ... #if defined (__BORLANDC__) setmode(_fileno(file), _O_BINARY); #else _setmode(_fileno(file), _O_BINARY); #endif ... #if defined (__BORLANDC__) setmode(_fileno(file), _O_TEXT); #else _setmode(_fileno(file), _O_TEXT); #endif ... Again, Embarcadero C doesn\u0026rsquo;t have _setmode as it has setmode instead, so that\u0026rsquo;s what we need to be calling here, twice.\nOnce the changes have been saved, exit from the editor and recompile.\nSecond Attempt The same command line is used:\nbcc32c -o sqlite3.exe shell.c sqlite3.c -tCM Which results in the following:\nEmbarcadero C++ 7.20 for Win32 Copyright (c) 2012-2016 Embarcadero Technologies, Inc. shell.c: sqlite3.c: Turbo Incremental Link 6.75 Copyright (c) 1997-2016 Embarcadero Technologies, Inc. No errors, no warnings. Job done, we have a shell!\nTesting, Testing It. Just. Works!\nDatabases can be created, deleted, opened closed, attached and so on, tables and indexes etc can be created and used. Life is good!\n","description":"","id":24,"section":"posts","tags":null,"title":"Compiling Sqlite3 Shell with Embarcadero/Borland C.","uri":"http://localhost:1313/RantsAndRaves/posts/2017/04/compiling-sqlite3-shell-with-embarcaderoborland-c/"},{"content":"Trace CollierÂ has been updated and rewritten. Numerous bugs and foibles have been fixed.\nTraceAdjust is a new, useful, utility to carry out some pre-processing on a trace file before you have to use your own weary eyes to work thorough potential Oracle performance problems! That\u0026rsquo;s why I wrote it!\nTrace Collier My old utility has been totally rewritten in C++ rather than vanilla C, which has had the bonus of allowing me to fix some bugs, and do away with the need to recompile whenever you hit a system limit of some kind. You can view the readme.pdf file here.\nSource code is available from GitHub as you will need to compile this utility to match your database server, or analysis systems. Tested on Windows and Linux.\nTraceAdjust TraceAdjust is a new utility which massages a tracefile to do the following:\nAdd a decimal point in the tim values, to separate full seconds from micro-seconds. My eyes are too old to do it manually! Add a delta to the end of the trace line to show the difference between the previous tim and the current tim. This saves me doing it with a calculator, and multi-digit tim values! Adds a running delta since the most recent timestamp record in the trace file. Perhaps not as useful as the above, but it works for me. Convert the deltas so far, into actual, locally oriented timestamps showing resolution down to the micro-second. Having the tracefile show these values already worked out does make life a bit easier when you have to get dirty in the raw traces.\nHave a look at the Readme.pdf file here, and, download the source code which, as ever, is available from GitHub as you will need to compile this utility to match your database server, or analysis system. Tested on Windows and Linux.\n","description":"","id":25,"section":"posts","tags":null,"title":"Trace Collier and TraceAdjust","uri":"http://localhost:1313/RantsAndRaves/posts/2017/03/traceminer2-and-traceadjust/"},{"content":"Trace CollierÂ has been updated again. Mostly bug fixes, but there\u0026rsquo;s a little enhancement too. The current release is 0.21.\nTrace CollierÂ is a utility that parses an Oracle trace file, with binds listed (event 10046 level 4 or 12, etc) and extracts all the user submitted SQL statements and writes them to an output file with the bind variables replaced by the actual literals used when the statement was executed.\nYou can download the C source code from my GitHub repository https://github.com/NormanDunbar/TraceMiner, or download it directly from https://github.com/NormanDunbar/TraceMiner/archive/master.zip and compile it for your own machine. Currently, I know that the utility is happily running on Windows, AIX and Linux.\nA number of fixes have been made between version 0.19 and the current, 0.21:\nV0.20, which didn\u0026rsquo;t get released, made the following changes:\nData type 96, NCHAR or NVARCHAR, intermittent bug fixed. Sometimes it tried to extract the hex values from the previous line of text, not the current one. Weird! Thankfully, verbose output showed it up. MAXBINDS upped from 50 to 150 - just because! CLOSE of a cursor now gets handled by removing it from the linked list. V0.21, which is the current release, has these changes:\nCLOSE of a cursor is now handled differently. A large trace file showed that Oracle can re-parse the same SQL after a cursor has been CLOSEd and only if the cursor id was never used with a different SQL statement. Instead of PARSING IN CURSOR #1234 â¦ followed by the full SQL text, it simply does PARSE #1234 again with no SQL text! That caused the subsequent EXEC #1234 to abort the program as the cursor wasn\u0026rsquo;t in the linked list - because CLOSE #1234 removed it - and was considered a fatal problem. Data Type 96, NCHAR or NVARCHAR can cause problems as it is possible to output more than one line of hex values. Up until now, I\u0026rsquo;ve only ever seen one line but this is now fixed. The hex values in the second and subsequent lines are simply ignored. ;-) There was a segfault at EOF, only if running in verbose mode and with config.h having set with OFFSETFORRICH = 0. It was when freeing the memory allocated for the SQL statement. The verbose output has been tidied up a bit. Also, if you ever need to extend the MAXBINDS or MAXBINDSIZE, you get a message in the output file, not in the debug file. Just in case you donât have a debug file! Enjoy.\n","description":"","id":26,"section":"posts","tags":null,"title":"Trace Collier Updated Again","uri":"http://localhost:1313/RantsAndRaves/posts/2017/01/traceminer-updated-again/"},{"content":"Trace CollierÂ has been updated after a couple of foibles were found during the processing of a trace file.\nThe version has been bumped to 0.19 as of today, 2nd December 2016. The bugs fixed were:\nThe utility now notices exec ERROR lines as well as PARSE ERRORs. Just because it\u0026rsquo;s nice to know where things might have gone wrong. These are in addition to the PARSE ERRORs that it has been processing up until now. Interesting bug, seemingly related to DBMS_METADATA.GET_CLOB calls where the value for one bind is the bind number of the next one. The text in the trace file is \u0026ldquo;value= Bind#\u0026rdquo; and is weird! It should be on two lines, everything after the equals sign, including the space, should be on the next line. Detected on Windows 11204 and on Solaris 11204. Sort of fixed the problem where a bind can be used more than once in a statement. Flagged in the SQL as \u0026ldquo;A:BIND_REUSED_\u0026rdquo; at the moment. This will be properly fixed in future but where the same bind variable is used more than once in a statement, the code now (sort of) handles it correctly. An issue has been raised on GitHub for this. Added test.cmd. Test harness for Windows users. You can read more about Trace Collier here if you wish.\n","description":"","id":27,"section":"posts","tags":null,"title":"Trace Collier Utility - Updated","uri":"http://localhost:1313/RantsAndRaves/posts/2016/12/traceminer-utility-updated/"},{"content":"I have a table with dates in, and some NULLs. Two people, on the same database, running the same SELECT query, in the same schema, with the same privileges, get vastly differing results. Why? Fine Grained Auditing is not at play here.\nTable names, column names etc have been changed to protect the guilty.\nIn the table in question, the A_DATE column is correctly defined as DATE, rather than anything else unsuitable.\nI have boiled the problem down to the following code. The table I\u0026rsquo;m using has 76 rows of which, 27 are NULLs in the A_DATE column. It took a while to notice the bug in the code though, maybe I should do some more development work?\n1 2 3 select count(\\*) from norm where trim(nvl(a_date, to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;))) = to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;); It looks ok, it does not give any errors, but running it gives inconsistent results depending on the setting of NLS_DATE_FORMAT:\n1 2 3 4 5 6 7 8 9 alter session set nls_date_format=\u0026#39;dd/mm/yyyy\u0026#39;; select count(\\*) from norm where trim(nvl(a_date, to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;))) = to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;); COUNT(\\*) ------- 27 1 2 3 4 5 6 7 8 9 alter session set nls_date_format=\u0026#39;dd-mon-rr\u0026#39;; select count(\\*) from norm where trim(nvl(a_date, to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;))) = to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;); COUNT(\\*) ------- 27 1 2 3 4 5 6 7 8 9 alter session set nls_date_format=\u0026#39;dd-mon-yy\u0026#39;; select count(\\*) from norm where trim(nvl(a_date, to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;))) = to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;); COUNT(\\*) ------- 0 WTH? Zero? Really?\n1 2 3 4 5 6 7 8 9 alter session set nls_date_format=\u0026#39;dd-mon-yyyy\u0026#39;; select count(\\*) from norm where trim(nvl(a_date, to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;))) = to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;); COUNT(\\*) ------- 27 1 2 3 4 5 6 7 8 9 alter session set nls_date_format=\u0026#39;mm-dd-yyyy\u0026#39;; select count(\\*) from norm where trim(nvl(a_date, to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;))) = to_date(\u0026#39;07/04/1960\u0026#39;,\u0026#39;dd/mm/yyyy\u0026#39;); COUNT(\\*) ------- 27 So, what\u0026rsquo;s going on here? Well, it seems from the docs that the TRIM() function is not really supposed to be applied to dates, but Oracle doesn\u0026rsquo;t complain. It returns a VARCHAR2 value, and not a DATE value as the code appears to return.\nThis VARCHAR2 is then compared with a DATE value given on the right side of the \u0026lsquo;=\u0026rsquo;, so there is a bit of implicit conversion going on, and I\u0026rsquo;m positive that the DATE is converted to a VARCHAR2 for the comparison, and this is a bad way to compare DATE values, as VARCHAR2s. After all, 07/04/1960 is bigger than 01/05/2016 isn\u0026rsquo;t it? (No, it isn\u0026rsquo;t, well, not as a DATE, but as a VARCHAR2 \u0026hellip;)\nSome of the other non-null dates in the table are:\n17/03/2016 11/12/2015 02/12/2014 30/10/2014 29/10/2014 02/10/2013 14/10/2009 08/07/2008 03/07/2008 24/06/2008 05/06/2008 The fix? Obvious really, the developer intended to use TRUNC() but mysteriously typed TRIM() instead. Once changed, it \u0026ldquo;just\u0026rdquo; worked - for all known values of NLS_DATE_FORMAT!\n","description":"","id":28,"section":"posts","tags":null,"title":"Interesting Foible with Oracle Dates","uri":"http://localhost:1313/RantsAndRaves/posts/2016/09/interesting-foible-with-oracle-dates/"},{"content":"I recently posted a useful oraenv for Windows utility. This has been updated so that you can run it in batch files by passing the desired Oracle SID on the command line. Details at http://qdosmsq.dunbar-it.co.uk/blog/2016/08/oraenv-for-windows/\n","description":"","id":29,"section":"posts","tags":null,"title":"Oraenv for Windows - Updated","uri":"http://localhost:1313/RantsAndRaves/posts/2016/08/oraenv-for-windows-updated/"},{"content":"Having recently had to learn a whole new way of working when I took on a contract migrating a database to the Windows \u0026ldquo;cloud\u0026rdquo;, I realised that there\u0026rsquo;s no equivalent to the useful Unix oraenv utility. I had to write my own. Give me a bash shell any day!\nThe utility is oraenv.cmd and executes like this:\nset ORATAB=c:\\\\users\\\\ndunbar\\\\oratab ... oraenv Update 30/08/2016: You can now pass the desired SID on the command line and avoid all that prompting stuff! Like this:\noraenv AZDBA01 ... Obviously, the ORATAB environment variable can be set in Control Panel, or in the shell session previously etc. As long as it is set somewhere.\nThe %ORATAB% file needs to look like the following:\nORACLE_SID | ORACLE_HOME | Optional comment text. The default Unix separator of a colon, \u0026lsquo;:\u0026rsquo;, cannot be used here as the ORACLE_HOME field will no doubt have a colon in its name, given that Windows uses it as part of the drive specification. To get around that foible, I use a pipe character - \u0026lsquo;|\u0026rsquo;.\nMy own oratab file looks like this:\nAZDBA01|C:\\\\OracleDatabase\\\\product\\\\11.2.0\\\\dbhome_1|# Staging database AZDBA02|C:\\\\OracleDatabase\\\\product\\\\11.2.0\\\\dbhome_1|# Test clone AZDBA91|C:\\\\OracleDatabase\\\\product\\\\11.2.0\\\\dbhome_1|# Standby for AZDBA01 AZDEV08|C:\\\\OracleDatabase\\\\product\\\\11.2.0\\\\dbhome_1|# Development AZDEV12|C:\\\\OracleDatabase\\\\product\\\\11.2.0\\\\dbhome_1|# Development Comments are not mandatory, but if present, there must be a pipe character - \u0026lsquo;|\u0026rsquo; - after the end of the Oracle Home or the comment becomes part of the %ORACLE_HOME% environment variable if not. Ask me how I know this!\nIn use, an example of the utility\u0026rsquo;s output would be similar to the following:\nC:\\\\Users\\\\ndunbar\u0026gt;oraenv Your session\u0026#39;s current Oracle SID is \u0026#39;azdba02\u0026#39;. Please enter a new Oracle SID from the following list: AZDBA01 AZDBA02 AZDBA91 AZDEV08 AZDEV12 Press ENTER/RETURN to use the current ORACLE_SID. New SID \\[azdba02\\]: azdba91 ORACLE_HOME\\\\bin is already on PATH. ORACLE_SID has been set to \u0026#39;azdba91\u0026#39;. ORACLE_HOME has been set to \u0026#39;C:\\\\OracleDatabase\\\\product\\\\11.2.0\\\\dbhome_1\u0026#39;. NLS_LANG has been set to \u0026#39;AMERICAN_AMERICA.WE8ISO8859P1\u0026#39;. NLS_DATE_FORMAT has been set to \u0026#39;yyyy/mm/dd hh24:mi:ss\u0026#39;. As noted, the current %ORACLE_SID% is the default and pressing RETURN accepts that and leaves things unchanged.\nIf the desired %ORACLE_HOME% is on the %PATH% already, it is not added again. And this identifies a problem.\nIf there are numerous different Oracle Homes in use on the server, then the new one will be added to %PATH% but the old one(s) will not, at present, be removed. I need to find a decent stream editor - like sed - for Windows to allow me the opportunity to do that. However, in my current installations, we have a single Oracle Home for all our databases on the servers, so that problem isn\u0026rsquo;t affecting me at present. Famous last words?\nIn the event of a problem, the following error codes are returned:\n0 = All ok. 1 = ORATAB environment variable not set. 2 = %ORATAB% not pointing to an (accessible) file. 3 = Requested Oracle SID not found in %ORATAB% file. Anyway, here\u0026rsquo;s the code. Copy this and paste into your own oraenv.cmd, somewhere on your %path%, and you are all set to go. Don\u0026rsquo;t forget to set ORATAB first though.\nEnjoy.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 @echo off REM ================================================================= REM Windows version, sort of, of the Unix oraenv script which REM will set the desired Oracle Environment. REM REM Requires the %ORATAB% environment variable pointing at a suitable REM text file, which has the lines set up in the following format: REM REM SID | ORACLE_HOME REM REM We can\u0026#39;t use the Unix default separator of a colon (:) as that REM is used already for the drive specifiers. REM REM EXIT CODES: REM REM 0 = All ok, environment set as requested. REM 1 = Oops! ORATAB env var not set. REM 2 = Oops! %ORATAB% not pointing to an accessible file. REM 3 = Oops! Requested ORACLE_SID not found in %ORATAB% file. REM REM ================================================================= REM Norman Dunbar (norman@dunbar-it.co.uk) REM June 2016. REM REM 30/08/2016 - Added ability to pass SID on command line. REM ================================================================= REM Check if %ORATAB% is already set. Bail out if not. REM This could be set in the System applet for Control Panel, REM or, set in the shell prior to calling this code. if \u0026#34;%ORATAB%\u0026#34;==\u0026#34;\u0026#34; ( echo ORATAB not set. Cannot continue. exit /b 1 ) REM ORATAB needs to point at a file. if NOT exist %ORATAB% ( echo Cannot find the file \u0026#39;%ORATAB%\u0026#39;. Cannot continue. echo Check the value in ORATAB is correctly set, or, that echo the file exists. exit /b 2 ) REM Did we have a SID passed as a parameter? set ORA_SID=%1 REM We don\u0026#39;t do the next bit if we have a SID already. if \u0026#34;%ORA_SID%\u0026#34;==\u0026#34;\u0026#34; ( REM Display the Current ORACLE_SID. echo Your session\u0026#39;s current Oracle SID is \u0026#39;%oracle_sid%\u0026#39;. SET ORA_SID=%oracle_sid% echo. REM List the available SIDs from the oratab file. echo Please enter a new Oracle SID from the following list: for /f \u0026#34;tokens=1 delims=|\u0026#34; %%a in (%ORATAB%) do ( echo %%a ) echo. REM Default to the current SID if the user just presses ENTER. echo Press ENTER/RETURN to use the current ORACLE_SID. SET /P ORA_SID=\u0026#34;New SID \\[%ORA_SID%?\\]: \u0026#34; ) REM Check the %ORATAB% file to see if this ORACLE_SID is listed REM If it\u0026#39;s not then exit to the command prompt with error 3. FIND /I \u0026#34;%ORA_SID%|\u0026#34; %ORATAB% \u0026gt; nul IF NOT %ERRORLEVEL%==0 ( echo Oracle SID not found exit /b 3 ) REM Set the ORACLE_SID. set ORACLE_SID=%ORA_SID% REM Now get the Oracle Home from the oratab file. FOR /F \u0026#34;tokens=2 delims=|\u0026#34; %%a IN (\u0026#39;FIND /I \u0026#34;%ORA_SID%|\u0026#34; %ORATAB%\u0026#39;) DO SET ORACLE_HOME=%%a REM Next thing to do is set the Path. Ok, possible problem area! REM In my own environment, everything has the same ORACLE_HOME so REM there\u0026#39;s no need to worry about removing any other ORACLE_HOME REM from the PATH before adding this one. I need to think about this. echo %PATH% | find /i \u0026#34;%ORACLE_HOME%\\\\bin\u0026#34; \u0026gt; nul if NOT %ERRORLEVEL%==0 ( SET PATH=%ORACLE_HOME%\\\\bin;%PATH% ) else ( echo ORACLE_HOME\\\\bin is already on PATH. ) REM And as a nice little touch we will Print out some details for the user. ECHO ORACLE_SID has been set to \u0026#39;%ORACLE_SID%\u0026#39;. ECHO ORACLE_HOME has been set to \u0026#39;%ORACLE_HOME%\u0026#39;. REM Uncomment this if you want your NLS_LANG and NLS_DATE_FORMATs setting. set NLS_LANG=AMERICAN_AMERICA.WE8ISO8859P1 echo NLS_LANG has been set to \u0026#39;%NLS_LANG%\u0026#39;. set NLS_DATE_FORMAT=yyyy/mm/dd hh24:mi:ss echo NLS_DATE_FORMAT has been set to \u0026#39;%NLS_DATE_FORMAT%\u0026#39;. exit /b 0 @echo on ","description":"","id":30,"section":"posts","tags":null,"title":"Oraenv for Windows","uri":"http://localhost:1313/RantsAndRaves/posts/2016/08/oraenv-for-windows/"},{"content":"It\u0026rsquo;s always nice to know which extra cost Oracle options are enabled, whether deliberately or silently as the result of some patching that has taken place. Keep yourself and your server room cool with blaux wearable ac.\nUpdated: 25th August 2017 to list DLL names for Oracle 12c.\nCopy and paste the code below into a Windows command file named - in my case - checkChopyOptions.cmd and execute it against any Oracle Home. It is called as follows:\n1 checkChoptOptions [Oracle_home\\] The Oracle Home is optional and if omitted, will check the currently set %ORACLE_HOME% location.\nThis version has been tested on a Windows 2012 server running Oracle 11.2.0.4.\nThe script displays output on the screen and also to a logfile in the script directory. If you see that an option is both enabled and disabled, beware - someone has executed chopt, probably without adminstrator rights and may well have created an empty *.dll.dbl file. You should probably delete the *.dll.dbl and do a proper chopt disable as admninistrator.\nEnjoy.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 @echo off Rem ================================================================ Rem | Check an existing Oracle Home for expensive EE Options | Rem | | Rem | Norman Dunbar. | Rem | August 2016. | Rem ================================================================ Rem Rem Rem ================================================================ Rem | USAGE: | Rem | | Rem | checkChoptOptions [oracle_home\\] | Rem | | Rem ================================================================ Rem setlocal EnableDelayedExpansion Rem ================================================================ Rem | Internal Variables. | Rem ================================================================ set VERSION=1.00 set ERRORS=0 set MYLOG=.\\\\%0.log set ORA_HOME=%1 Rem ================================================================ Rem | The next two define the first and last entry in the following | Rem | \u0026#34;arrays\u0026#34; which are not really arrays, honest! | Rem ================================================================ set FirstEntry=0 set LastEntry=6 Rem ================================================================ Rem | The following \u0026#34;arrays\u0026#34; are not arrays. They look like they are | Rem | but are actually just a pile of scalar variables with \u0026#39;[n\\]\u0026#39; in | Rem | their name. Oh, and we have to use \u0026#39;!variable_name! later on | Rem | too for some unfathomable reason. | Rem ================================================================ Rem ================================================================ Rem | List of Oracle chopt\u0026#39;able options. | Rem ================================================================ set Option[0\\]=Partitioning set Option[1\\]=OLAP set Option[2\\]=Label Security set Option[3\\]=Data Mining set Option[4\\]=Database Vault option set Option[5\\]=Real Application Testing set Option[6\\]=Database Extensions for .NET Rem ================================================================ Rem | List of DLLs that exist for enabled options. | Rem ================================================================ set Enabled[0\\]=oraprtop11.dll set Enabled[1\\]=oraolapop11.dll set Enabled[2\\]=oralbac11.dll set Enabled[3\\]=oradmop11.dll set Enabled[4\\]=oradv11.dll set Enabled[5\\]=orarat11.dll set Enabled[6\\]=clr Rem ================================================================ Rem | List of DLLs that exist for disabled options. | Rem ================================================================ set Disabled[0\\]=oraprtop11.dll.dbl set Disabled[1\\]=oraolapop11.dll.dbl set Disabled[2\\]=oralbac11.dll.dbl set Disabled[3\\]=oradmop11.dll.dbl set Disabled[4\\]=oradv11.dll.dbl set Disabled[5\\]=orarat11.dll.dbl set Disabled[6\\]=clr.dbl Rem ================================================================ Rem | Clear any existing logfile. | Rem ================================================================ Rem del %MYLOG% \u0026gt; nul 2\u0026gt;\u0026amp;1 call :log %0 - v%VERSION% : Logging to %MYLOG% call :log Executing: %0 %* :check_oracle_home if \u0026#34;%ORA_HOME%\u0026#34; EQU \u0026#34;\u0026#34; ( set ORA_HOME=%ORACLE_HOME% ) if \u0026#34;%ORACLE_HOME%\u0026#34; EQU \u0026#34;\u0026#34; ( call :log ORACLE_HOME is not defined. set ERRORS=1 ) if not exist %ORA_HOME% ( call :log ORACLE_HOME \u0026#34;%ORA_HOME%\u0026#34; - not found. set ERRORS=1 ) :check_errors if %ERRORS% EQU 1 ( call :log Cannot continue - too many errors. goto :eof ) Rem ******************************************************************* Rem ******************************************************************* :JDI call :log Checking ORACLE_HOME = \u0026#34;%ORA_HOME%\u0026#34;. for /L %%f in (%FirstEntry%, 1, %LastEntry%) do ( Rem Is this option enabled? if exist %ORA_HOME%\\\\bin\\\\!Enabled[%%f\\]! ( call :log !Option[%%f\\]! - is currently enabled. ) Rem Also check if it is disabled too. This needs investigating. if exist %ORA_HOME%\\\\bin\\\\!Disabled[%%f\\]! ( call :log !Option[%%f\\]! - is currently disabled. ) ) Rem ******************************************************************* Rem ******************************************************************* Rem ================================================================ Rem | And finally, turn off the \u0026#34;doing it correctly\u0026#34; setting. | Rem | And skip over the sub-routines. | Rem ================================================================ Rem call :log %0 - complete. endlocal exit /b Rem ================================================================ Rem | LOG() | Rem ================================================================ Rem | Set up a logging procedure to log output to the %MYLOG% file. | Rem | Each line is yyyy/mm/dd hh:mi:ss: | Rem ================================================================ :log echo %* echo %date:~6,4%/%date:~3,2%/%date:~0,2% %time:~0,8%: %* \u0026gt;\u0026gt; %MYLOG% goto :eof A test run gives the following, on screen:\n1 2 3 4 5 6 7 8 9 10 11 12 C:\\\\Users\\\\ndunbar\\\\Desktop\u0026gt;CheckOracleOptions.cmd CheckOracleOptions.cmd - v1.00 : Logging to .\\\\CheckOracleOptions.cmd.log Executing: CheckOracleOptions.cmd Checking ORACLE_HOME = \u0026#34;C:\\\\OracleDatabase\\\\product\\\\11.2.0\\\\dbhome_1\u0026#34;. Partitioning - is currently disabled. OLAP - is currently disabled. Label Security - is currently disabled. Data Mining - is currently disabled. Database Vault option - is currently disabled. Real Application Testing - is currently enabled. Database Extensions for .NET - is currently disabled. CheckOracleOptions.cmd - complete. And the logfile looks like this:\n2016/08/25 15:29:58: CheckOracleOptions.cmd - v1.00 : Logging to .\\\\CheckOracleOptions.cmd.log 2016/08/25 15:29:58: Executing: CheckOracleOptions.cmd 2016/08/25 15:29:58: Checking ORACLE_HOME = \u0026#34;C:\\\\OracleDatabase\\\\product\\\\11.2.0\\\\dbhome_1\u0026#34;. 2016/08/25 15:29:58: Partitioning - is currently disabled. 2016/08/25 15:29:58: OLAP - is currently disabled. 2016/08/25 15:29:58: Label Security - is currently disabled. 2016/08/25 15:29:58: Data Mining - is currently disabled. 2016/08/25 15:29:58: Database Vault option - is currently disabled. 2016/08/25 15:29:58: Real Application Testing - is currently enabled. 2016/08/25 15:29:58: Database Extensions for .NET - is currently disabled. 2016/08/25 15:29:58: CheckOracleOptions.cmd - complete. Oracle 12c Changes If you are running Oracle 12c on Windows, then the code above needs a couple of minor changes to cope. Oracle have, as usual, changed the names of the various DLLs that need to be checked.\nChange the appropriate parts of the above code, to the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Rem ================================================================ Rem | List of DLLs that exist for enabled options. | Rem ================================================================ set Enabled[0\\]=oraprtop12.dll set Enabled[1\\]=oraolapop12.dll set Enabled[2\\]=oralbac12.dll set Enabled[3\\]=oradmop12.dll set Enabled[4\\]=oradv12.dll set Enabled[5\\]=orarat12.dll set Enabled[6\\]=clr Rem ================================================================ Rem | List of DLLs that exist for disabled options. | Rem ================================================================ set Disabled[0\\]=oraprtop12.dll.dbl set Disabled[1\\]=oraolapop12.dll.dbl set Disabled[2\\]=oralbac12.dll.dbl set Disabled[3\\]=oradmop12.dll.dbl set Disabled[4\\]=oradv12.dll.dbl set Disabled[5\\]=orarat12.dll.dbl set Disabled[6\\]=clr.dbl That\u0026rsquo;s all.\n","description":"","id":31,"section":"posts","tags":null,"title":"Which Extra Cost Oracle Options is my Windows Server Running?","uri":"http://localhost:1313/RantsAndRaves/posts/2016/08/which-extra-cost-oracle-options-is-my-windows-server-running/"},{"content":"I found a broken check constraint, one that simply wouldn\u0026rsquo;t work, on a database. It was created as:\n1 ... CHECK(COLUMN_NAME IN (\u0026#39;Y\u0026#39;,\u0026#39;N\u0026#39;,NULL)) ; Try it yourself, it doesn\u0026rsquo;t work! Anyway, I needed to find if there were any other check constraints broken in this manner, so I did the following:\n1 2 3 4 5 6 7 8 9 select owner, table_name, constraint_name, to_lob(search_condition) search_condition from dba_constraints where owner = \u0026#39;XXXXX\u0026#39; and constraint_type = \u0026#39;C\u0026#39; and upper(search_condition) like \u0026#39;%IN%,%NULL%\u0026#39; order by 1,2,3; Of course, that barfed because the SEARCH_CONDITION column is a LONG data type. Sigh! I thought those things were deprecated! Never mind, I did this next:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 -- Can\u0026#39;t filter search_condition as it\u0026#39;s a LONG data type. create global temporary table check_constraints on commit preserve rows as ( select owner, table_name, constraint_name, search_condition from dba_constraints where owner = \u0026#39;XXXXX\u0026#39; and constraint_type = \u0026#39;C\u0026#39; ) ; select \\* from check_constraints where upper(search_condition) like \u0026#39;%IN%,%NULL%\u0026#39;; -- Do other meaningful stuff here .... -- Then eventually ... drop table check_constraints; But the DROP TABLE command resulted in an error ORA-14452: attempt to create, alter or drop an index on temporary table already in use. Hmm!\nFirst of all, I had no indexes, so the message is only slightly misleading, but regardless, I couldn\u0026rsquo;t drop my temporary table when I was finished with it.\nThe solution is amazingly simple:\n1 truncate table check_constraints; After that, dropping the table \u0026ldquo;just works\u0026rdquo;.\nAnd yes, there were quite a few broken check constraints. Duvelopers!\nWhat\u0026rsquo;s Broken? Well, if you create a check constraint as per the one listed back at the start of this eRant, you will not see any errors from Oracle. Nor will you see any errors when you INSERT or UPDATE rows with invalid values in the column. It\u0026rsquo;s that NULL thing that kills your constraint. It simply means that any value you have in the column will be accepted.\n1 2 3 4 5 6 7 8 9 10 11 drop table test cascade constraints purge; create table test(a varchar2(1)); alter table test add constraint chk_a check (a in (\u0026#39;Y\u0026#39;,\u0026#39;N\u0026#39;, NULL)); insert into test(a) values (\u0026#39;N\u0026#39;); insert into test(a) values (\u0026#39;Y\u0026#39;); insert into test(a) values (NULL); insert into test(a) values (\u0026#39;T\u0026#39;); Huh? That last one couldn\u0026rsquo;t have worked, could it?\n1 2 3 4 5 6 7 8 select A from test; A -- N Y T Yup, the constraint is indeed useless.\nHave fun!\n","description":"","id":32,"section":"posts","tags":null,"title":"Dropping Temporary Tables  (With Bonus, Broken Check Constraints!)","uri":"http://localhost:1313/RantsAndRaves/posts/2016/08/dropping-temporary-tables-with-bonus-broken-check-constraints/"},{"content":"This applies to Linux, Unix as well as Windows, but affected me on a Windows 2012 Server running Oracle 11.2.0.4 Enterprise Edition.\nMy user on the server was an administration user, but not in the ora_dba group. This is required to connect / as sysdba within SQL*Plus. The SYS password had been changed recently but whoever did it, did not update the password vault. The users were urgently requiring their database be started, I was the only DBA in the office, the SYS password was unknown, and my user didn\u0026rsquo;t belong directly to the ora_dba group. What to do?\nIt\u0026rsquo;s not quite the dreadful hack that the title of this post may indicate. Depending on the setup for the server, you may need administrator rights to move files around. Plus, most importantly, you do need to know the SYS password for at least one database on the server.\nMy user account was indirectly a member of the ora_dba group, via my administrator rights, but it seems I need to be directly a member of the group to login / as sysdba.\nThat said, the short, bullet point method is as follows:\nCd %ORACLE_HOME%\\database set ORACLE_SID=dbadb01. Rename the current password file pwddbadb01.ora to pwddbadb01.ora.keep. Copy another password file, for which I did know the SYS password, to pwddbadb01.ora. Sqlplus sys/known_password as sysdba. startup. exit. Delete pwddbadb01.ora. Rename pwddbadb01.ora.keep to pwddbadb01.ora. This way I got the database started, the users were happy, and I made sure I got the password vault updated to save me this grief next time!\n","description":"","id":33,"section":"posts","tags":null,"title":"How to Start an Oracle Database When You Are Not in the DBA Group","uri":"http://localhost:1313/RantsAndRaves/posts/2016/06/how-to-start-an-oracle-database-when-you-are-not-in-the-dba-group/"},{"content":"For no reason, after many weeks of use, RMAN suddenly cannot connect:\nrman target sys/******@dbadb01 catalog ... ... RMAN-00571: =========================================================== RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS =============== RMAN-00571: =========================================================== RMAN-00601: fatal error in recovery manager RMAN-03010: fatal error during library cache pre-loading RMAN-10038: database session for channel default terminated unexpectedly Setting debug and trace on the command line has no effect, there is nothing of use in the trace file.\nrman target sys/****** debug all trace=trace.log The contents of trace.log after this were as follows, which is pretty much normal, except for the error messages at the end. No help at all in other words.\nDBGMISC: ENTERED krmksimronly [09:55:45.190] ... Calling krmmpem from krmmmai RMAN-00571: =========================================================== RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS =============== RMAN-00571: =========================================================== RMAN-00601: fatal error in recovery manager RMAN-03010: fatal error during library cache pre-loading RMAN-10038: database session for channel default terminated unexpectedly Setting event 10046 on the entire database with alter system ... and/or running a session 10046 trace for SYS logins using a database trigger also revealed nothing. Not even a trace file for any RMAN sessions.\nMuch searching with Google resolved nothing. My MOS account is not connected yet, despite requests, to the current support identifier, so I can\u0026rsquo;t go hunting for a fix on MOS. And the admins are off today as well. However, this was a subtle clue:\nset oracle_sid=dbadb01 rman target / catalog ... ... connected to target database: dbadb02 (DBID=1170775433) ... Really? Dbadb02 02? I asked for 01, so what\u0026rsquo;s going on?\nThe database, dbadb02, was cloned, using RMAN yesterday. It should have a new DBID and such like, so let\u0026rsquo;s check:\ntnsping dbadb01 Used TNSNAMES adapter to resolve the alias Attempting to contact (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = test_server)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = dbadb01))) OK (20 msec) So far so good, next:\ntnsping dbadb02 Used TNSNAMES adapter to resolve the alias Attempting to contact (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = test_server)(PORT = 1521)) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = dbadb02))) OK (20 msec) That looks fine too. Next, check the database:\nset oracle_sid=dbadb02 sqlplus / as sysdba select name,value from v$parameter where lower(value) like \u0026#39;%dbadb01%\u0026#39;; NAME\tVALUE ------------------------------------------------------ instance_name dbadb01 service_names dbadb01 dispatchers (PROTOCOL=TCP) (SERVICE=dbadb01XDB) audit_file_dest\tC:\\ORACLEDATABASE\\ADMIN\\dbadb01\\ADUMP Bingo! RMAN doesn\u0026rsquo;t change these parameters after a clone, so the database needs fixing. It looks like RMAN is attempting to connect to the dbadb02 database with the service name of dbadb01, rather than connecting to dbadb01 on that service name.\nThere was supposed to be a script executed by the clone script, to fixup those parameters but obviously it didn\u0026rsquo;t work, or some other fault occurred (The DBA responsible will getting a slapped wrist shortly!) - log files will need to be checked!\nA quick fix later:\n1 2 3 4 5 6 7 8 9 10 alter system set instance_name=\u0026#39;dbadb02\u0026#39; scope=spfile; alter system set service_names=\u0026#39;dbadb02\u0026#39; scope=spfile; alter system set dispatchers=\u0026#39;(PROTOCOL=TCP) (SERVICE=dbadb02XDB)\u0026#39; scope=spfile; alter system set audit_file_dest=\u0026#39;C:\\ORACLEDATABASE\\ADMIN\\dbadb02\\ADUMP\u0026#39; scope=spfile; startup force ... select name,value from v$parameter where lower(value) like \u0026#39;%dbadb01%\u0026#39;; no rows selected And now, does RMAN work?\nrman target sys/******@dbadb01 ... connected to target database: dbadb01 (DBID=673233917) Result!\n","description":"","id":34,"section":"posts","tags":null,"title":"RMAN Connection Troubles, RMAN-03010 \u0026 RMAN-10038","uri":"http://localhost:1313/RantsAndRaves/posts/2016/06/rman-connection-troubles-rman-03010-rman-10038/"},{"content":"I\u0026rsquo;ve recently begun a new contract migrating a Solaris 9i database to Oracle 11gR2 on Windows, in the Azure cloud. I hate windows with a vengeance and this hasn\u0026rsquo;t made me change my opinion!\nOne of the planned improvements is to have everyone using a standard, central tnsnames.ora file for alias resolution. A good plan, and the company has incorporated my own tnsnames checker utility to ensure that any edits are valid and don\u0026rsquo;t break anything.\nI found that the tnsnames.ora in my local Oracle Client install, was not working. Here\u0026rsquo;s what I had to do to fix it.\nIn my local tnsnames.ora, I had something like the following:\nIFILE=\u0026#34;\\\\servername\\share_name\\central_tnsnames\\tnsnames.ora\u0026#34; (Server names etc have been obfuscated to protect the innocent!)\nHowever, using the above caused tnsping commands, or connection attempts to time out or simply fail:\ntnsping barney TNS Ping Utility for 64-bit Windows: Version 11.2.0.1.0 - Production on 19-MAY-2 016 12:16:42 ... TNS-03505: Failed to resolve name If the standard tnsnames.ora file was copied locally, and IFILE\u0026rsquo;d, then it all just worked as expected.\nThe problem is simple, Oracle isn\u0026rsquo;t fond of IFILEing files from networked drives. So, to get around this, I needed to map a network drive instead, and use the drive specifier in my IFILE.\nFirst map a persistent network drive to be my (new) Y: drive. This should be reconnected at logon until further notice. Note that this mapping uses my current credentials to make the connection.\n1 net use Y: \\\\servername\\share_name /PERSISTENT:YES And in my tnsnames.ora, I now have this:\nIFILE=\u0026#34;Y:\\central_tnsnames\\tnsnames.ora\u0026#34; And now, it all just works!\nC:\\Users\\ndunbar\\Downloads\u0026gt;tnsping barney TNS Ping Utility for 64-bit Windows: Version 11.2.0.1.0 - Production on 19-MAY-2 016 12:21:23 ... Used TNSNAMES adapter to resolve the alias Attempting to contact (DESCRIPTION= (ADDRESS= (PROTOCOL=TCP) (HOST=bedrock) (PORT=1521)) (CONNECT_DATA= (SERVER=dedicated) (SERVICE_NAME=barney))) OK (160 msec) HTH\n","description":"","id":35,"section":"posts","tags":null,"title":"Tnsnames.ora, IFILE and Network Drives on Windows","uri":"http://localhost:1313/RantsAndRaves/posts/2016/05/tnsnames-ora-ifile-and-network-drives-on-windows/"},{"content":"As a contractor I often have to fill in and sign various contract agreements. These are usually tens of pages in length, and while I only have to sign one page, I still must scan in and send back each and every page, even the untouched ones. It would help if a PDF with form filling abilities was supplied, but hey, that\u0026rsquo;s only rarely the case. This is how I do it. (On Linux - Windows users mileage may vary!)\nFirst of all, install pdftk. You only have to do this once.\n1 2 sudo apt-get update sudo apt-get install pdftk Your commands may vary, I\u0026rsquo;m using Linux Mint 17.3 at the time of writing. Use whatever your package manager requires.\nNext, create a couple of useful utility scripts to split a large PDF file into separate pages, and another to join the pages back up into one file again.\nSplit Out the Pages The script to split an input PDF file into separate pages is called splitPDF.sh, and is as follows. Please be aware that there is very limited error checking - I\u0026rsquo;m supposed to know what I\u0026rsquo;m doing! (Famous last words!)\n1 2 3 4 5 6 7 8 9 10 11 12 13 ORIGINALPDF=\u0026#34;${1}\u0026#34; PAGESPDF=\u0026#34;${ORIGINALPDF%%pdf}page_%04d.pdf\u0026#34; echo \u0026#34;Processing \\\u0026#34;${ORIGINALPDF}\\\u0026#34; to \\\u0026#34;${PAGESPDF}\\\u0026#34;\u0026#34; pdftk \u0026#34;${ORIGINALPDF}\u0026#34; \\ burst \\ output \u0026#34;${PAGESPDF}\u0026#34; echo \u0026#34; \u0026#34; echo \u0026#34;Pages created.\u0026#34; echo \u0026#34;Copy/Overwrite replacement pages with same file name, then,\u0026#34; echo \u0026#34;./joinPDF.sh \\\u0026#34;${ORIGINALPDF}\\\u0026#34;.\u0026#34; echo \u0026#34; \u0026#34; What this code does is accepts the full PDF file name on the command line, wrapped in double quotes if there are spaces or other special characters, and splits it into separate pages. If, for example, the file is called Contract.pdf, running the script as ./splitPDF.sh Contract.pdf will produce a number of files named Contract.page_0001.pdf, Contract.page_0002.pdf and so on until there is a separate file for each page of the original document.\nAll Join Together To join the separate pages back together again is dependent on the following script, named joinPDF.sh:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ORIGINALPDF=\u0026#34;${1}\u0026#34; UNCOMPRESSEDPDF=\u0026#34;${ORIGINALPDF%%pdf}uncompressed.pdf\u0026#34; COMPLETEDPDF=\u0026#34;${ORIGINALPDF%%pdf}completed.pdf\u0026#34; PAGESPDF=\u0026#34;${ORIGINALPDF%%pdf}\u0026#34; echo \u0026#34;Processing \\\u0026#34;${PAGESPDF}page_*.pdf\\\u0026#34; to \\\u0026#34;${UNCOMPRESSEDPDF}\\\u0026#34;\u0026#34; pdftk \u0026#34;${PAGESPDF}\u0026#34;page_*.pdf \\ output \u0026#34;${UNCOMPRESSEDPDF}\u0026#34; echo \u0026#34;\\\u0026#34;${UNCOMPRESSEDPDF}\\\u0026#34; created.\u0026#34; echo \u0026#34;Compressing... \u0026#34; pdftk \u0026#34;${UNCOMPRESSEDPDF}\u0026#34; \\ output \u0026#34;${COMPLETEDPDF}\u0026#34; \\ compress echo \u0026#34;\\\u0026#34;${COMPLETEDPDF}\\\u0026#34; created.\u0026#34; Again, this is executed with the original file name used when splitting the pages. For example, with our example above, we would run the following command ./joinPDF.sh Contract.pdf. This will take all the pages named Contract.page_000n.pdf, in order, and create a temporary uncompressed file (in some cases, but not all) named Contract.uncompresses.pdf. This file will be compressed into the final output file, named Contract.completed.pdf.\nThe scripts will not overwrite the original files, just in case, and all temporary files are left around afterwards as well. You never know.\nIn Use Using the utilities above is simple. The workflow, if it can be called such, is as follows:\nSave a copy of the master file somewhere temporary. Split out the individual pages. Print, sign, scan the pages that must be signed, etc. Copy the newly scanned pages over the originals. Join everything back together. So, here\u0026rsquo;s a worked example from my latest contract which required 13 pages to be returned with only page 12 requiring a signature.\n1 2 3 norman@hubble $ ### List current files: norman@hubble $ ls Contract.pdf joinPDF.sh splitPDF.sh 1 2 3 4 5 norman@hubble $ ### Split Contract.pdf into pages: norman@hubble $ ./splitPDF.sh Contract.pdf Processing \u0026#34;Contract.pdf\u0026#34; to \u0026#34;Contract.page_%04d.pdf\u0026#34; Pages created. Copy/Overwrite replacement pages with same file name, then:\n1 ./joinPDF.sh \u0026#34;Contract.pdf\u0026#34;. 1 2 3 4 5 6 7 8 norman@hubble $ ### Check result: norman@hubble $ ls Contract.page_0001.pdf Contract.page_0007.pdf Contract.page_0013.pdf Contract.page_0002.pdf Contract.page_0008.pdf Contract.pdf Contract.page_0003.pdf Contract.page_0009.pdf doc_data.txt Contract.page_0004.pdf Contract.page_0010.pdf joinPDF.sh Contract.page_0005.pdf Contract.page_0011.pdf splitPDF.sh Contract.page_0006.pdf Contract.page_0012.pdf 1 2 norman@hubble $ ### Overwrite original page 12 with signed \u0026amp; scanned page 12: norman@hubble $ cp /tmp/Contract_page_12.pdf ./Contract.page_0012.pdf 1 2 3 4 5 6 norman@hubble $ ### Merge originals plus signed page: norman@hubble $ ./joinPDF.sh Contract.pdf Processing \u0026#34;Contract.page_*.pdf\u0026#34; to \u0026#34;Contract.uncompressed.pdf\u0026#34; \u0026#34;Contract.uncompressed.pdf\u0026#34; created. Compressing... \u0026#34;Contract.completed.pdf\u0026#34; created. 1 2 3 4 5 6 7 8 9 norman@hubble $ ### Check result: norman@hubble $ ls Contract.completed.pdf Contract.page_0007.pdf Contract.pdf Contract.page_0001.pdf Contract.page_0008.pdf Contract.uncompressed.pdf Contract.page_0002.pdf Contract.page_0009.pdf doc_data.txt Contract.page_0003.pdf Contract.page_0010.pdf joinPDF.sh Contract.page_0004.pdf Contract.page_0011.pdf splitPDF.sh Contract.page_0005.pdf Contract.page_0012.pdf Contract.page_0006.pdf Contract.page_0013.pdf And finally, I can email the file Contract.completed.pdf back to the agent and I\u0026rsquo;m ready to start work. :-)\n","description":"","id":36,"section":"posts","tags":null,"title":"Printing, Completing \u0026 Scanning PDF Documents","uri":"http://localhost:1313/RantsAndRaves/posts/2016/03/printing-completing-scanning-pdf-documents/"},{"content":"If you find that your Raspberry Pi \u0026ndash; those with built in WiFi and Bluetooth \u0026ndash; loses the WiFi connection after a period of inactivity, then this thread on the Raspberry Pi Forums, which will open in a new tab, might be of interest. Have a read. If you want to miss out on the preliminaries of the thread, start reading here instead.\nThe problem is likely to be the power saving mode of the Wifi. To check the current setting:\n1 sudo iw dev wlan0 get power_save If you are/will be affected, it will reply with:\npower_save: on If so, all you need to do is:\n1 sudo iw dev wlan0 set power_save off I\u0026rsquo;m not affected yet, but I\u0026rsquo;m making a note here, just in case!\nRaspberry Pi Zero 2W I received a new Raspberry Pi Zero 2W as a gift from the MagPi Magazine. I decided to upgrade the Zero W that was running my puppy monitoring camera. Everything was fine running Buster (I simply swapped over the microSD card) except power_save mode was constantly turned back on after a reboot on the Zero 2W, but not on the Zero W. Weird!\nThis caused the Zero 2W to drop off the network and sometimes pinging it constantly would kick it back on, but other times a reboot was required (it runs headless).\nAdding the following line to /etc/network/interfaces.d/wlan0.conf solved the problem.\nwireless-power off This appears to have worked. At least, after a reboot the power saving mode is disabled. My config file looks like this now:\nauto wlan0 allow-hotplug wlan0 iface wlan0 inet manual wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf wireless-power off ","description":"","id":37,"section":"posts","tags":null,"title":"Does Your Raspberry Pi 3 Lose WiFi Connections After a While?","uri":"http://localhost:1313/RantsAndRaves/posts/2016/03/does-your-raspberry-pi-3-lose-wifi-connections-after-a-while/"},{"content":"Have you ever needed to trawl through an Oracle Trace file to extract the SQL statements executed and found a whole load of bind variables have been used, so you need to find the BINDS section, extract the values, and virtually paste them into the parsed SQL statement?\nNo? This utility isn\u0026rsquo;t for you then.\nUpdate to version 0.16 and you too can compile and run this useful utility on Windows. See the Readme for details.\nTrace Collier As of 3rd March, after a Cease and Desist letter from a German lawyer, https://www.wuesthoff.de, acting on behalf of their client, Synaxus, this utility has a new name, Trace Collier. It appears that Synaxus has an unrelated software product with a very similar name to my old name, and have registered it as a trade mark.Â Their product can be found at https://www.synaxus.de/index.php/en/traceminer/tm-40 why not take a look?\nNote: Updated 2nd December 2016 for version 0.19.\nTrace Collier, as it is now known, is available for download as source code, from my Git Hub repository. Click on the Download Zip button to get hold of it, then simply unzip it, cd to the created folder, edit the config.h file to suit your system, and then execute make to build the Trace CollierÂ utility.\nThe README files (either markdown or HTML) have all the details.\nSo, given a trace file - which must have binds (10046, level 4 or 12 etc), the output will look something like this:\nTrace Collier: Version 0.12 Processing: Trace file /full/path/to/nfpdpr_ora_16153.trc --------------------------------------------------------------------------------------------------------------------------------------- EXEC Line : Cursor ID : PARSE Line : SQL Text with binds replaced --------------------------------------------------------------------------------------------------------------------------------------- 7555 : : : COMMIT 7556 : #140136345356328: 7477 : INSERT INTO \u0026#34;U_NFP\u0026#34;.\u0026#34;NFP_LW_MEASURED\u0026#34; (\u0026#34;SPECIES_RUN_ID\u0026#34;,\u0026#34;LENGTH\u0026#34;,\u0026#34;WEIGHT\u0026#34;,\u0026#34;SCALE_PACKET\u0026#34;,\u0026#34;CHARACTERISTIC_ID\u0026#34;,\u0026#34;AGE_BAND_ID\u0026#34;,\u0026#34;PRE_FIRST_SPAWN_ID\u0026#34;,\u0026#34;POST_FIRST_SPAWN_ID\u0026#34;,\u0026#34;TOTAL_SEA_AGE_ID\u0026#34;,\u0026#34;NUMBER_MARKS\u0026#34;,\u0026#34;NALL_AGE\u0026#34;,\u0026#34;TRAP_NUMBER\u0026#34;,\u0026#34;LW_MEASURED_COMMENT\u0026#34;) VALUES (664499,186,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL,NULL) 7581 : : : COMMIT 7582 : #140136345356328: 7568 : select 00016EF5.0015.0006 from dual 7619 : : : COMMIT ... So far, it has handled all the trace files I\u0026rsquo;ve thrown at it, but if yours breaks or doesn\u0026rsquo;t produce the correct results, give me a shout. My email is in the README.\nThere\u0026rsquo;s an option to run Trace CollierÂ in --verbose (or just -v) mode. Don\u0026rsquo;t! You have been warned. However, if you do (and you really shouldn\u0026rsquo;t!) make sure to redirect stderr to a file which will get very very very big.\n","description":"","id":38,"section":"posts","tags":null,"title":"Trace Collier - An Oracle Utility to Mine 10046 Trace Files","uri":"http://localhost:1313/RantsAndRaves/posts/2016/02/traceminer-an-oracle-utility-to-mine-10046-trace-files/"},{"content":"I hate it when I click on a task bar icon to open up a snoozing application, and a couple of seconds later, there\u0026rsquo;s a preview of the application showing at the bottom of the screen. I don\u0026rsquo;t need it as the icon tells me what it is, thanks! Getting rid is easy:Â Updated 25th February 2021 after Mint 20 upgrade.\nCinnamon 20.x Right-click the task bar on a blank space. Enable the switch to allow \u0026lsquo;Panel Edit Mode\u0026rsquo; Right-click any task\u0026rsquo;s icon in the Task Bar. Select \u0026lsquo;Configure\u0026rsquo;. Click on the \u0026lsquo;Thumbnails\u0026rsquo; tab. Deselect \u0026lsquo;Show thumbnails\u0026rsquo;. Close the Configuration Dialogue. Right-click the task bar, on a blank area again, and disable \u0026lsquo;Panel Edit Mode\u0026rsquo;. The Cinnamon 17.3 options still get you to the same place, but unfortunately, there doesn\u0026rsquo;t appear to be a way to unlock the \u0026lsquo;Window List\u0026rsquo; option to allow it to be changed. Sigh!\nYou can also turn off the application grouping as well. This is on the \u0026lsquo;General\u0026rsquo; tab in the Configuration Dialogue, disable the \u0026lsquo;Group windows by application\u0026rsquo; option.\nCinnamon 17.3 - 19.x Click the Menu button. (Mint\u0026rsquo;s equivalent of the \u0026lsquo;start\u0026rsquo; button in some other operating systems!) Choose \u0026lsquo;Preferences\u0026rsquo;. Choose \u0026lsquo;Applets\u0026rsquo;. Scroll down, or search, for \u0026lsquo;Window List\u0026rsquo;. Select it, click the \u0026lsquo;configure\u0026rsquo; button. Deselect \u0026lsquo;Show thumbnails on hover\u0026rsquo;. You would think, as I did, that the setting might be found within the System Tray applet\u0026rsquo;s configuration, but you will find otherwise!\n","description":"","id":39,"section":"posts","tags":null,"title":"Turn Off Task Bar Thumbnail Previews on Linux Mint Cinnamon 17.3 through 20.1","uri":"http://localhost:1313/RantsAndRaves/posts/2016/02/turn-off-task-bar-thumbnail-previews-on-linux-mint-cinnamon-17-3/"},{"content":"With a title like that, I should get some hits! ;-)\nNote: Updated 6th October 2018 to cover Raspbian Stretch which is slightly different to Jessie. But not much.\nNote: Updated 8th February 2016 to cover the new Raspberry Pi 3 with built in WiFi and Bluetooth.\nNote: Checked 20th October 2017 to make sure that the instructions below still apply, and work, on the new stretch release of Raspbian. They do!\nSince Jessie came along, networking have changed slightly, and there are numerous people having problems getting Jessie to connect to the internet, or just to WiFi. This might help. Oh, by the way, this is only a problem if the package named \u0026ldquo;raspberrypi-net-mods\u0026rdquo; has been installed - or so it seems.\nIf the above package has been installed, at least it backs up your old config files in /etc/network/*-old. So you can revert, if necessary. However, we want to use the latest and greatest, so read on.\nThere are a couple of files that need to be updated:\nFile /etc/network/interfaces (Jessie Only) If you are on Raspbian Stretch, see the next section. This section applies to Jessie only.\nThis file should resemble the following:\n1 2 3 4 5 6 7 8 9 10 11 auto lo iface lo inet loopback auto eth0 allow-hotplug eth0 iface eth0 inet manual auto wlan0 allow-hotplug wlan0 iface wlan0 inet manual wpa-conf /etc/wpa\\_supplicant/wpa\\_supplicant.conf In case you are wondering, the \u0026ldquo;auto\u0026rdquo; lines mean that the Pi will attempt to bring up these networks at boot time. If you don\u0026rsquo;t want this, remove the lines and use sudo ifup eth0 or sudo ifup wlan0 to bring them up manually.\nIt\u0026rsquo;s probably best not to touch the \u0026ldquo;lo\u0026rdquo; device though, just in case! ;-)\nFile /etc/network/interfaces (Stretch Only) If you are on Raspbian Jessie, see the previous section. This section applies to Stretch only.\nThis file should resemble the following:\n1 2 3 4 5 6 7 # interfaces(5) file used by ifup(8) and ifdown(8) # Please note that this file is written to be used with dhcpcd # For static IP, consult /etc/dhcpcd.conf and \u0026#39;man dhcpcd.conf\u0026#39; # Include files from /etc/network/interfaces.d source-directory /etc/network/interfaces.d So, that\u0026rsquo;s a little different to Jessie. What it means is that instead of filling the interfaces file with our settings, we need separate files for each device. (Ok, we could put them all in the same file, but keep different devices apart I say!)\nCreate the following files, and enter the text as shown, with whatever minor adjustments you need for your system.\n/etc/network/interfaces.d/lo.conf That\u0026rsquo;s Ell, Oh.conf by the way, in lower case.\n1 2 auto lo iface lo inet loopback /etc/network/interfaces.d/eth0.conf 1 2 3 auto eth0 allow-hotplug eth0 iface eth0 inet manual /etc/network/interfaces.d/wlan0.conf 1 2 3 4 auto wlan0 allow-hotplug wlan0 iface wlan0 inet manual wpa-conf /etc/wpa\\_supplicant/wpa\\_supplicant.conf In case you are wondering, the \u0026ldquo;auto\u0026rdquo; lines mean that the Pi will attempt to bring up these network devices at boot time. If you don\u0026rsquo;t want this, remove the lines and use sudo ifup eth0 or sudo ifup wlan0 to bring them up manually.\nFile /etc/wpa_supplicant/wpa_supplicant.conf So far, so good. On both Jessie and Stretch, the following applies.\nFor a WiFi network, we need to update the wpa_supplicant.conf file.\nEdit the file /etc/wpa_supplicant/wpa_supplicant.conf and make it look like the following example, similar that is, not identical.\n1 2 3 4 5 6 7 8 9 10 11 country=GB ctrl\\_interface=DIR=/var/run/wpa\\_supplicant GROUP=netdev update\\_config=1 network={ ssid=\u0026#34;YourNetworkSSID\u0026#34; psk=\u0026#34;YourVerySecretPassword\u0026#34; # scan\\_ssid=1 # priority=1 } Ok, points to be very careful of:\nThe country=GB is required from the 26/02/2016 Raspbian release. It probably does no harm to have it in with earlier versions, your mileage may vary of course. It\u0026rsquo;s definitely required with the new Raspberry Pi 3 with built in WiFi and Bluetooth. If you are not sure of your country code, go to the Menu-\u0026gt; Preferences-\u0026gt;Raspberry Pi Configuration utility and set it on the Localisation tab. You can pick from a list.\nThere are no spaces between names, equal signs and parameters etc. If you have any spaces, networking will not start. Ask me how I know!\nLines with a \u0026lsquo;#\u0026rsquo; as the first character are comments. They are ignored.\nYou need one network={...} entry for each WiFi network that you might use. Home, work, college, whatever. Each will have a different name in the ssid entry.\nThe scan_ssid and priority entries are optional.\nPriority determines the order in which your Pi will connect if more than one of the networks is available. It\u0026rsquo;s best to give each one a different priority in that case! If more than one networks have the same priority, then the signal strength and/or security policy may be used to choose the best one to connect to.\nScan_ssid determines how the network will be scanned for. From the docs, and if you follow this, you are better than I am!\nSSID scan technique; 0 (default) or 1. Technique 0 scans for the SSID using a broadcast Probe Request frame while 1 uses a directed Probe Request frame. Access points that cloak themselves by not broadcasting their SSID require technique 1, but beware that this scheme can cause scanning to take longer to complete.\nI have two WiFi networks that the Pi can connect to, so I have two different entries in my configuration file.\nSo, replace \u0026quot;YourNetworkSSID\u0026quot; above, with the name of your WiFi network. If the network doesn\u0026rsquo;t broadcast the SSID, then you need to know exactly what it is.\nReplace \u0026quot;YourVerySecretPassword\u0026quot; with your own network\u0026rsquo;s password.\nSave the changes. My own file looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 country=GB ctrl\\_interface=DIR=/var/run/wpa\\_supplicant GROUP=netdev update\\_config=1 network={ ssid=\u0026#34;HomeWIFI\u0026#34; psk=\u0026#34;YoullNeverGuessThisPassword\u0026#34; priority=1 } network={ ssid=\u0026#34;OfficeWIFI\u0026#34; psk=\u0026#34;YoullNeverGuessThisPasswordEither\u0026#34; priority=2 } I have a home and office Wifi in the same place. I work from home and can use either.\nIn the past, you would set up your static IP addresses, gateways, netmasks etc in the /etc/network/interfaces file. No longer! It is now held in the /etc/dhcpcd.conf file. (Which I find very difficult to (a) remember and (b) type!\nFile /etc/dhcpcd.conf Only do the following if you wish to assign a static IP address to your Pi when it\u0026rsquo;s on wired (eth0) and/or WiFi (wlan0) networks. I have mine set to have a static IP on both, so my file looks similar (ok, identical) to the example below.\nEdit the file /etc/dhcpcd.conf and search for the interface eth0 or interface wlan0 lines. They may not be present. If you don\u0026rsquo;t find the entries, scroll to the end of the file.\nAdd the following configuration lines:\n1 2 3 4 5 6 7 8 9 10 11 # Static IP configuration for eth0. interface eth0 static ip\\_address=192.168.1.50/24 static routers=192.168.1.1 static domain\\_name\\_servers=8.8.4.4 8.8.8.8 # Static IP configuration for wlan0. interface wlan0 static ip\\_address=192.168.1.51/24 static routers=192.168.1.1 static domain\\_name\\_servers=8.8.4.4 8.8.8.8 Some more points to be very careful of:\nAs mentioned above, you only need to do this if you wish to allocate a static IP address to your Pi when it\u0026rsquo;s on wired (eth0) and/or WiFi (wlan0) networks. If you don\u0026rsquo;t care what IP it has, simply ignore this section.\nAgain, there are no spaces between names, equal signs and parameters etc. If you have any spaces, networking will not start. Ask me how I know!\nYou need one interface ... entry for each iface ... that you have defined in /etc/network/interfaces - don\u0026rsquo;t set one up for the \u0026ldquo;lo\u0026rdquo; device though! The names must match too.\nYes, you do need to repeat the information for routers etc in each entry. They don\u0026rsquo;t get shared, which is a shame. It would be nice to have a global section where I could define these once, then only define the changeable stuff in each interface entry. Still, c\u0026rsquo;est la vie, as they say in Wales.\nOk, the static ip_address line defines that I\u0026rsquo;m using an IP address of 192.168.1.50 when on eth0, or wired network, and 192.168.51 when on WiFi. This allows me to use both, at the same time, if I wish, without an IP clash.\nThe /24 means that the first 24 bits of the IP address are my netmask, or 255.255.255.0. This identifies my network as 192.168.1.xxx and my devices as 50 and 51, in this case.\nMy router is on address 192.168.1.1, so that\u0026rsquo;s what goes into the routers entry.\nI use Google\u0026rsquo;s name servers for my DNS name resolution, they are usually always available, but you can add others if you wish. Your ISP might have some that you can use. Separate each one with a space.\nBy the way, don\u0026rsquo;t use a name here, always use an IP address. You can\u0026rsquo;t resolve the names you use for the DNS servers if you don\u0026rsquo;t have a DNS server to resolve them! (No, I didn\u0026rsquo;t do that one myself, thanks for asking!)\nSave the file.\nThat\u0026rsquo;s should do it. A quick reboot is probably the best way to make sure everything is set up correctly, so insert a slight pause here while we reboot.\n1 sudo reboot When the system comes back up, run the following command to see what appears:\n1 hostname -I ## That\u0026#39;s a capital \u0026#39;eye\u0026#39; not a \u0026#39;one\u0026#39;. You should see the two static IP addresses you configured, if you did so, and if you have both eth0 and wlan0 up and running, like I do. If you are only connected to WiFi, for example, you should only see the one IP address for that.\nIf you see an extra IP address, then you probably still have some dhcp or static entries in your /etc/network/interfaces file, in the old style format, that need to be removed. Make sure that your file looks like mine above to get rid of the ghost IP address.\nAnd Finally\u0026hellip; This might be a good article to have a look at if you have problems with your WiFi connection seemingly going to sleep which was known to be a problem with early raspberry Pi 3+ machines.\nHTH\nCheers.\n","description":"","id":40,"section":"posts","tags":null,"title":"Raspberry Pi, PiZero, Raspbian Jessie, Networking and WiFi Setup","uri":"http://localhost:1313/RantsAndRaves/posts/2016/01/raspberry-pi-pizero-raspbian-jessie-networking-and-wifi-setup/"},{"content":"The standby database had the RMAN archivelog deletion policy set to \u0026lsquo;NONE\u0026rsquo; instead of being \u0026lsquo;APPLIED ON ALL STANDBY\u0026rsquo; and the FRA filled up to within an inch of its life, or 99% of its allocated quota! Not a major problem as this database was not in production, but still, an alert is an alert and has to be dealt with. However, things did not go quite as expected.\nFirst things first, check the archivelog deletion policy on the standby database:\n$ rman target / catalog user/password@catalog RMAN\u0026gt; show archivelog deletion policy; RMAN configuration parameters for database with db_unique_name xxxxxxxx are: CONFIGURE ARCHIVELOG DELETION POLICY TO NONE; RMAN\u0026gt; configure archivelog deletion policy to applied on all standby; old RMAN configuration parameters: CONFIGURE ARCHIVELOG DELETION POLICY TO NONE; new RMAN configuration parameters: CONFIGURE ARCHIVELOG DELETION POLICY TO APPLIED ON ALL STANDBY; new RMAN configuration parameters are successfully stored RMAN\u0026gt; exit As the FRA was currently at 99% usage, I expected Oracle to start clearing out archived logs that had been applied on the standby, in other words, the vast majority of them, Strangely, this didn\u0026rsquo;t happen. Never mind, sometimes you have to encourage Oracle by transporting a new archived log from the primary, which usually kicks off the tidy up process. Let\u0026rsquo;s do that, on the primary database.\n1 $ sqlplus / as sysdba 1 2 3 4 SQL\u0026gt; alter system archive log current; System altered. SQL\u0026gt; exit Back on the standby database, tailing the alert log shows that the new archived log had arrived and been applied, but still no deletions. Hmmm, maybe a little more encouragement is required.\n1 $ sqlplus / as sysdba 1 2 3 4 5 6 SQL\u0026gt; show parameter recovery_file_dest NAME TYPE VALUE ------------------------------------ ----------- ------------------------------ db_recovery_file_dest string +FRA db_recovery_file_dest_size big integer 40G So, we have a quota of 40gb in the FRA for this standby, it\u0026rsquo;s not yet in production, so that\u0026rsquo;s a reasonable quota. How much is actually used? We know it\u0026rsquo;s a lot - 99% - from the alert that started all this off, but let\u0026rsquo;s check anyway.\n1 2 3 4 5 6 7 8 9 10 11 12 13 SQL\u0026gt; set lines 300 pages 300 numw 15 trimspool on SQL\u0026gt; col name format a10 SQL\u0026gt; col space_limit format 999,999,999,999 SQL\u0026gt; col space_used format 999,999,999,999 SQL\u0026gt; col space_reclaimable format 999,999,999,999 SQL\u0026gt; col number_of_files format 9,999,999 SQL\u0026gt; select * from v$recovery_file_dest; NAME SPACE_LIMIT SPACE_USED SPACE_RECLAIMABLE NUMBER_OF_FILES CON_ID ---------- ---------------- ---------------- ----------------- --------------- --------------- +FRA 42,949,672,960 42,652,925,952 42,633,003,008 1,313 0 Pretty much, all of it, as expected. What makes up the usage? Is it all archived logs or could there be restore points and/or flashback logs as well.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 SQL\u0026gt; col percent_space_used format 990.00 SQL\u0026gt; col percent_space_reclaimable format 990.00 SQL\u0026gt; select * from v$flash_recovery_area_usage; FILE_TYPE PERCENT_SPACE_USED PERCENT_SPACE_RECLAIMABLE NUMBER_OF_FILES CON_ID ----------------------- ------------------ ------------------------- --------------- --------------- CONTROL FILE 0.00 0.00 0 0 REDO LOG 0.00 0.00 0 0 ARCHIVED LOG 99.31 99.26 1,313 0 BACKUP PIECE 0.00 0.00 0 0 IMAGE COPY 0.00 0.00 0 0 FLASHBACK LOG 0.00 0.00 0 0 FOREIGN ARCHIVED LOG 0.00 0.00 0 0 AUXILIARY DATAFILE COPY 0.00 0.00 0 0 In this case, it\u0026rsquo;s definitely archived logs and the vast majority can be reclaimed. Time to get rid! We know that gentle persuasion didn\u0026rsquo;t have any effect, so let\u0026rsquo;s increase the pressure a little:\n1 2 SQL\u0026gt; alter system set db_recovery_file_dest_size = 35g scope=memory; System altered. And tailing the alert log, in another session, shows that things are finally happening.\n1 2 3 4 5 6 7 8 9 Fri Dec 11 14:03:13 2015 ALTER SYSTEM SET db_recovery_file_dest_size=35G SCOPE=MEMORY; Fri Dec 11 14:03:13 2015 Deleted Oracle managed file +FRA/xxxxxxxx/ARCHIVELOG/2015_09_29/thread_1_seq_70.835.891673917 Deleted Oracle managed file +FRA/xxxxxxxx/ARCHIVELOG/2015_09_29/thread_1_seq_71.750.891675635 Deleted Oracle managed file +FRA/xxxxxxxx/ARCHIVELOG/2015_09_29/thread_1_seq_72.751.891675635 Deleted Oracle managed file +FRA/xxxxxxxx/ARCHIVELOG/2015_09_29/thread_1_seq_73.752.891675639 Deleted Oracle managed file +FRA/xxxxxxxx/ARCHIVELOG/2015_09_29/thread_1_seq_74.754.891680149 ... A few more gentle reductions, to avoid completely hanging things up, and we were eventually right down at 2gb FRA quota. Finally, I reset the quota back to the original 40gb and checked again:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 SQL\u0026gt; alter system set db_recovery_file_dest_size = 40g scope=memory; System altered. SQL\u0026gt; select * from v$recovery_file_dest; NAME SPACE_LIMIT SPACE_USED SPACE_RECLAIMABLE NUMBER_OF_FILES CON_ID ---------- ---------------- ---------------- ----------------- --------------- --------------- +FRA 42,949,672,960 2,112,880,640 2,092,957,696 61 0 SQL\u0026gt; select * from v$flash_recovery_area_usage; FILE_TYPE PERCENT_SPACE_USED PERCENT_SPACE_RECLAIMABLE NUMBER_OF_FILES CON_ID ----------------------- ------------------ ------------------------- --------------- --------------- CONTROL FILE 0.00 0.00 0 0 REDO LOG 0.00 0.00 0 0 ARCHIVED LOG 4.92 4.87 61 0 BACKUP PIECE 0.00 0.00 0 0 IMAGE COPY 0.00 0.00 0 0 FLASHBACK LOG 0.00 0.00 0 0 FOREIGN ARCHIVED LOG 0.00 0.00 0 0 AUXILIARY DATAFILE COPY 0.00 0.00 0 0 Job done. With a little encouragement!\n","description":"","id":41,"section":"posts","tags":null,"title":"Archivelog Deletion Policy Changes Don't Always Take Immediate Effect.","uri":"http://localhost:1313/RantsAndRaves/posts/2015/12/archivelog-deletion-policy-changes-dont-always-take-immediate-effect/"},{"content":"A process that called \u0026lsquo;\u0026lsquo;UTL_FILE\u0026rsquo;\u0026rsquo; was failing in the test system, but worked fine with exactly the same set up in production. Why? The error was ORA-29283: invalid file operation. How do we find out exactly why it was failing?\nMY_DIRECTORY is a directory, owned by SYS with READ and WRITE privileges granted to a schema that uses it to create, write and read files in that location.\nThe oracle account on the server can create and read files in the directory location, touch and cat prove this.\nRunning a PL/SQL package, however, fails. The failing code was reduced to the following test sample:\n1 2 3 4 5 6 7 declare v_fd utl_file.file_type; begin v_fd:=utl_file.fopen(\u0026#39;MY_DIRECTORY\u0026#39;,\u0026#39;norman.txt\u0026#39;,\u0026#39;w\u0026#39;); utl_file.fclose(v_fd); end; / Which blows up with the less than helpful message:\n1 2 3 4 5 ERROR at line 1: ORA-29283: invalid file operation ORA-06512: at \u0026#34;SYS.UTL_FILE\u0026#34;, line 536 ORA-29283: invalid file operation ORA-06512: at line 4 Here\u0026rsquo;s a nice trick, stolen blatantly from Michael Schwalm at http://blog.dbi-services.com/troubleshooting-ora-29283-when-oracle-is-member-of-a-group-with-readwrite-privileges/ which shows how to actually see what the real underlying problem is for this exception.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 SQL\u0026gt; -- Change define, we need to use an ampersand. SQL\u0026gt; set define # SQL\u0026gt; -- Get my current process ID into a variable. SQL\u0026gt; column spid new_value unix_pid SQL\u0026gt; select spid from v$process p 2 join v$session s on p.addr=s.paddr 3 and s.sid=sys_context(\u0026#39;userenv\u0026#39;,\u0026#39;sid\u0026#39;); SPID ------------------------ 121080 SQL\u0026gt; -- Trace open calls from my session. SQL\u0026gt; -- Without the \u0026amp;, the host call never returns! SQL\u0026gt; -- We know that it is the utl_file.fopen call that is SQL\u0026gt; -- failing, so only trace open calls. SQL\u0026gt; host strace -e trace=open -p #unix_pid \u0026amp; echo $! \u0026gt; tmp.pid Process 121080 attached - interrupt to quit SQL\u0026gt; -- Paste in the offending code... SQL\u0026gt; declare 2 v_fd utl_file.file_type; 3 begin 4 v_fd:=utl_file.fopen(\u0026#39;MY_DIRECTORY\u0026#39;,\u0026#39;norman.txt\u0026#39;,\u0026#39;w\u0026#39;); 5 utl_file.fclose(v_fd); 6 end; 7 / That throws up the following helpful message, followed closely by the expected Oracle exception message again (not shown):\n1 open(\u0026#34; /logfiles/MYDB/norman.txt\u0026#34;, O_WRONLY|O_CREAT|O_TRUNC, 0666) = -1 ENOENT (No such file or directory) At this point, I need to press CTRL-C to detach the strace session.\n1 2 SQL\u0026gt; ^C Process 121080 detached Looking at the above message, I can see (almost) straight away that the file path has a leading space. This implies that whoever set up the original directory, created it with a minor typo that is hard to detect when looking at DBA_DIRECTORIES.\nThe fix was simple:\n1 2 SQL\u0026gt; create or replace directory MY_DIRECTORY as \u0026#39;/logfiles/MYDB\u0026#39;; SQL\u0026gt; grant read, write on directory MY_DIRECTORY to [whoever needs it]; And now, past in the offending code again, and it \u0026ldquo;just works\u0026rdquo;:\n1 2 3 4 5 6 7 8 9 SQL\u0026gt; declare 2 v_fd utl_file.file_type; 3 begin 4 v_fd:=utl_file.fopen(\u0026#39;MY_DIRECTORY\u0026#39;,\u0026#39;norman.txt\u0026#39;,\u0026#39;w\u0026#39;); 5 utl_file.fclose(v_fd); 6 end; 7 / PL/SQL procedure successfully completed. ","description":"","id":42,"section":"posts","tags":null,"title":"UTL_FILE Operation fails with ORA-29283","uri":"http://localhost:1313/RantsAndRaves/posts/2015/11/utl_file-operation-fails-with-ora-29283/"},{"content":"Running a business, I like to keep a small stock of spare printer ink cartridges, so I usually have a couple of spare colour and a couple of black ones, just in case. However, after a recent cartridge change, one black and one colour at the same time, the printer has suddenly stopped working. This was after about a month of perfect usage, not immediately after the change. The printer is an HP Photosmart 2610 All in one - and now, it\u0026rsquo;s an HP Photo-not-very-smart-at-all 2610 none in one!\n2 November 2015 - Updated, see below!\nSo, what went wrong? Who knows, on a day when I needed to print a few invoices for posting and do some scanning ready for a new contract, the machine came up with an error alert - \u0026ldquo;Remove and check color cartridge\u0026rdquo;. So I did, not much to check really, but these were new around a month ago, so they should be ok, right? No joy at all. On replacing the cartridge, the same error appeared on the display panel.\nChecking the internet, I could see that I was not alone. On the HP forums an employee of HP stated quite categorically that rumours of there being an expiry date on HP genuine cartridges were simply a myth and while there was a date on each one (which was news to me) that was nothing more than a warranty date, and, the cartridges do not expire. Good news indeed because my cartridges appeared to be dated March 2014 which is more than a year ago - I wonder where I bought these from because as far as I\u0026rsquo;m aware, I bought them fairly recently at least in the last 6 months. Still, they don\u0026rsquo;t expire do they?\nAnyway, long story short, I replaced the cartridge with another new one from the same 2 pack. Oh joy, same error. I tried the HP support web site for the UK and selected to \u0026ldquo;chat with an expert\u0026rdquo;, and after filling in all my details and my problem, the web site failed. Sigh! One refresh later and I had to fill it all in again. On hitting submit, it failed again with another database error. I gave up and turned to Twitter to let @HPSupport know. I almost immediately got a reply asking what was up and that the web site was fine.\nAfter numerous back and forth tweets, Teri must have gone home for the evening and Eric took over. None of them mentioned anything about expiry dates even though I\u0026rsquo;d given them all the details. It has to be said that both were very helpful. In the end, Eric gave me a phone number to try (Monday to Friday 08:00 until 17:00 it seems - that\u0026rsquo;s no good to me!) I want help RIGHT NOW!\nEventually, I managed to find a couple of PDFs of the files I needed to send off, but I had to use CamScanner on my phone to scan my passport and bank accounts etc for the new contract. I still haven\u0026rsquo;t got my invoices printed out!\nCome Saturday, and I purchased a brand new colour cartridge (334) from PC World, and fitted it. Hooray, no more error messages telling me to \u0026ldquo;remove and check color cartridge\u0026rdquo;. Now the error is \u0026ldquo;Insert black or Photo Print cartridge in right stall\u0026rdquo; - For the love of f*ck! I have a two pack of 339 black cartridges, which I\u0026rsquo;ve had for a wee while, not too long though, and I opened one of those. The date? August 2015. Remove all the protective bits and slip it in. Same. F*cking. Message!\nSo, I\u0026rsquo;m now rather annoyed at my printer, and at HP in general. It appears that the so called warranty date on each cartridge is actually some sort of expiry date, contrary to what was written on HP\u0026rsquo;s own forums, by an HP employee (admittedly, his disclaimer was that he was not speaking for HP). I now appear to have a stock of expensive genuine HP cartridges that are totally unusable, plus, the whole so called all in one, is sitting there like a pile of junk because none of the features of the machine will work until I get another new black cartridge fitted into the damned thing. I want to send a fax? No chance. Scan something? Ha ha ha!!\nAccording to HP\u0026rsquo;s support site, somewhere, I can remove said faulty colour cartridge and print in mono - which had been working until this tale of woe began - but it appears not. The printer simply sits there begging me to \u0026ldquo;insert a color cartridge\u0026rsquo;. Sigh.\nGiven that I have, on occasion, needed to print photos out, and I have a Photo Print cartridge, I tried that too, no joy, the printer doesn\u0026rsquo;t want to know. Mind you, that\u0026rsquo;s definitely an old one, I checked the date and it\u0026rsquo;s September 2006, but what\u0026rsquo;s the point of having these removable and re-sealable cartridges if the damned things stop working for no bloody reason other that the fact that they have gone past the date stamped on the front?\nSo, I have in my collection a pile of brand spanking new cartridges, which were not cheap, are mostly unused and have cost me Â£55.99 for the two 339 black ones, Â£61.99 for the two 334 colour ones and Â£28.99 for the Photo Print colour one. Not including the Â£25.00 I paid in PC World for a new 334 Colour just to make sure that the printer was fine! Â£175.97 of my money that is now wasted.\nHP, I am not happy with you. This is no way to treat your customers and if it turns out that my printer is fine, but there is an expiry date on the cartridges (ie, not a warranty date), them I\u0026rsquo;m looking for a refund and/or replacement for all of the above. Somehow I doubt that much will be forthcoming, but given that nothing is mentioned about this in any part of the packaging or warranty information - which is printed in such tiny lettering, it\u0026rsquo;s almost impossible to read - I don\u0026rsquo;t see that you have any reason to refuse.\nPlus - because I don\u0026rsquo;t ask for much - how dare you build software into your devices that prevent me from using other parts of the so called all in one, when there is no printer ink present. At least, when the software thinks there is none, there is actually a pretty much full set of blank and colour inks thank you very much. Surely it doesn\u0026rsquo;t take a rocket scientist to write software that would prevent me from using the copier or the printer if I did truly have an out of ink problem, but would let me use the fax and/or scanner, which don\u0026rsquo;t actually need ink.\nAnd as for preventing me from using my bought and paid for, genuine HP cartridges just because some arbitrary date has been and gone is probably against one or more of the laws of the EU. I\u0026rsquo;m thinking that I need an update to the firmware on this device to remove said restrictive practices. Failing that, my next printer will not be an HP one.\n2nd November 2015 - Update Oh well, today is Monday and I\u0026rsquo;ve once more returned to the fray. It seems that there have been numerous problems with any number of HP\u0026rsquo;s printers/All in ones etc, whereby the tension spring that holds the cartridge in place (I assume) have come loose as a result of a tiny little - about 3 by 6mm (or 1/8\u0026rdquo; by 1/4\u0026quot; for the folks in the US) - plastic tab breaking off.\nI decided to have a look and found that while the colour cartridge was fine, the black cartridge\u0026rsquo;s wire was indeed hanging loose and the plastic tab was lying in the depths of the machine - where all the overflow ink goes. I\u0026rsquo;m not delving in there! There are new life forms evolving in the primordial gloop that\u0026rsquo;s in there!\nThe workaround is to glue up the spring and hope that it has not punched holes in the rather delicate mylar circuit board behind where the cartridge sits. I think mine is ok, but there is a mark near the bottom of the black cartridge\u0026rsquo;s mylar strip that isn\u0026rsquo;t present on the colour side. Here\u0026rsquo;s hoping it\u0026rsquo;s not serious.\nThese printers apparently, even while on sale and under warranty, do not have spare parts or any other way of replacing bits - they appear to be build down to a price, and are simply expected to be replaced when they develop a fault. Another WTF!\nI would rather pay decent money for something that is substantially built, lasts and works - and can be repaired with spares etc - than buy a cheap printer that will most likely end up in a landfill site at some point. Unless, of course, I\u0026rsquo;ve gutted it for the stepper motors and silver steel guides etc!\nI\u0026rsquo;m waiting for the Superglue to cure even as I type! Wish me luck.\n2nd November 2015 - Update 2 Ok, Superglue, as advised on fixyourownprinter.com , doesn\u0026rsquo;t work. I didn\u0026rsquo;t think it would.\nI\u0026rsquo;m on Araldite Rapid now. I should know in an hour or two if this has been successful! If not, the guts of this printer might end up as a small CNC machine of some kind!\n2nd November 2015 - Update 3 Araldite Rapid has worked. After leaving it to cure for an hour, I have a printer that works again. It reports a failure to align the heads after replacing the cartridges, but it prints text and graphics fine, so far, so I shall not be worrying.\nConclusions HP appears to have a design fault with the retaining springs - they break out of the plastic tabs and trash the mylar circuit connectors - I was lucky, my mylar was only slightly nicked, so I might have got away with it.\nHP were taken to court in the USA for their cartridge expiry \u0026ldquo;feature\u0026rdquo; and, as far as I can see, they lost and had to remove it from the printer firmware. (Sadly, I can\u0026rsquo;t get back to the web site with the details!)\nThe dates on the cartridges might be a warranty date, but it might be that after 4 years from that printed date, they will expire anyway. Time will tell - that\u0026rsquo;s its job after all!\nHaving a fault with the black cartridge retaining spring, displays a message that the colour cartridge is the one at fault. Sigh!\nHaving a printer resources problem, or perceived problem, causes the other features of the all in one to be unusable. A minor software problem, who would have thought? Scanning doesn\u0026rsquo;t need ink, neither does sending a fax - at least write the software to allow those to be used.\nSo, the question has to be, are there any decent long lived and repairable printers out there in the world any more?\n","description":"","id":43,"section":"posts","tags":null,"title":"HP Printer Ink - WTF?","uri":"http://localhost:1313/RantsAndRaves/posts/2015/10/hp-printer-ink-wtf/"},{"content":"Do I like problems or what? :-) I\u0026rsquo;m running Linux Mint 17.2 as my host, and I have a VirtualBox 5.0 VM running Windows 7 Professional. I decided I\u0026rsquo;d like to be able to run the Arduino software from within the VM, but not talking to an Arduino, but to a bare bones setup and programming AtTiny85 devices. The following might be of use to other people\u0026rsquo;s needs as it explains how the FDTI device cane be automatically assigned to the VM rather than to the host, when plugged in and the VM is running.\nSet up the Host First There are two things you need to do on a Linux host to enable the USB deviced for the guest(s). The first is to ensure that the user you normally run VirtualBox under is a member of the vboxusers group, if not, add the user and logout and back in again. (You don\u0026rsquo;t need to reboot the machine, just logout of your user and back in again.\n1 2 3 4 $ groups adm dialout cdrom sudo dip plugdev lpadmin sambashare $ sudo usermod -a -G vboxusers your_user_name Now logout and back in again.\n1 2 $ groups adm dialout cdrom sudo dip plugdev lpadmin sambashare vboxusers We can see that the vboxusers is now active.\nThe second task that needs doing is to discover the FDTI device\u0026rsquo;s vendor, product and serial numbers.\n1 2 3 4 $ lsusb ... Bus 007 Device 007: ID 10c4:ea60 Cygnal Integrated Products, Inc. CP210x UART Bridge / myAVR mySmartUSB light ... The above is fine, the main ID field shows that the id of this particular device is 10c4:ea60 with is the vendor id and the product id, both of which should be unique. There is no serial number though, and that might be needed if there are more than one revision of the device. Try this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ VBoxManage list usbhost ... UUID: 9f963985-1fb7-49e0-b066-e1261a6a1985 VendorId: 0x10c4 (10C4) ProductId: 0xea60 (EA60) Revision: 1.0 (0100) Port: 0 USB version/speed: 1/Full Manufacturer: Silicon Labs Product: CP2102 USB to UART Bridge Controller SerialNumber: 0001 Address: sysfs:/sys/devices/pci0000:00/0000:00:1d.1/usb7/7-1//device:/dev/vboxusb/007/007 Current State: Captured So, we have serial number 0001. We can now set up a VirtualBox USB filter to cause that particular FDTI device to be connected directly to the Windows 7 guest when it is running and the device is plugged in. Note that we must note down the vendor and product ids exactly as they are displayed, not the uppercased version in brackets after it!\nCreate a VirtualBox USB Filter The Windows VM needs to be closed, for best results. The FDTI device should also be plugged in.\nRun the VirtualBox manager and click on the Windows VM on the VM list on the left. Click settings to open the settings dialogue for that particular VM. Scroll down and select the USB setings on the left. On the right, make sure that USB 2.0 (EHCI) Controller is selected. I\u0026rsquo;ve found that setting the USB 3.0 controller fails with my laptop and the Windows guest complains that it has no USB drivers. I don\u0026rsquo;t have USB 3 on my laptop, so 2 it is for me! Click the second button down on the far right, it\u0026rsquo;s a USB plug with a \u0026lsquo;+\u0026rsquo; on it. This adds a new filter. Select the FDTI from the list that appears. Right-click it, and select edit. Make sure that product and vendor Ids, and the serial number match. The other details can be either left as is, or ignored. Be aware that anything in any of these fields must match the devices information exactly. OK back out of this dialogue. If no devices appear in the list, your user is not in the vboxusers group, or it is not yet active. Make sure that the filter is enabled by checking the checbox to the left of the name. Start the VM. If the device is unplugged when the VM is subsequently started, it won\u0026rsquo;t matter. With the filter in place, plugging the device into a USB slot will cause it to be assigned to the guest and not to the host.\nInstall Arduino In the Windows VM, open Firefox - other browsers are available. Internet Explorer is not a browser ;-) - and navigate to https://www.arduino.cc/en/Main/Software and download the software, 1.6.5 was the latest version at the time of writing. Make a donation, if you wish. When the software has downloaded, install it. It will attempt to install a few USB drivers and such like, make sure that you allow it to do so. Windows is so naggy!\nYou may have to reboot Windows to make the new drivers stick. Sigh.\nStartup the Arduino IDE and configure it for your Arduino board (Tools-\u0026gt;Board), Port (Tools-\u0026gt;Port) and programmer (Tools-\u0026gt;Programmer). Make a note of the port. Open the ubiquitous blink sketch in the usual manner, and upload it. If it uploaded then well done, you are now able to program Arduinos from within a VM.\nIn my case, it failed as it couldn\u0026rsquo;t open port COM1, it said that the port could not be found. Running device manager (start, search, type device, select device manager from the list) and scrolling down to COM and LPT ports showed that only LPT1 was present.\nInstall a Proper Driver Go directly to http://www.silabs.com/products/mcu/Pages/USBtoUARTBridgeVCPDrivers.aspx where you can download a driver to make FDTI devices work on just about every version of Windows. Select the appropriate version for your guest operating system. You will download a zip file.\nFind the zip file, and extract it, then change into the folder that was created (CP210x_VCP_Windows\\CP210x_VCP_Windows in my case) and run either the 32bit (x86) or 64 bit installer depending on your guest. Mine was 64 bit, so I executed CP210xVCPInstaller_x64.exe.\nGuess what? Another reboot is required. Windows sucks!\nAfter the reboot, open the blink sketch again and go to tools-\u0026gt;port, your com port should now appear in the list, mine was deemed to be COM3, so I chose that.\nClick upload, and be amazed as your Arduino starts working!\nAtTiny85 Devices Need to program these? Or the AtTiny25? Or AtTiny45? See how to set up Arduino IDE 1.6.x at http://highlowtech.org/?p=1695 where all is explained. It just works.\nIf anyone is interested, I\u0026rsquo;m connecting my FDTI device to an Arduino Uno lookalike (actually, it\u0026rsquo;s a Shrimp, see https://start.shrimping.it/kit/stripboard.html, built on a breadboard style strip board) with an AtMega328 running the ArduinoISP program. My programmer in the IDE is Arduino as ISP and not ArduinoISP - confusing or what - I\u0026rsquo;ve installed the AtTiny stuff as per the link above. It all just works.\nOh, and one other thing, the AtTiny25/45/85 has PWM on physical pin 3; PB4/OC1B, as well as on physical pins 5 and 6; PB0/OC0A and PB1/OC0B/OC1A. The diagram on the above link isn\u0026rsquo;t quite correct, but I\u0026rsquo;ve informed them of this.\nHave fun.\n","description":"","id":44,"section":"posts","tags":null,"title":"Getting Arduino Working from a Windows 7 VirtualBox Guest","uri":"http://localhost:1313/RantsAndRaves/posts/2015/09/getting-arduino-working-from-a-windows-7-virtualbox-guest/"},{"content":"I was able to use HPLIP with Mint 17.1 but when I upgraded to 17.2, I had the problem where attempting to run the utility caused nothing at all to happen. The solution:\n1 sudo apt-get install hplip-gui Works perfectly for me now.\n","description":"","id":45,"section":"posts","tags":null,"title":"HPLIP Stops Working After Linux Mint 17.2 Upgrade","uri":"http://localhost:1313/RantsAndRaves/posts/2015/08/hplip-stops-working-after-linux-mint-17-2-upgrade/"},{"content":"It appears that the free, introductory Perl course has gone away. The links below are still pointing at the original tutorial, but here\u0026rsquo;s a free Perl course from Udemy as a replacement. I\u0026rsquo;m not a Perl developer, I like to be able to read my code after I\u0026rsquo;ve written it! ;-) However, some people do like Perl and there are even people out there who would like to learn it.\nI was recently contacted by a company called Udemy who create tutorials and carry out training courses for various topics - why not give them a look if you need any training. Anyway, they were updating me on their new introductory Perl tutorial. I think it\u0026rsquo;s well worth a look - if you are that way inclined!\nYou can check out their web site as well, there is probably something there that might be useful to you.\n","description":"","id":46,"section":"posts","tags":null,"title":"A Useful Perl Tutorial","uri":"http://localhost:1313/RantsAndRaves/posts/2015/08/a-useful-perl-tutorial/"},{"content":"\nMum passed away recently. What follows is the full service as conducted by Janet Donnelly of the Humanist Society Scotland. Janet has done a number of funerals for our family in recent years and I\u0026rsquo;d happily have her do mine when the time comes!\nWe asked Janet for a loyalty card - you know, pay for 9 get the 10th free - but she was having none of it! :-D\nI\u0026rsquo;m grateful to Janet for allowing me to put her whole service up here on the web for all to see. Thanks Janet. I\u0026rsquo;ve corrected a couple of bits in the following as the text below is from Janet\u0026rsquo;s draft version, which is close enough to the final version. (It was a gerbil, not a hamster, and it was a vicious \u0026ldquo;love child\u0026rdquo; of a rodent and would chew through the bars of the cage to get you!)\nA Celebration of the Life of Jean Martini.\n28th March 1939 â 14th August 2015.\nWilliam Watsons.\nElgin Cemetery.\n11am Thursday 20th August 2015.\nCeremony composed and conducted by:\nJanet Donnelly\nAuthorised Celebrant\nHumanist Society Scotland\n(Sixties music compilation)\nGood morning ladies and gentlemen, family and friends and a very warm welcome to you all. We have come together this morning to celebrate the life of Jean Martini and to offer one another support in our sorrow.\nJeanâs life was a full one. She packed a lot into it and whilst today is of course a very sad occasion, I hope that it will also be an uplifting and positive appreciation of who she was and how she lived.\nMy name is Janet Donnelly and Iâm an authorised celebrant from the Humanist Society of Scotland. Itâs my great privilege to have been asked by Jeanâs family to lead this celebration of her life today.\nHumanism is a philosophy of life which reflects the views of millions of people around the world. We believe that we have one life to live and that we should live that life to the full whilst respecting the views and beliefs of others.\nThis was very much how Jean lived and her family thought that a Humanist ceremony of celebration would allow them to say their farewells to Jean in a way that felt right for her and for them.\nIn keeping with the principles of Humanism, our ceremony for Jean wonât have any religious elements but there will be a pause for reflection a little later on which those of you who have a religious faith or belief may wish to use to remember Jean in your own private way.\nI didnât know Jean and so I met with her family and with some of the friends and neighbours who knew her best on Tuesday night so that they could tell me about her. There were tears of course as they reflected on the place that she occupied in their hearts but it wonât come as any surprise to any of you when I tell you that there was a lot of laughter too that night. I left, nearly three hours after Iâd arrived with twelve pages of notes â some of which Iâll share with you all just now and some which are definitely best left to be shared later on at The Mansefield over a glass or two or three of wineâ¦â¦.\nIâd like to begin with a brief biography of Jeanâs life and then there will be lots of memories and recollections of who she was and what made her tick. She was, amongst other things, a very proud mum and granny, a treasured mother in law and a good friend and neighbour and I hope that you all recognise the Jean that you knew and loved in our ceremony today.\nJean came into the world on the 28th of March 1939 at The Wards Dairy in Elgin and she was the eldest of four children for Sandy Stephen who was a milk roundsman and his wife Margaret.\nJean went to school in Archiestown and as a teenager she loved dancing. She even got caught one night climbing out of her bedroom window to go off to a dance â much to her fatherâs disapproval. On another night she was climbing back in the window when she managed to put her foot right in to a cake that had been baked earlier on for a special occasion.\nWhen she left school, Jean had various jobs â amongst them she worked at Barmuckity, as an egg packer at Brumley Brae and at The Laichmoray as a chambermaid. She used to tell tales from her time at the Laich, of local businessmen who used to chase her round the bed whilst she was trying to get on with her work. She was, presumably too nimble to ever let them catch her.\nIn 1958 Jean married Joe Dunbar and they went on to have three sons together â firstly Norman, then Gordon and finally Sandy who completed their family.\nAfter 16 years of marriage, Jean and Joe went their separate ways and divorced and then, in the mid-seventies Jean met Enrico at a wedding dance and they hit it off. Enrico and Jean shared a similar outlook on life â they were both warm and welcoming characters who enjoyed life to the full and they set up a very happy home together.\nJean learned to speak fluent Italian and they went often to Italy to stay at their house in San Remo. She knew all of the local dialects and was practically an honorary Italian â haggling at the markets and tasting all of the produce before buying it. Jean used to say that going to Italy wasnât going on holiday â it was just swapping one kitchen sink for another.\nThey often took friends with them and Caroline and Jake have lots of wonderful memories of the journey south through France â and this is all the more surprising when you understand that Jean did a lot of the driving despite the fact that she took her test three times and failedâ¦threeâ¦timesâ¦.\nEnrico was an accomplished chef of course but Jean too was a wonderful cook and hostess and she loved nothing more than a houseful of people, long leisurely meals and copious amounts of wine to make the evening go with a swing. The alcohol wasnât just to be found in the glasses though. There are some vivid and some not so vivid memories of the night when there were brandied cherries for dessert and even if you werenât sozzled when they arrived, you definitely were by the time the coffee was poured.\nAs Norman, Gordon and Sandy made lives for themselves, Jean became a mother in law and then a granny too. Kathy spent a lot of time with Jean when she and Sandy were together and she credits Jean with teaching her skills like how to cook game Italian style, hanging her sheets out properly on a rope and how to âthoroughâ a room. Jean was always incredibly house-proud and even when her health began to fail in later life, the house was still spotless.\nJeanâs insistence on having her house spick and span, combined with her naturally warm personality meant that running a bed and breakfast was the perfect occupation for her. The house at the bottom of Reidhaven Street, being opposite the railway station was in prime position and Jean was the perfect hostess.\nAs a granny, Jean loved to spend time with Amber, Michael and Rachel and she used to bake with the girls: rock cakes and cupcakes, and make up games for them all to play. Key in the Tree will go down in family legend, as will Jeanâs mini sports days with her own egg and spoon and sack races. At the B\u0026amp;B, Jean would offer guests a tray of tea and biscuits when they arrived and she did the same for her grandchildren â but for them it would be juice in a mug, rich tea biscuits sandwiched together with butter and a kinder egg to complete the treat.\nJeanâs neighbour Doreen remembers Jean as being caring and kind â when Doreen had to dash off to the hospital because her daughter was in labour, Jean didnât think twice about taking in the three students from Shetland who had just arrived to lodge with Doreen. Jean welcomed them in to her home, fed them and watered them until Doreen came home later on. When Doreenâs cat sadly died after being hit by a car, it was Jean who tenderly wrapped it in a towel and then buried it in the garden to save Doreen the distress and upset of having to do it herself.\nSeventeen years ago Enrico was diagnosed with Parkinsonâs disease and Jean effectively became his carer. They were a devoted couple and there is nothing that she wouldnât have done for him, or he for her. In 2003 they were married after being together for nearly 30 years.\nSeveral years ago, Jeanâs family noticed that she wasnât herself and she was diagnosed with vascular dementia. It came on gradually but eventually it became clear that she couldnât manage safely on her own. After falling and breaking her leg, she moved into Abbeyvale and then into the Grove where she was happy and contented. Enrico moved into The Grove as well so they could still be together.\nJean and Enrico still managed to have âdate nightsâ courtesy of The Laichmoray Hotel. They would send over a meal for them and they would have a table laid specially for them so that they could continue some semblance of the life that they had enjoyed before their advancing years slowed them down. The only difference was that the glasses of wine were fewer â much to Enricoâs disgust.\nIn April 2013, Enrico passed away and Jean continued to be looked after at The Grove. She had lots of visitors who did their best to enrich her quality of life with jigsaws, crosswords and often just a good blether but about three weeks ago, Jean was admitted to Dr Grays with a chest infection which turned to pneumonia.\nAgain she had lots of visitors but her frail health meant that she couldnât fight off the infection. She had someone with her round the clock and Sandy was sustained by the love and support of those around him â especially Jeanâs sister Phyllis, his partner Jo and his daughter Amber. They all played their part in making Jeanâs final days as comfortable as possible â sitting chatting by her bedside, fetching MacDonalds to keep Sandyâs strength up, or filling his squeezy sports bottle with Ribena to sustain him through the night. Amber especially showed enormous stamina â getting up early and spending 11 or 12 hours a day at her grannyâs bedside. Jean would have been very proud of her.\nEventually though, Jeanâs strength ran out and she died gently and peacefully with Sandy by her side on Friday the 14th of August. She was in her 77th year.\nWhen I went to visit Sandy and everyone else on Tuesday night, everyone had their own personal stories of Jean and what she meant to them. There were far too many to tell just now but Iâd like to share just a few which I hope will reflect just what a wonderful lady Jean was.\nJean â as I said earlier enjoyed life and was known on occasion to partake of a glass of wine or two. I saw pictures of her dressed as a chicken â but nobody quite seems to know why she was sporting such a colourful costume.\nChickens also feature in another portion of Jeanâs life: one day a random chicken appeared in the garden and roosted in the plum tree. Jean adopted it and named it Brenda. Jean also adopted a baby blackbird in the hot summer of 1976 and to feed it, she had to dig worms out of the garden. The drought meant that the worms were only to be found several feet down but Jean persevered with her spade.\nAs a mum, Jean always welcomed her sonâs girlfriends and was never stuffy or old fashioned. When one of them was visiting in the evening, Jean would go off to bed after asking âWill it be one or two for breakfastâ¦?â One day she took Gordon to one side and said to him seriously: âGordon, I donât mind you bringing girls home, but could you take home a nice looking one now and againâ¦â!\nShe was just joking of course â Jean was open minded, liberal and accepting. Her good friend Graham told me that whoever you were, royalty or street sweeper, Jean would treat you all the same. Jean never took sides in her sonâs relationships and even when Kathy and Sandy split up, Jean and Kathy stayed close.\nOn one of their frequent trips to Italy, Caroline, Jake, Enrico and Jean drove away up into the mountains to visit a friend and, as was usually the case, the welcome was warm and the wine flowed. The host asked Caroline to choose a rabbit from the many fluffy bundles that were running about and when she said that she couldnât because sheâd never be able to get it home, the host laughed and told her it would be for the pot for their supper. On the way home, rather the worse for wear, they pulled the car up at the side of a road next to an allotment, Jean peered drunkenly over the fence at the vegetables growing there and asked Caroline: âare they cabbagesâ¦.or are they rabbitsâ¦â¦?â and they all nearly wet themselves laughing. Jean was always good fun and always up for a giggle. (You had to be there!)\nAs wee boys, her sons wanted a pet but they werenât allowed one. Undaunted, they hatched a plan and so, on Motherâs Day, they bought their mum a gerbil called Hamishâ¦.as you do. Problem solved.\nA similar problem was all of the dirty dishes and pans that they had to wash on Christmas Day. Their solution? They bought their mum a dishwasher for Christmas!\nJean had green fingers and her garden was her pride and joy. She taught her grandchildren to grow flowers, she grew veggies and she swapped cuttings with neighbour Doreen. Jean also had a particular fondness for her favourite wheelbarrow which was called into service often.\nShe hated water and never learned to swim and she could never master the hand eye co-ordination and balance of a pushbike â which frustrated her greatly.\nShe could be stubborn. When she slipped on the ice and broke her hip, she still managed to walk up to the Lido and back because she didnât want to make a fuss or bother anyone.\nThere is so much to say about Jean and it would be easy to remember only how she was in the most recent years but in the prime of her life she was vital and bubbly and vibrant. She was warm and loving but could sometimes be fiery too. She was at the heart of her family and even as her health failed, she was surrounded by people who loved and cared for her. She inspired love and loyalty and I know that there will be many as well outside the close circle of family and friends who will mourn her passing.\nI canât possibly hope to have done her justice here in such a short time but I hope that by sharing some of the recollections and memories that I was lucky enough to hear, I have stirred some of your own memories too.\nIn the time to come, talk often about Jean and remember the part she played in your life. I know that the next wee while will be tinged with sadness that sheâs no longer physically here but I hope that in time, that sadness will be replaced by a sense joy and gratitude that you had the privilege of sharing her life. The writer AC Grayling wrote:\nAs long as we love each other, and remember the feeling of love we had, we can die without really going away. All the love you created is still there. All the memories are still there. You live on â in the hearts of everyone you have touched and nurtured while you were here. Death ends a life, not a relationship.\nAnd so I hope it is with Jean. Sheâll always be a part of you because she made your lives better. That will be her legacy.\nShe was very much loved and sheâll be very much missed.\nIâd like to invite you all now to take a few moments to remember Jean privately in whichever way brings you comfort.\n(1 minuteâs silence)\nIn a few moments weâll be moving to Elgin Cemetery where weâll lay Jean to rest in a short graveside ceremony. Those of you who would like to join us will be most welcome.\nBefore we move on though, Norman, Gordon and Sandy have asked me to thank you all for coming today to offer your support and for all of the cards and messages of sympathy that theyâve received since Jeanâs death. Itâs been a great comfort to them to know that theyâre in your thoughts at such a sad time.\nOver the years there were many people who played supporting roles at different stages in Jeanâs life but there are special thanks to the staff at The Grove for looking after Jean and making sure that she was happy and contented, as well as the staff in ward 7 at Dr Grays who went above and beyond the call of duty for Jean and for the family as her life drew to a close. There is an extra special thank you too for Beth who was close to Jean for a long time.\nNorman, Gordon and Sandy would like to invite you to join them for tea and refreshments at the Mansefield where they look forward to sharing some of your memories of Jean. There is a lot I havenât had a chance to talk about so please do go if you can.\nLastly, thereâs a collection at the door as you leave if youâd like to make a contribution and the money will go to local charities in Jeanâs name.\nThank you all again for coming. Sit for a moment now and remember Jean as we listen to our final song today. Jean loved her sixties music which you heard as you arrived but this is her all time favourite track by one of her favourite artists. Elvis Presley and Love Me Tender.\n(Music â Love Me Tender)\nAt the cemetery\nLet her be safe in sleep As leaves folded together As young birds under wings As the unopened flower.\nLet her be hidden in sleep As islands under rain, As mountains within their clouds, As hills in the mantle of dusk.\nLet her be free in sleep As the flowing tides of the sea, As the travelling wind on the moor, As the journeying stars in space.\nLet her be healed in sleep In the quiet waters of the night In the mirroring pool of dreams Where memory returns in peace, Where the troubled spirit grows wise And the heart is comforted.\nWe have gladly shared our memories of Jean as we celebrated her life and now we must say our final farewells to that part of her which cannot remain with us. We gather here to pay our final respects and to return her body to the elements that nurtured and sustained her for 76 years. To our memories we commit: Jeanâs warm and vibrant personality Her skills as a cook and as a hostess Her sense of humour Her openness Her love for her family and her friends And the happiness she found with Enrico who was her best friend and her soulmate.\nWe are glad that she lived and that we knew her We honour her life, accept her departure and cherish her memory. We have enjoyed her company and the times we shared, Now in peace and thoughtfulness we bid her farewell.\nThank you all once again for coming ladies and gentlemen. Please do go if you can to The Mansefield and in the time to come talk often about Jeanâs place in your hearts. In that way sheâll always be a part of who you are.\nIâd like to leave you with a few words from the Native American tradition which may help you in the time to come.\nMay the stars carry your sadness away.\nMay the flowers fill your hearts with beauty.\nMay hope forever wipe away your tears.\nAnd above all, may love make you strong.\nThank you.\nThe photo shows Alison (Norman\u0026rsquo;s wife\u0026rsquo;) and Jean at Enrico\u0026rsquo;s funeral wake.\n","description":"","id":47,"section":"posts","tags":null,"title":"Cheerio and RIP Mum","uri":"http://localhost:1313/RantsAndRaves/posts/2015/08/cheerio-and-rip-mum/"},{"content":"Recently I was tasked to do something that I hadn\u0026rsquo;t done before. I was required to swap out all the existing discs in the two diskgroups +DATA and +FRA, with minimal downtime. Almost all the places I looked seemed to indicate that I had to add the new discs, rebalance, drop the old discs and rebalance again. My colleague, Ian Greenwood, had a much better idea - thanks Ian.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 alter diskgroup DATA add disk -- \u0026#39;/path/to/disk_1\u0026#39; name DISK_1001, \u0026#39;/path/to/disk_2\u0026#39; name DISK_1002, ... \u0026#39;/path/to/disk_n\u0026#39; name DISK_100N -- drop disk -- DISK_0001, DISK_0002, ... DISK_000N -- -- This is Spinal Tap! -- rebalance power 11; Then the same again for +FRA and we were done. Well, I say done, once the rebalance had finished we were done, and the Unix team could then remove completely, the old discs. That did need ASM to be bounced though, which was a bit of a nuisance for the (one) database on the server, but the users were happy to let us take it down.\nJob done and very little messing around. Sometimes, it\u0026rsquo;s helpful to look at the Oracle Manuals before hitting MOS or Google (other web search engines are available - but they are not as good!) for hints when you have new stuff to do.\nYes, I spell disc with a \u0026lsquo;c\u0026rsquo; while Oracle spell it with a \u0026lsquo;k\u0026rsquo;. :-)\n","description":"","id":48,"section":"posts","tags":null,"title":"Add and Drop Discs From ASM in a Single Command","uri":"http://localhost:1313/RantsAndRaves/posts/2015/06/add-and-drop-discs-from-asm-in-a-single-command/"},{"content":"Sometimes an ASM instance hangs for no apparent reason and this causes problems when backing up the ASM Metadata. Running queries against V$ASM_DISK and similar views may also hang. This blog post should go some way to helping diagnose the problem, and providing a fix.\nASM metadata backups on a couple of our servers had been failing, the backups were run from a system called CommVault and the job scheduler there showed that they simply sat at 0% forever, or would have if we allowed them! There were no error messages or codes to speak of - the job simply sits in CommVault at 0% and never ends.\nThis was tracked down to the md_backup command being run from asmcmd. Running the command manually also just hung and the session had to be killed to release it.\nUsing sqlplus on the ASM instance and attempting to query V$ASM_DISK or other similar ASM views, also hung.\nLooking at V$SESSION in the ASM instance, with the following query shows the problem:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 set lines 350 trimspool on pages 300 select sid, state, event, seconds_in_wait, blocking_session from v$session where blocking_session is not null or sid in (select blocking_session from v$session where blocking_session is not null) order by sid; SID STATE EVENT SECONDS_IN_WAIT BLOCKING_SESSION ---------- -------- --------------------- --------------- ---------------- 15 WAITING enq: DD - contention 73683 254 16 WAITING enq: DD - contention 15692 254 17 WAITING enq: DD - contention 117109 254 93 WAITING enq: DD - contention 61107 254 95 WAITING enq: DD - contention 242327 254 96 WAITING enq: DD - contention 68731 254 167 WAITING GPnP Get Item 2471652 172 WAITING enq: DD - contention 117109 254 173 WAITING enq: DD - contention 147026 254 176 WAITING enq: DD - contention 37787 254 177 WAITING enq: DD - contention 658138 254 178 WAITING enq: DD - contention 42238 254 251 WAITING enq: DD - contention 315711 254 253 WAITING enq: DD - contention 97075 254 254 WAITING rdbms ipc reply 0 167 255 WAITING enq: DD - contention 4140 254 257 WAITING enq: DD - contention 521537 254 17 rows selected. Almost all hung sessions are waiting for sid 254. Sid 254 is itself waiting on 167 which is not waiting on a session, but on the GPnP Get Item event.\nA search of MOS shows that this is caused by an unpublished bug. Note 1375505.1 which mentions killing the gpnpd.bin process with a HUP, which will cause it to immediately restart, refers the reader to note 1392934.1 for full details. That latter note simply says:\n1 kill -HUP Full details indeed! There\u0026rsquo;s not even a pid to be killed.\nIn our specific case, the following was required:\n1 2 3 4 5 6 7 8 ps -ef | grep -i g\\[p\\]npd grid 4084 1 0 Jul 15 ? 04:37:48 /app/gridsoft/11.2.0.3/bin/gpnpd.bin su - grid Password: ****** kill -HUP 4084 It can be seen that this is safe, according to Oracle, and the gpnpd.bin process will be automatically restarted - even on production systems!\n1 2 3 ps -ef | grep -i g\\[p\\]npd grid 19015 1 14 09:23:10 ? 00:00:00 /app/gridsoft/11.2.0.3/bin/gpnpd.bin It can be seen from the above that the daemon is running and has a new pid and start time. If we check in the database again, there will be no waiting sessions and the ASM Metadata backups will work, as will querying V$ASM_DISK etc.\n","description":"","id":49,"section":"posts","tags":null,"title":"Asmcmd or ASM Instance Backups or Queries Hang","uri":"http://localhost:1313/RantsAndRaves/posts/2015/04/asmcmd-or-asm-instance-backups-or-queries-hang/"},{"content":"My server rebooted itself and when it came back up, none of the databases or ASM had restarted. Everything is 11.2.0.3 or 11.2.0.1 with ASM being 11.2.0.3 - so Oracle Restart should have kicked in.\nAs usual, any identifying names, servers, domains, databases etc have been obfuscated to protect the innocent.\n1 2 3 4 5 $srvctl start asm PRCR-1079 : Failed to start resource ora.asm CRS-5017: The resource action \u0026#34;ora.asm start\u0026#34; encountered the following error: ORA-00119: invalid specification for system parameter LOCAL_LISTENER ORA-00132: syntax error or unresolved network name \u0026#39;myserver.mydomain.net:1899\u0026#39; The LOCAL_LISTENER parameter is incorrect, it should be \u0026lsquo;myserver.mydomain.com:1899\u0026rsquo; with a \u0026lsquo;.com\u0026rsquo; and not \u0026lsquo;.net\u0026rsquo;.\nWe have a problem in the spfile that needs to be fixed. Where is it located so that it can be converted to a pfile and corrected? The usual place to check is $ORACLE_HOME/dbs.\n1 2 3 $cd $ORACLE_HOME/dbs $ls spfile* spfile* not found It isn\u0026rsquo;t in the normal location, what does Oracle Restart know?\n1 2 $srvctl config asm -a | grep -i spfile Spfile: +DATA/asm/asmparameterfile/registry.123.123456789 The spfile name may also be listed in the alert.log as part of a startup. It is for me in this case:\n1 2 $grep \u0026#34;^Using.*spfile\u0026#34; alert_+ASM.log | tail -1 Using parameter settings in server-side spfile +DATA/asm/asmparameterfile/registry.123.123456789 Now we have a \u0026ldquo;Catch 22 chicken and egg\u0026rdquo; problem. The spfile is located inside ASM and we can\u0026rsquo;t start ASM to extract and fix it, because we need the (broken) parameter file to start ASM.\nThere are numerous blog postings on the internet that explain how to start ASM, or extract the spfile, when the spfile it needs to start is in ASM, but due to a missing $GRID_HOME/gpnp/myserver/profiles/peer/profile.xml file, those were not an option here. (I think the problem is that the profile.xml is used by RAC only.)\nOn a normal database, you can create a pfile from the spfile even if the instance is not running. Will that work?\n1 2 3 4 5 6 7 8 9 10 $sqlplus / as sysasm Connected to an idle instance. SQL\u0026gt; create pfile=\u0026#39;/home/oracle/pfile.ora\u0026#39; from spfile=\u0026#39;+DATA/asm/asmparameterfile/registry.123.123456789\u0026#39;; create pfile=\u0026#39;/home/oracle/pfile.ora\u0026#39; from spfile=\u0026#39;+DATA/asm/asmparameterfile/registry.123.123456789\u0026#39; * ERROR at line 1: ORA-01565: error in identifying file \u0026#39;+DATA/asm/asmparameterfile/registry.123.123456789\u0026#39; ORA-17503: ksfdopn:2 Failed to open file +DATA/asm/asmparameterfile/registry.123.123456789 ORA-01034: ORACLE not available That was expected, but it had to be tried!\nMethod 1 Maybe a default pfile can be created from the alert log\u0026rsquo;s listing of the non-default startup parameters from the last time it started?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $cd /app/oracle/diag/asm/+asm/+ASM/trace $view alert_+ASM.log ... Using parameter settings in server-side spfile +DATA/asm/asmparameterfile/registry.123.123456789 System parameters with non-default values: large_pool_size = 12M instance_type = \u0026#34;asm\u0026#34; remote_login_passwordfile= \u0026#34;EXCLUSIVE\u0026#34; local_listener = \u0026#34;myserver.mydomain.com:1899\u0026#34; asm_diskstring = \u0026#34;/dev/oracleasm/disks/disk*\u0026#34; asm_diskgroups = \u0026#34;FRA\u0026#34; asm_power_limit = 1 diagnostic_dest = \u0026#34;/app/oracle\u0026#34; USER (ospid: 9251): terminating the instance due to error 119 Instance terminated by USER, pid = 9251 So that\u0026rsquo;s one way of extracting the non-default startup parameters into a temporary pfile, for those awkward times when you cannot get at the spfile to start ASM as the spfile is located within ASM itself. Extract the above settings from the alert.log and startup with that temporary pfile. Once started, create a new spfile, update Oracle Restart and Robert is your mother\u0026rsquo;s brother.\nHowever, depending on how long ASM has been up, what\u0026rsquo;s to say that any of the listed parameters are still valid? After all, since startup, someone changed the LOCAL_LISTENER parameter and it was only when the instance next started up that the foul up became apparent.\nMethod 2 There is another way. Thinking, as they say outside the box (Yuk! I avoid cliches like the plague!) about how tnsnames.ora allows IFILE commands, suggests that perhaps Oracle might allow me to create a pfile which specifies the existing spfile name and lets me set the correct LOCAL_LISTENER parameter to overwrite the broken setting in the spfile?\nI confess, I also had a very vague recollection from way back when spfiles were first introduced, that I had seen/read/heard/tried something like this already, but as mentioned, it was a very vague recollection! Nevertheless, let\u0026rsquo;s create a plain vanilla pfile:\n1 2 3 4 $vi /home/oracle/initASMtemp.ora *.spfile=\u0026#34;+DATA/asm/asmparameterfile/registry.123.123456789\u0026#34; *.LOCAL_LISTENER=\u0026#39;myserver.mydomain.com:1899\u0026#39; The correction goes after the spfile, so that it takes effect rather than being overridden by the broken one in the spfile - assuming this trick works!\n1 2 3 4 5 6 7 8 9 10 11 $sqlplus / as sysasm Connected to an idle instance. SQL\u0026gt; startup pfile=\u0026#39;/home/oracle/initASMtemp.ora\u0026#39;; ASM instance started Total System Global Area 283930624 bytes Fixed Size 2181896 bytes Variable Size 256582904 bytes ASM Cache 25165824 bytes ASM diskgroups mounted We have a running ASM system!\nFix the broken parameter in the existing spfile:\n1 2 3 4 5 6 7 8 SQL\u0026gt; alter system set local_listener=\u0026#39;myserver.mydomain.com:1899\u0026#39; scope=spfile; System altered. SQL\u0026gt; show parameter local NAME TYPE VALUE ---------------- ----------- ------------------------------- local_listener string myserver.mydomain.com:1899 A shutdown and restart later and the spfile is once more working correctly.\n","description":"","id":50,"section":"posts","tags":null,"title":"How to Fix a Broken ASM SPFILE, held within ASM","uri":"http://localhost:1313/RantsAndRaves/posts/2015/04/how-to-fix-a-broken-asm-spfile-held-within-asm/"},{"content":"Have you ever wanted a quick and easy way of converting all those database entries in your tnsnames.ora file, into something that Toad can use to populate the \u0026ldquo;sessions\u0026rdquo; grid? Read on.\nNormally Toad offers you a drop down list of the various database entries in the tnsnames.ora that is being used, however, if your tnsnames.ora file contains an IFILE entry, then Toad doesn\u0026rsquo;t follow the included file, and any aliases defined there - or in subsequent nested IFILEs - will not appear in the drop down list. Working all day on a bad position can really affect your body.\nThe above is true only if your included filename is surrounded by double (or single? Untested!) quotes. This was due to a bug in Toad which has been fixed already. If your included file had no quotes, then the individual entries would appear in the drop down.\nSuffice to say, mine was wrapped in double quotes!\nYou can get around this by connecting to each database in turn and in doing so, this populates the grid of sessions in the \u0026ldquo;New Sessions\u0026rdquo; dialogue. However, this process is a tad on the fiddly side and very boring indeed. Therefore I\u0026rsquo;ve created a utility that allows you to read a tnsnames.ora file and from that, create a file that can be imported to populate the grid.\nYou will need Toad 11.x or higher to be able to import your connections. Previous versions do not have the ability to export and import the sessions. I tested this with Toad 11.6 which is the oldest version I currently have.\nThe utility is based on the tnsnames_checker that I previously announced. You can find that announcement at this location.\nThis utility doesn\u0026rsquo;t make any attempt at semantic validation though, however the lexer or parser may highlight some syntax errors in the tnsnames.ora file. Everything in the tnsnames.ora file which is a database alias entry, will be written to the output file.\nOutput is always to stdout and so, should be redirected to a proper file of your choosing at run time, if you wish to import the results that is.\nDownload and Install Source code is available on GitHub. But you do not need it if you don\u0026rsquo;t intend to build or modify the utility.\nYou need to download the compiled code. It is then a simple case of unzipping it and running the tns2toad.cmd file if you are on Windows, or the tns2toad.sh script if you are on some form of Unix. Your PATH is assumed to contain the location of the java executable. You can check by running the java -version command. If it barfs, you need to sort out your PATH.\nJava 1.6 (aka Java 6) is the minimum required version of Java. The software has been tested with Oracle\u0026rsquo;s Java 6 and Java 7. It should work with Java 8 as all versions are supposed to be backward compatible, but I have not been able to test it with OpenJDK\u0026rsquo;s version of Java.\nTns2toad is a command line utility and you should run it from a DOS or shell session while your current directory is the location where you unzipped it to.\nThe following is a list of files that you should find:\nantlr-4.4-complete.jar : the ANTLR4 runtime support for the parser section of the code. tns2toad.jar : the runtime support for the actual utility itself. tns2toad.cmd : a batch file for Windows users. tns2toad.sh : a shell script for Linux and/or Unix users. If you need to change the classpath, edit the latter two files as necessary to suit your system.\nParameter Details If you run the utility with an invalid parameter, the correct usage details will be displayed, as follows:\n1 2 3 4 5 6 7 8 9 10 11 C:\\Software\\ANTLR\\TNS2Toad\\test \u0026gt; tns2toad --help Invalid option \u0026#39;--help\u0026#39; Usage: tns2toad filename Options: --oracle_home \u0026#34;path to Oracle home\u0026#34; The default oracle home to be used. --user The default user for all connections. Parameter: filename. The tnsnames.ora file to be parsed. Tns2toad requires the options to be specified first and the mandatory file name last. You cannot mix and match. Options may be specified in any letter case, lower, upper or mixed. The options are as follows, and all are optional:\n--help Displays usage details. Technically this is an error, but any incorrect option will display the usage details as shown above. --oracle_home If you wish, you can set all entries in the generated file to use the same Oracle Home folder. This folder should be specified in full, on the command line. If omitted, the import file will not specify an Oracle Home and Toad will use whatever you have configured as the default when you run the import. If there are spaces or special characters in the path name, wrap the full path in double quotes. Beware, it is not likely that Oracle will work correctly from a folder which has spaces in the path name. --user If you wish, you can set each and every one of the imported sessions to use the same user name. Obviously, a tnsnames.ora file doesn\u0026rsquo;t have user details, so by default, there will be none. If you use the same user on each (or most) of your connections, specify it here and save some typing later on in life. Running the Utility As mentioned above, the utility reads a tnsnames.ora file and writes a Toad connections export file to stdout so you will need to trap the output and redirect it to a file of your choosing.\nTo run the utility with all defaults set:\n1 tns2toad c:\\tns_admin\\tnsnames.ora \u0026gt;c:\\myToadSessions.txt That will set OracleHome and User to blank and ConnectAs set to \u0026ldquo;Normal\u0026rdquo;.\nTo run the utility with a specific Oracle Home for all connections:\n1 tns2toad --oracle_home c:\\oracle\\product\\11gr2\\client1 c:\\tns_admin\\my_tnsnames.ora \u0026gt;c:\\myToadSessions.txt That will set OracleHome to the supplied value for all connections, User will be set to blank and ConnectAs will be set to \u0026ldquo;Normal\u0026rdquo;.\nTo run the utility with a specific database login for all connections:\n1 tns2toad --user system c:\\tns_admin\\tnsnames.ora \u0026gt;c:\\myToadSessions.txt That will set OracleHome to blank, User will be set to \u0026ldquo;system\u0026rdquo; for all connections and ConnectAs will be set to \u0026ldquo;Normal\u0026rdquo;. If the user supplied is \u0026ldquo;sys\u0026rdquo; then the ConnectAs would be set to SYSDBA.\nOutput File Format Each entry in the output file will resemble the following. There will be one section for each database alias in the input file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 [LOGIN1] User=SYS Server=barney AutoConnect=0 OracleHome=c:\\oracle\\home SavePassword=0 Favorite=0 SessionReadOnly=0 Alias= Host= InstanceName= ServiceName= SID= Port= LDAP= Method=0 Protocol=TNS ProtocolName=TCP Color=8421376 ConnectAs=SYSDBA LastConnect=19600407031549 RelativePosition=0 GUID= There will be some other text at the end of the output which is required for Toad to recognise the file and to import it, but that is not shown here. The following entries are of note:\n[LOGINn] this is the section header. The numeric suffix will increase by 1 from 1 for each new entry. There will be one of these sections for each database alias found in the tnsnames.ora file. RelativePosition this is set to the [LOGINn] value minus 1. The grid starts numbering its entries from zero while the logins start numbering at 1. If your grid is currently sorted into any desired order, the RelativePosition value will be ignored and the entry will be placed in the grid according to your chosen sort order. User Normally blank and if so, you will be prompted at login to supply a user name and password for the connection. May be populated if you specified the --user option on the command line. Server this is the alias name, including domain name if present, read from the tnsnames.ora file. In the event that an entry in tnsnames.ora consists of an alias list, each one will get a separate entry in the output file. OracleHome Normally blank and if so, Toad will use the default Oracle Home at runtime, for the connection. May be populated if you specified the --oracle_home option on the command line. ConnectAs This will normally be \u0026ldquo;Normal\u0026rdquo; but if a --user sys option was specified on the command line, it will change to \u0026ldquo;SYSDBA\u0026rdquo; as all SYS connections must be as sysdba. Warning Don\u0026rsquo;t tell Bert if you use SYS though! :-)There is no option available to allow connections as SYSOPER. LastConnect this is set to something resembling my date and time of birth in YYYYMMDDHHMMSS format. Yes, I am that old! This value should allow you to sort your grid by the Last Connect column and keep all the new tnsnames entries at the bottom. Until you need them of course. Color Similar to LastConnect above, this is set purely to separate the imported entries from the ones you added yourself.As far as I am aware, no-one in the world actually likes the teal colour - except a company I used to work for that is, sadly now no longer in business - so it should be safe enough to assume that it will indeed help keep the imported entries separate from your manually entered ones. Obviously, passwords are not part of a tnsnames.ora file, so the utility is unable to set those up for you. Equally, these are encrypted based on your login to your computer amongst other things, and so, it\u0026rsquo;s practically impossible for tns2toad to be able to set passwords.\nAnd finally, at least for the UK, no, I haven\u0026rsquo;t spelt favorite or color wrongly, Toad has! But that\u0026rsquo;s how it has to be when dealing with \u0026ldquo;foreigners\u0026rdquo; ;-)\nImporting the Results The following applies to Toad 11.6 because that\u0026rsquo;s how I tested it, later versions may be slightly different. Older versions will most likely not have the ability to import connection details, however, I have a plan \u0026hellip;.see below.\nStart Toad. Click Session -\u0026gt; New Connection When the dialogue appears, there will be two buttons showing icons resembling a 3.5\u0026quot; floppy disc, with a blue arrow - like the ones you can see somewhere close to here, over on the right. You want the one with the arrow pointing out of the disc. Hover over the icon and it should pop up a hint that says \u0026ldquo;import\u0026rdquo;. Click it. On the subsequent \u0026ldquo;Connections Import File\u0026rdquo; dialogue, navigate in the usual manner to the location where you saved your file. Select it, and click the \u0026ldquo;open\u0026rdquo; button. After a short delay, the grid should be showing all the new connections. If your grid is sorted by the Last Connect column, the new connections will be added at the bottom. Strangely, on my Toad at least, the date doesn\u0026rsquo;t appear but maybe it\u0026rsquo;s because it\u0026rsquo;s such a long long time ago!\nAhem! No, it\u0026rsquo;s because there was a bug in the LastConnect value, it had two extra digits! This has been fixed.\nMy grid now looks like the following, with the newly imported sessions nicely collected at the bottom.\nDeleting Extraneous Entries In the unlikely event that you imported some sessions that you really do not need, simply select them (click, CTRL-click etc) and press the DEL key to delete the unwanted ones.\nDid I mention a Plan? Prior to Toad 11.x, it wasn\u0026rsquo;t possible to import connections. There is a way to get around this, but I can\u0026rsquo;t accept responsibility for foul ups and I have not tested this method. The format and content of the connections export file and the connections.ini file are remarkably similar.\nYou need to shut down Toad, and then find the user files location on your PC and, with Toad closed, copy the connections.ini file to a safe place.\nOpen the connections.ini file and replace the contents with the contents of the file generated by tns2toad. Before saving the file, scroll to the bottom and remove _only_the following line:\n1 \u0026lt;\u0026lt;Split file here. CONNECTIONS.INI above, CONNECTIONPWDS.INI below\u0026gt;\u0026gt; Save the file and exit. When you start Toad, the list of connections on the grid should be set to the ones you generated - without any passwords.\nYou could try appending the generated file contents to the existing connections.ini file but note that you will/may have to renumber the various section headers - I do not know how Toad copes when you import two connections with the same LOGINn name.\nEnjoy.\n","description":"","id":51,"section":"posts","tags":null,"title":"Convert a Tnsnames.ora File to a Toad Session Import File","uri":"http://localhost:1313/RantsAndRaves/posts/2014/12/convert-a-tnsnames-ora-file-to-a-toad-session-import-file/"},{"content":"I have made available for free a utility that will parse a tnsnames.ora file and report back on anything that it doesn\u0026rsquo;t like such as duplicate entries, invalid characters, redefinitions of parameters, errors etc etc.\nVersion 0.5 released to_date('08/06/2015', 'dd/mm/yyyy')**. Version 0.4 released to_date('07/11/2015', 'dd/mm/yyyy')**. Version 0.3 released to_date('06/12/2014', 'dd/mm/yyyy')**. ** Oracle joke! :-) ** Also avoids Date confusion for my American readers. :-)\nIt\u0026rsquo;s a small utility, based on the ANTLR4 parser/compiler generator tool. I\u0026rsquo;ve had an interest in compilers and parsers for many many years - more than I care to remember - but this is only my second ever parser tool of any great use.\nTo run the utility, you will need a Java 7 (aka Java 1.7) or higher JRE.\nTo compile the utility, you will need a Java 7 (aka Java 1.7) or higher JDK.\nThe downloadable version has been compiled with Java 10 but with the command line option to allow JREs from Java 7 upwards to be used at runtime.\nYou will need a Java 7 (which is actually version 1.7 when you run java -version) or higher. It has been tested with version 1.7. I don\u0026rsquo;t have 1.8 yet. (I loathe don\u0026rsquo;t actually like Java but in this case, I gritted my teeth and got dirty made an exception!)\nIt will run on version 1.6 if you compile the source (from GitHub) yourself, however, the download was compiled with version 1.10 and the --release 7 setting so will need at least Java 1.7\u0026rsquo;s JRE to run.\nIt works on Windows or Linux.\nThe compiled program itself is downloadable from here, and there is a small pdf file which tries to explain it as well. Enjoy. Source code is now available in my GitHub repository.\nThe grammar for a Tnsnames.ora file is based on the Oracle specification for an 11gR2 file.\nChange Log Version 0.5\nANTLR 4.7.1 upgrade. Now uses the absolute latest version of ANTLR4. Some changes internally required for the 4.7.1 update. ANTLRInputStream deprecated, changed to use CharStreams instead. Now compiled with Java 10, but with --release 7 specified to enable JREs from Java 7 upwards to be used at run time. Version 0.4\nDownloadable binaries now compiled with Java 7 aka Java version 1.7 instead of Java 6/1.6. The code will still compile with Java 6/1.6, it\u0026rsquo;s just that I no longer have that version to build the download with. Allows IFILE entries to contain filenames delimited by single, double or no quotes at all. The list of IFILEs at the end is in a better format. (Other opinions are available!) Allows host, service, instance etc names to contain reserved words. For example, a service of \u0026ldquo;someservice.service.domain.co.uk\u0026rdquo; is now acceptable. Version 0.3 produced errors. This was issue 232 on GitHub - see https://github.com/antlr/grammars-v4/issues/232 The tnsnames_checker.sh file was, in version 0.3, pretty much useless as it was simply a copy of the Windows file, tnsnames_checker.cmd. Major oops! This has been corrected and tested. Version 0.3\nFirst version released into the wild. A Gory Test Output Log 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 ================================================================================ Tnsnames Checker - Version 0.4. ================================================================================ Using TNSNAMES specification version for Oracle 11gR2, as defined at: http://docs.oracle.com/cd/E11882_01/network.112/e10835/tnsnames.htm. Using grammar defined in tnsnamesParser.g4, downloadable from: https://github.com/NormanDunbar/grammars-v4/tree/master/tnsnames. The official ANTLR version is downloadable from: https://github.com/antlr/grammars-v4/tree/master/tnsnames. (This may not be as up to date if there are any pull requests outstanding.) ================================================================================ Parsing tnsnames file \u0026#39;/home/norman/tnsnames/tnsnames.test.ora\u0026#39; ... ================================================================================ -------------------------------------------------------------------------------- Syntax checking ... -------------------------------------------------------------------------------- line 210:7 extraneous input \u0026#39;)\u0026#39; expecting \u0026#39;(\u0026#39; line 310:15 no viable alternative at input \u0026#39;rmancatalog=()\u0026#39; Syntax checking complete. -------------------------------------------------------------------------------- Semantic checking ... -------------------------------------------------------------------------------- Line 12:1 Listener alias found: LSNR_FRED *** INFO: 0, WARNING: 0, ERRORS: 0 Line 21:1 Listener alias found: LSNR_HEXFRED *** INFO: 0, WARNING: 0, ERRORS: 0 Line 30:1 Listener alias found: LSNR_OCTFRED *** INFO: 0, WARNING: 0, ERRORS: 0 Line 39:1 Listener alias found: LSNR_MIXFRED *** INFO: 0, WARNING: 0, ERRORS: 0 Line 48:1 Listener alias found: LSNR_MESSFRED Line 50:40 ERROR: dotQuad \u0026#39;0xFe0\u0026#39; in \u0026#39;10.0xFe0.029.0X27\u0026#39; out of range 0 - 254. *** INFO: 0, WARNING: 0, ERRORS: 1 Line 57:1 Listener alias found: LSNR_MESSFRED Line 57:1 ERROR: Duplicate alias - LSNR_MESSFRED Line 59:40 ERROR: dotQuad \u0026#39;0xFe0\u0026#39; in \u0026#39;10.0xFe0.029.0X27\u0026#39; out of range 0 - 254. *** INFO: 0, WARNING: 0, ERRORS: 2 Line 66:1 Listener alias found: LSNR_WILMA Line 67:46 WARNING: PROTOCOL parameter redefines PROTOCOL parameter at line 67. Line 67:62 WARNING: KEY parameter redefines KEY parameter at line 67. *** INFO: 0, WARNING: 2, ERRORS: 0 Line 74:1 Listener alias found: lsnr_barney *** INFO: 0, WARNING: 0, ERRORS: 0 Line 86:1 Listener alias found: lsnr_betty Line 87:3 WARNING: Missing ADDRESS_LIST, 2 ADDRESS entries found. *** INFO: 0, WARNING: 1, ERRORS: 0 Line 107:1 Database alias found: alias_1,alias_2.world,alias3.dunbar-it.co.uk Line 110:8 WARNING: LOAD_BALANCE parameter redefines LOAD_BALANCE parameter at line 109. Line 119:84 WARNING: Port number, 80 \u0026lt; 1024. May be invalid. Line 120:52 ERROR: dotQuad \u0026#39;0\u0026#39; in \u0026#39;0.2.3.400\u0026#39; out of range 1 - 254. Line 120:52 ERROR: dotQuad \u0026#39;400\u0026#39; in \u0026#39;0.2.3.400\u0026#39; out of range 0 - 254. Line 125:21 WARNING: PROTOCOL parameter redefines PROTOCOL parameter at line 123. Line 127:21 WARNING: PORT parameter redefines PORT parameter at line 126. Line 128:21 WARNING: HOST parameter redefines HOST parameter at line 124. Line 130:85 ERROR: Port number 65536. Out of range 1024 - 65535. Line 135:21 WARNING: PROTOCOL parameter redefines PROTOCOL parameter at line 133. Line 136:21 WARNING: KEY parameter redefines KEY parameter at line 134. Line 143:28 WARNING: SEND_BUF_SIZE parameter redefines SEND_BUF_SIZE parameter at line 141. Line 144:28 WARNING: RECV_BUF_SIZE parameter redefines RECV_BUF_SIZE parameter at line 142. Line 149:21 WARNING: PROTOCOL parameter redefines PROTOCOL parameter at line 146. Line 156:21 WARNING: ARGV0 parameter redefines ARGV0 parameter at line 154. Line 157:21 WARNING: PROGRAM parameter redefines PROGRAM parameter at line 153. Line 158:21 WARNING: ARGS parameter redefines ARGS parameter at line 155. Line 163:21 WARNING: ARGV0 parameter redefines ARGV0 parameter at line 162. Line 164:84 WARNING: LOCAL parameter redefines LOCAL parameter at line 164. Line 164:96 WARNING: ADDRESS parameter redefines ADDRESS parameter at line 164. Line 164:107 WARNING: PROTOCOL parameter redefines PROTOCOL parameter at line 164. Line 169:28 WARNING: PIPE parameter redefines PIPE parameter at line 168. Line 170:28 WARNING: SERVER parameter redefines SERVER parameter at line 167. Line 171:28 WARNING: PROTOCOL parameter redefines PROTOCOL parameter at line 166. Line 175:13 ERROR: SDU value 256. Out of range 512 - 65535. Line 176:13 WARNING: SDU parameter redefines SDU parameter at line 175. Line 177:13 WARNING: SDU parameter redefines SDU parameter at line 175. Line 178:13 WARNING: SDU parameter redefines SDU parameter at line 175. Line 178:13 INFO: SDU value 8192. This is the default setting. Line 179:13 WARNING: SDU parameter redefines SDU parameter at line 175. Line 180:13 WARNING: SDU parameter redefines SDU parameter at line 175. Line 180:13 ERROR: SDU value 65536. Out of range 512 - 65535. Line 183:28 WARNING: SID parameter redefines SID parameter at line 183. Line 183:41 WARNING: SID parameter redefines SID parameter at line 183. Line 186:15 WARNING: SERVER parameter redefines SERVER parameter at line 184. Line 191:15 WARNING: HS parameter redefines HS parameter at line 188. Line 192:15 WARNING: UR parameter redefines UR parameter at line 190. Line 193:15 WARNING: SERVICE_NAME parameter redefines SERVICE_NAME parameter at line 185. Line 194:15 WARNING: SERVICE_NAME parameter redefines SERVICE_NAME parameter at line 185. Line 196:13 WARNING: RETRY_COUNT parameter redefines RETRY_COUNT parameter at line 181. Line 199:13 WARNING: SDU parameter redefines SDU parameter at line 175. Line 199:13 ERROR: SDU value 65536. Out of range 512 - 65535. Line 200:13 WARNING: SEND_BUF_SIZE parameter redefines SEND_BUF_SIZE parameter at line 112. Line 201:13 WARNING: RECV_BUF_SIZE parameter redefines RECV_BUF_SIZE parameter at line 115. Line 203:8 WARNING: Missing ADDRESS_LIST, 4 ADDRESS entries found. Line 205:48 ERROR: dotQuad \u0026#39;255\u0026#39; in \u0026#39;10.0.254.255\u0026#39; out of range 0 - 254. *** INFO: 1, WARNING: 37, ERRORS: 7 Line 217:1 Database alias found: barney *** INFO: 0, WARNING: 0, ERRORS: 0 Line 230:1 Database alias found: alias3.dunbar-it.co.uk Line 230:1 ERROR: Duplicate alias - alias3.dunbar-it.co.uk *** INFO: 0, WARNING: 0, ERRORS: 1 Line 243:1 Database alias found: pebbles *** INFO: 0, WARNING: 0, ERRORS: 0 Line 263:1 Database alias found: pebbles_two Line 276:11 WARNING: FAILOVER_MODE parameter redefines FAILOVER_MODE parameter at line 267. Line 281:17 WARNING: TYPE parameter redefines TYPE parameter at line 280. Line 282:17 WARNING: METHOD parameter redefines METHOD parameter at line 277. Line 284:17 WARNING: BACKUP parameter redefines BACKUP parameter at line 278. Line 285:17 WARNING: DELAY parameter redefines DELAY parameter at line 279. Line 286:17 WARNING: RETRIES parameter redefines RETRIES parameter at line 283. Line 287:17 WARNING: TYPE parameter redefines TYPE parameter at line 280. *** INFO: 0, WARNING: 7, ERRORS: 0 Line 299:1 Database alias found: alias.foo.bar *** INFO: 0, WARNING: 0, ERRORS: 0 ================================================================================ Parsing Information: =================== INFORMATION : 1 PARSER WARNINGS : 47 PARSER ERRORS : 11 DUPLICATE ENTRIES: 2 Duplicate[0] = \u0026#39;LSNR_MESSFRED\u0026#39; Duplicate[1] = \u0026#39;alias3.dunbar-it.co.uk\u0026#39; ================================================================================ -------------------------------------------------------------------------------- IFILE List: -------------------------------------------------------------------------------- Please run the tnsnames_checker script on the following 3 files: IFILE[0] = \u0026#39;/this/is/a/double_quoted/ifile/entry/tnsnames.ora\u0026#39; (not found on this computer.) IFILE[1] = \u0026#39;/this/is/a/single_quoted/ifile/entry/tnsnames.ora\u0026#39; (not found on this computer.) IFILE[2] = \u0026#39;/this/is/an/unquoted/ifile/entry/tnsnames.ora\u0026#39; (not found on this computer.) End of IFILE List. Semantic checking complete. ================================================================================ Parsing tnsnames file \u0026#39;/home/norman/tnsnames/tnsnames.test.ora\u0026#39; complete. ================================================================================ ","description":"","id":52,"section":"posts","tags":null,"title":"Tnsnames Checker Utility","uri":"http://localhost:1313/RantsAndRaves/posts/2014/12/tnsnames-checker-utility/"},{"content":"Ever wanted to set a variable to the name of another variable, and from there, somehow get the value of the other variable? I did, recently, and this is what I had to do.\nI work with numerous databases but of all the ones I have, there are only 18 different types and these cover all possible (at present) systems in production or development. The first 3 characters of $ORACLE_SID define the system name and we use a script that duplicates any of 18 template databases to create the desired new one. The script has to run on both the Bash and Korn shells, and must validate that the new database is going to be built from the correct database template - to catch DBA finger troubles! :)\nThe template databases exist as RMAN backups and have been created with the correct options, default tablespaces, users and all the desired options for the system, so after the build is complete, and the post-build validation script has been executed, the new database can be handed over to the users without any further changes. Passwords are set up randomly after the build has completed, by the build script, so anyone who knows the template database passwords won\u0026rsquo;t know them on a new build.\nThe script is passed a template database name on the command line and this needed to be validated to ensure that ORACLE_SID, the new database name, was permitted to be built with that particular template.\n1 $ myscript.sh -t XXX ... The script already validates XXX as one of the 18 allowed values, but a recent change means that now, the script needs to carry out 1 of 18 different validations depending on the XXX parameter passed at run time.\nHow difficult could it be? As it turned out, not very - once you know about variable indirection in bash, and the following code also works in the Korn shell which I also needed it to work on.\nThe Obligatory Hello World Example The following example can be typed in at the Bash or Korn shell prompt.\n1 2 3 4 5 6 ABC=\u0026#34;Hello World\u0026#34; X=\u0026#34;ABC\u0026#34; eval Y=\\$\u0026#34;${X}\u0026#34; echo \u0026#34;${Y}\u0026#34; Hello World Variable $ABC holds the value I am after, $X holds the name of the variable that holds the value I want. The eval function sets variable $Y to the value held in the variable whose name is held in $X. So $Y is set to \u0026ldquo;Hello World\u0026rdquo;. Simple!\nThe Database Creation Script In my database creation script, all I had to do was set up a one variables for each of the different template databases allowed. The name of the variable had to match the name of the template database that would be passed in, in upper case, by the -t parameter.\n1 duplicate_database -t T_7 -o $ORACLE_SID ... In the validation function, all I had to do was get the correct list of valid database prefixes into a separate variable using indirection, and from there it was a simple case of greping to see if the desired system prefix was present in the allowed list.\nThe code looked remarkably similar to the following and, as ever, systems and database names are not based on reality - to protect the innocent!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 ... #---------------------------------------------------------------------------- # The following list of variables holds the permitted 3 character prefix for # a database name created with the appropriate template. # # For example, If the template is T_1, only databases named SYSxxxxx and # DBAxxxxx are valid. #---------------------------------------------------------------------------- T_1=\u0026#34;|SYS|DBA|\u0026#34; T_2=\u0026#34;|PAY|HMN|SOP|\u0026#34; ... T_18=\u0026#34;|PRE|RTL|XXX|DAT|\u0026#34; #---------------------------------------------------------------------------- # A function to validate the database name passed in against a database type. #---------------------------------------------------------------------------- # $1 is the Database Template passed on the command line with \u0026#34;-t T_1\u0026#34; etc. # $2 is the database name to be built. (aka ORACLE_SID) # # Note, this uses indirection and so the database type MUST match the name of a # validation variable set up previously. # # Return Code: # $? = 0 if first three characters of ORACLE_SID are valid for the template. # $? = 1 if not. #---------------------------------------------------------------------------- validate_db_template() { #---------------------------------------------------- # Make sure both parameters come to us in upper case. #---------------------------------------------------- DB_TEMPLATE=`echo $1 | tr \u0026#39;[:lower:]\u0026#39; \u0026#39;[:upper:]\u0026#39;` DB_PREFIX=`echo $2 | tr \u0026#39;[:lower:]\u0026#39; \u0026#39;[:upper:]\u0026#39;` #---------------------------------------------------- # DB_TEMPLATE now holds the upper case T_1 .. T_18 # name of a template database that we will use to # build a new database as per ORACLE_SID/$2/DB_PREFIX. # We now need to get a list of the valid database # names for this template. #---------------------------------------------------- eval VALID_LIST=\\$\u0026#34;${DB_TEMPLATE}\u0026#34; #---------------------------------------------------- # We only need the first 3 characters of the DB name. #---------------------------------------------------- DB_PREFIX=`echo \u0026#34;${DB_PREFIX}\u0026#34;|cut -c 1-3` #---------------------------------------------------- # Then check if it is in VALID_LIST. # $? = 0 if found, 1 if not. #---------------------------------------------------- echo \u0026#34;${VALID_LIST}\u0026#34; | grep -qi \u0026#34;${DB_PREFIX}\u0026#34; return $? } To validate that a passed in template database permits the database named as per ORACLE_SID to be created, it was a simple matter to call validate_db_template passing the desired parameters, and check the value in $? after the function had returned:\n1 2 3 4 5 6 7 8 9 10 ... validate_db_template \u0026#34;${TEMPLATE}\u0026#34; \u0026#34;${ORACLE_SID}\u0026#34; if [ \u0026#34;${?}\u0026#34; != \u0026#34;0\u0026#34; ] then echo \u0026#34;*** ERROR: validate_db_template failed\u0026#34; echo \u0026#34;*** ${ORACLE_SID} cannot be built with a database template of \\\u0026#34;${TEMPLATE}\\\u0026#34;.\u0026#34; exit \u0026#34;${ERROR_DBNAME_VALIDATION_FAILED}\u0026#34; fi ... Using indirection in this manner saved me the horror of typing in a huge case statement where I would set the VALID_LIST according to what the current template name was. In addition, future amendments, perhaps for Oracle 12c, will simply require a new template variable and allowed systems to be created, no code need be changed.\nYou might not need to have a database duplication script like I have, but I\u0026rsquo;m sure that the ability to get the value of a variable whose name is unknown until run time, might prove useful.\nHave fun.\n","description":"","id":53,"section":"posts","tags":null,"title":"Shell Variable Indirection in a Database Build Script","uri":"http://localhost:1313/RantsAndRaves/posts/2014/11/shell-variable-indirection-in-a-database-build-script/"},{"content":"There is, out there in Oracle Land, a silent database killer. You never know when it will strike and it affects all databases right up to and including 12c. When it strikes, it does so silently, there is no evidence of its passing, until it is far too late.\nWhat is This Killer? The database killer is any code which runs in NOLOGGING or UNRECOVERABLE or, in some cases prior to 11g, DIRECT PATH loads.\nThe following are examples of these sorts of commands:\nINSERT /\\*+ Append \\*/ INTO ... (11g is not affected by this.) Any DML after ALTER .... UNRECOVERABLE; Any DML after ALTER .... NOLOGGING; CREATE ... UNRECOVERABLE; CREATE ... NOLOGGING; Alternatively, using SQL Loader with any of the following parameters in the control file:\nUNRECOVERABLE OPTIONS(DIRECT=TRUE) Or, running SQL Loader with the following command line option:\nsqlldr direct=true Useful MOS Documents The following documents will prove very useful if you are affected by this silent killer.\n290161.1 : The Gains and Pains of Nologging Operations. 269274.1 : Check For Logging / Nologging On DB Object(s). 751249.1 : Dbv-111 Ora-1219 Sys.X$Dbms_dbverify. (Or, in English, what to do when DBV on a standby data file throws an OCI error ORA-01219.) 472231.1 : How to identify all the Corrupted Objects in the Database with RMAN. 605234.1 : How to Copy ASM datafiles from Primary to Standby Database on ASM using RMAN. Prevention is Better Than Cure The following command must be executed on the primary and standby databases, if you have any.\n1 alter database force logging; Downtime is not required and if there is any of the NOLOGGING work in progress, the ALTER DATABASE will hang until such time as the NOLOGGING work completes. A message will be logged to the alert.log advising you of this.\nThe command executes quite happily on a physical standby database without the need to cancel recovery.\nThe result of the above command is that any attempt by an SQL operation to attempt a NOLOGGING operation will be ignored, and full logging will take place, resulting in the safety of your data and the continuing viability of the standby database(s).\nHow Can I Tell if I\u0026rsquo;m Infected? The following SQL statement will hopefully return no rows. However, if there are any rows returned, those are the data files that have been updated at some point in the past, with a NOLOGGING operation of some kind. They are sitting there, silently, waiting for an excuse to kill your database.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 alter session set nls_date_format = \u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39;; select file#, name, unrecoverable_change#, unrecoverable_time from v$datafile where unrecoverable_change# \u0026lt;\u0026gt; 0 order by file#; The `V$DATAFILE` view only holds the most recent unrecoverable change details against each datafile. There may have been others in the past, but the one you see is the most recent. No rows selected is good, but sometimes, you may see something like this: FILE# NAME UNRECOVERABLE_CHANGE# UNRECOVERABLE_TIME ----- ----------------------------------- --------------------- ------------------- 6 +DATA/orcl/data/users.293.825959933 10917504423565 13/10/2014 15:00:34 As usual, database names etc have been changed to protect the innocent!\nIt makes no difference if the files are in ASM, as in this example, or on file systems, the problem is the same and needs to be attended to urgently.\nIf you find any data files with an unrecoverable change, as above, then the most obvious thing to do is immediately take a full backup of the database (or just the affected data files) because if you have to restore and recover the affected data files, that is the time when the corruptions are introduced into the primary database. The standby database, on the other hand, is already dead - it just doesn\u0026rsquo;t know it yet.\nTo check if your primary database is also dead, and to save you backing up a potentially dead primary, you can run the following in RMAN - there\u0026rsquo;s no need to connect with the catalog, if you use one:\n1 backup validate check logical datafile 6; If you see a non-zero number in the section entitled \u0026ldquo;Marked Corrupt\u0026rdquo; then it means that at some point since the unrecoverable change was applied to the primary database, it has been restored and recovered, and the data that was loaded is now missing. This database is mortally wounded - unless you know what data needs to be reloaded and you will need to \u0026ldquo;uncorrupt\u0026rdquo; those affected datafiles by restoring or recreating them, and their contents.\nIf your database has a standby, then the standby is now not viable to be used as a primary. All the data that have been loaded into the primary database using NOLOGGING, or similar operations, has never been loaded into the standby database. If you detect any \u0026ldquo;corrupt\u0026rdquo; data files on the primary database, as above, with a date \u0026amp; time later than the standby database\u0026rsquo;s creation date \u0026amp; time, then you will need to rebuild or repair the standby database. You can check the CREATED column in V$DATABASE to determine the standby database\u0026rsquo;s creation details.\nYou will be able to run a data guard switch over, either manually, or with OEM or DGMGRL, without error. When the current standby comes up as the new primary database, there will be missing data. If the application and/or developers/vendor continues to run NOLOGGING data loads, then the amount of data loss simply increases. When you switch back to the old primary at some point, everything will be in a mess - and you will not know!\nHow big a mess?\nThere is the data originally loaded into the old primary, that is not present on the old standby - now the new primary - but which is present on the new standby. There is also now, the data being loaded into the new primary, that is not being copied to the new standby (the old primary) - so both databases are missing some data. If you switch back and forthe a few times, the mess just keeps getting messier!\nSo What\u0026rsquo;s Going On? Some vendors and/or developers, and possibly even the odd DBA, have read in the manuals that using NOLOGGING, UNRECOVERABLE or DIRECT PATH operations can \u0026ldquo;save time\u0026rdquo; or \u0026ldquo;improve performance\u0026rdquo; by not logging the data changes to the redo logs for the actual data. Changes to the data dictionary will still be logged and transferred to the standby databases.\nThese same people, however, appear to completely ignore the documentation where it says that \u0026ldquo;whenever you use a NOLOGGING operation, you must take a full database backup\u0026rdquo; immediately afterwards.\nThis is a problem. If you load a table with millions of rows in this manner, the table may extend by adding extents. These new extents will be recorded in the dictionary and will match the actual data usage of the table. The redo logs, however, will only record the changes to the dictionary. The standby database will update its dictionary with the details, but the table on the standby will not be updated with either the data or the new extents - it will not change it\u0026rsquo;s row or extent count at all.\nThe manual states that these operations should only be used on objects that are not required to be recovered. You might have a table that is simply used as the temporary source for a data load before it is transformed and loaded into the correct, final tables. You don\u0026rsquo;t care about recovering the data when it was temporary to begin with. However, the database is not a mind reader and doesn\u0026rsquo;t know when you create a temporary table, use it, and the perhaps drop it, that that object was never required to be recovered. The standby datafiles will be flagged corrupt and the primary datafiles will still log an unrecoverable change.\nAs long as the primary database continues to run happily, the data thus loaded, can be manipulated at will in the normal manner.\nIf the primary database is ever restored and recovered using the archived redo logs created by the data loads, no errors or warnings will be displayed, but the previously loaded data will not be present afterwards. Attempting to access the data after a restore and recover will result in an error similar to the following:\n1 2 3 ORA-01578: ORACLE data block corrupted (file # 6, block # 139) ORA-01110: data file 6: \u0026#39;+DATA/orcl/data/users.293.825959933\u0026#39; ORA-26040: Data block was loaded using the NOLOGGING option A similarly nasty error will occur when the current standby database is opened read only, or switched over to become the new primary, but remember, the error only becomes apparent when the data are accessed in some way - this is really nasty!\nHow to Determine if You Are Affected On the primary database, you can run the dbv utility against all the data files. Use the userid parameter if the data files live in ASM:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 dbv file=\u0026#39;+DATA/orcl/data/users.293.825959933\u0026#39; userid=sys/password DBVERIFY: Release 11.2.0.4.0 - Production on Sat Oct 25 15:03:44 2014 Copyright (c) 1982, 2011, Oracle and/or its affiliates. All rights reserved. DBVERIFY - Verification starting : FILE = +DATA/orcl/data/users.293.825959933 DBV-00201: Block, DBA 25165963, marked corrupt for invalid redo application DBV-00201: Block, DBA 25165964, marked corrupt for invalid redo application DBV-00201: Block, DBA 25165965, marked corrupt for invalid redo application DBV-00201: Block, DBA 25165966, marked corrupt for invalid redo application DBVERIFY - Verification complete Total Pages Examined : 16 Total Pages Processed (Data) : 8 Total Pages Failing (Data) : 0 Total Pages Processed (Index): 0 Total Pages Failing (Index): 0 Total Pages Processed (Other): 7 Total Pages Processed (Seg) : 0 Total Pages Failing (Seg) : 0 Total Pages Empty : 1 Total Pages Marked Corrupt : 4 Total Pages Influx : 0 Total Pages Encrypted : 0 Highest block SCN : 0 (0.0) The above shows that 4 pages (aka blocks) are marked corrupt. This is what you will see on a primary that has been restored and recovered. You will not see any corruption if the primary has not been restored and recovered, the data are still present in that case.\nChecking the standby is equally as simple as non-corrupt data files will happily dbv. However, any data file that is corrupted will cause dbv to abort with then following errors:\n1 2 3 DBV-00111: OCI failure (4157) (ORA-00604: error occurred at recursive SQL level 1 ORA-01219: database not open: queries allowed on fixed tables/views only ORA-06512: at \u0026#34;SYS.X$DBMS_DBVERIFY\u0026#34;, line 22 If you see this, then your standby is invariable corrupt, however, to be absolutely certain, and you can do this on the primary as well - it\u0026rsquo;s quicker than dbv by the way - use RMAN to do a pretend backup with a CHECK LOGICAL clause:\n1 backup validate check logical datafile 6; Note: From 11g onwards, the backup part of the command is not required:\n1 validate check logical datafile 6; A corrupt data file will throw up results similar to the following:\n1 2 3 4 5 6 7 8 9 10 11 12 List of Datafiles ================= File Status Marked Corrupt Empty Blocks Blocks Examined High SCN ---- ------ -------------- ------------ --------------- ---------- 6 OK 4 50 12801 7251912 File Name: +DATA/orcl/data/users.293.825959933 Block Type Blocks Failing Blocks Processed ---------- -------------- ---------------- Data 0 8 Index 0 0 Other 0 12742 The Marked Corrupt column looks interesting, and shows that this data file, on the standby, is indeed corrupt. The data that should be present, is not and this is what silently makes our standby database completely and utterly useless.\nWe can check the extent and reasons for the corruption after an RMAN check by reading from V$DATABASE_BLOCK_CORRUPTION in SQL*Plus:\n1 2 3 4 5 select \\* from v$database_block_corruption; FILE# BLOCK# BLOCKS CORRUPTION_CHANGE# CORRUPTIO ---------- ---------- ---------- ------------------ --------- 6 139 4 7251895 NOLOGGING This shows that there are 4 blocks, beginning at block 139, in data file 6 which have had NOLOGGING operations applied. If there are numerous corruptions, the following might be a better advisory query:\n1 2 3 4 5 6 7 select file#, corruption_type, count(\\*) from v$database_block_corruption group by file#, corruption_type; FILE# CORRUPTIO COUNT( ---------- --------- --------- 6 NOLOGGING 1 If you need to find out what objects are corrupted, the following might be useful, you will need to plug in the starting block numbers from the query above.\n1 2 3 4 5 6 7 8 select owner, segment_type, segment_name from dba_extents where file_id = 6 and 139 between block_id and block_id+blocks; OWNER SEGMENT_TYPE SEGMENT_NAME -------------------- -------------------- -------------------- NORMAN TABLE TEST Rebuilding The Standby Database If you have detected corruptions then your standby databases are useless and need rebuilding. Oracle advise that only the affected data files need to be rebuilt, which is great if the database is huge and only a subset of the files are corrupt. This is documented in MOS note 605234.1, but I have found that it doesn\u0026rsquo;t appear to work.\nI have, however, worked out the problem and a suitable workaround until the bug gets fixed. I have raised an SR on this matter.\nBasically what happens is that the first switch datafile to copy command works correctly. The second one which should switch to the latest datafile copy does not, and switches back to the previous data file, the corrupted one.\nThe work around is to carry out the first switch, list the copies of the affected datafile, and delete the copy that was the original corrupt file.\nThe process starts on the primary with an RMAN backup of a non-corrupted data file. There\u0026rsquo;s no point running this rebuild if the primary data files have been restored and recovered, the data will be missing there too, so check first as advised way back near the start of this article.\n1 2 copy datafile \u0026#39;+DATA/orcl/data/users.293.825959933\u0026#39; to \u0026#39;/tmp/users.dbf; exit This file, /tmp/users.dbf should be copied over to a safe place on the standby server. If you use ftp then remember to transfer in binary. Scp or sftp will only do binary transfers.\nOnce the file exists on the standby, we can use RMAN to get the file copied into ASM and used by the standby:\n1 2 3 4 5 6 7 8 9 10 11 12 13 catalog datafilecopy \u0026#39;/tmp/users.dbf\u0026#39;; sql \u0026#34;alter database recover managed standby database cancel\u0026#34;; switch datafile 6 to copy; # Now running on the file in /tmp. backup as copy datafile 10 format \u0026#39;+DATA\u0026#39;; # Get a copy of the /tmp file into ASM. switch datafile 6 to copy; # Should switch to the new file, but doesn\u0026#39;t. --======================================== -- Insert workaround here. See text above. --======================================== sql \u0026#34;alter database recover managed standby database using current logfile disconnect\u0026#34;; As mentioned above, doing all this and running another validate check logical command, still shows corruptions on the standby, even though there are none on the primary. This is because the second switch datafile to copy command actually switched back to the old corrupted file instead of the new one. To get around this problem, follow these steps:\nSwitch to the /tmp copy as above. Backup the /tmp file into +DATA as above. Switch datafile n to copy; again. This is now using the corrupt file again. List copy of datafile n; will show the /tmp file and the new backup in ASM. Delete copy of datafile n tag \u0026quot;tag for the /tmp copy\u0026quot;; will delete the /tmp file leaving only the new file in ASM - the one we want. Switch datafile n to copy; yet again! This is now correctly using the latest uncorrupted data file in ASM. List copy of datafile n; will show the old corrupt file as the sole remaining copy. Delete copy of datafile n; will get rid of the corrupt file. So, a bit of mucking about to get the problem worked around, and you may need to do a lot more mucking about if you have other copies of the same datafile in RMAN, to get the correct new file switched into use and to get rid of the unwanted corrupt file. I didn\u0026rsquo;t have this problem as my own backups are done as backupsets, not copies.\nDon\u0026rsquo;t let a silent killer destroy your databases, keep force loggging turned on - it\u0026rsquo;s sometimes the only defence against half informed vendors, developers and the odd DBA! ;-)\n","description":"","id":54,"section":"posts","tags":null,"title":"Beware of the Silent Database Killer!","uri":"http://localhost:1313/RantsAndRaves/posts/2014/10/beware-of-the-silent-database-killer/"},{"content":"The following error popped up in an RMAN backup which was attempting to delete archived logs that had been backed up twice, at least, and were created more than two days ago:\n1 2 RMAN-03009: failure of delete command on default channel at 09/12/2014 08:43:50 ORA-15028: ASM file \u0026#39;+FRA/MY\\_DBNAME/archivelog/2014\\_09\\_09/thread\\_1\\_seq\\_35804.3258.857840113\u0026#39; not dropped; currently being accessed (Database names changed to protect the innocent, as usual.)\nDavid Marcos has a blog entry from September 2010 on this very matter at http://davidalejomarcos.wordpress.com/2010/09/07/unable-to-delete-archive-log-and-fra-is-filling/ which suggests killing the various arc processes for the database in question, one by one. Oracle will restart them and the database will stay up.\nNow I don\u0026rsquo;t know about you, but in my case, this was a production database and I have a certain trepidation about killing off background processes at random in the hope that it will cure a fault. That sort of approach may work for Windows faults and problems, but this is a real server, running under Unix. ;-)\nIn the asmcmd shell at Oracle version 11.2 there is the useful lsof command that will return details of who, or what, has a file opened, but in my case, the version was only 11.1, which doesn\u0026rsquo;t have anything much in the way of useful commands!\nI decided, in the absence of lsof to search the alert log for the archived log\u0026rsquo;s filename to see if any of the arc processes recorded having had a problem with the file. They didn\u0026rsquo;t, but I did find the following:\n1 2 3 4 LOGMINER: Begin mining logfile ... +FRA/MY\\_DBNAME/archivelog/2014\\_09\\_09/thread\\_1\\_seq\\_35804.3258.857840113 LOGMINER: Begin mining logfile ... +FRA/MY\\_DBNAME/archivelog/2014\\_09\\_09/thread\\_2\\_seq\\_37567.2138.857839917 LOGMINER: End mining logfile ... +FRA/MY\\_DBNAME/archivelog/2014\\_09\\_09/thread\\_2\\_seq\\_37567.2138.857839917 LOGMINER: Begin mining logfile ... +FRA/MY\\_DBNAME/archivelog/2014\\_09\\_09/thread\\_2\\_seq\\_37568.2810.857841217 From the above, it is plain to see that it is most unlikely that the arc processes would have been the culprits here. One of the other DBAs had started a log mining session for a few files, but had not ended the session for two of them.\nGetting the DBA to run a quick DBMS_LOGMNR.REMOVE_LOGFILE(...) and/or a DBMS_LOGMNR.END_LOGMNR resolved the problem. Thankfully, the session running the log mining was still open, I am not sure what would happen if the session had already been closed, perhaps a full database restart would have been required - but as Wikipedia often says, \u0026ldquo;needs validation\u0026rdquo;.\n","description":"","id":55,"section":"posts","tags":null,"title":"RMAN Error ORA-15028: Archived Log Not Dropped.","uri":"http://localhost:1313/RantsAndRaves/posts/2014/09/rman-error-ora-15028-archived-log-not-dropped/"},{"content":"It was a simple enough request, flashback this particular database to a guaranteed restore point. What could possibly go wrong?\nDatabase names etc have been changed to protect the innocent, and me!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ sqlplus / as sysdba SQL\u0026gt; shutdown immediate Database closed. Database dismounted. ORACLE instance shut down. SQL\u0026gt; startup mount ORACLE instance started. ... Database mounted. SQL\u0026gt; flashback database 2 to restore point GRP_2014_05_17_13_10; However, this raised the following error:\n1 2 ORA-38748: cannot flashback data file 1 - file is in use or recovery ORA-01110: data file 1: \u0026#39;+DATA/XXXXXX/datafile/system.269.759338709\u0026#39; A minor panic then ensued! It\u0026rsquo;s a production database, and I have limited downtime allocated!\nIn the back of my brain, I thought \u0026ldquo;maybe it\u0026rsquo;s a RAC database\u0026rdquo;?\n1 2 3 4 5 6 7 8 9 10 11 12 SQL\u0026gt; show parameter cluster NAME TYPE VALUE -------------------------------------- ----- cluster_database boolean TRUE cluster_database_instances integer 2 SQL\u0026gt; show parameter instance_name NAME TYPE VALUE ------------------------------------ ----------- ------------------------------ instance_name string XXXXXX_1 Ok, It is RAC, there are two instances, and I\u0026rsquo;m on instance XXXXXX_1. It is obviously still in use by the other instance which will be XXXXXX_2 using the naming conventions for this system. I need to bring it all down. The easiest way is to use srvctl:\n1 SQL\u0026gt; exit 1 2 $ srvctl stop database -d XXXXXX -o immediate $sqlplus / as sysdba 1 2 3 4 5 6 7 8 9 10 SQL\u0026gt; startup mount ORACLE instance started. ... Database mounted. SQL\u0026gt; flashback database to restore point GRP_2014_05_17_13_10; Flashback complete. SQL\u0026gt; alter database open resetlogs; Database altered. All I need to do now is start the other instance:\n1 $ srvctl start instance -d XXXXXX -i XXXXXX_2 Job done, and still within the downtime!\nAnd here\u0026rsquo;s a quick tip. Because the database is effectively right back at 17th May 2014, the guaranteed restore point and its numerous archived and flashback logs, which are taking up space in the FRA, can be dropped and recreated to save FRA space:\n1 2 3 4 5 SQL\u0026gt; drop restore point GRP_2014_05_17_13_10; Restore point dropped. SQL\u0026gt; create restore point GRP_2014_05_17_13_10 guarantee flashback database; Restore point created. ","description":"","id":56,"section":"posts","tags":null,"title":"Oracle RAC - Flashback Database","uri":"http://localhost:1313/RantsAndRaves/posts/2014/09/oracle-rac-flashback-database/"},{"content":"Recently at work, I was on an HP server and needed to grep -B3 a large log file to find the three lines prior to a number of Oracle error messages I was searching for, in order to fix things. It turned out that HP-UX doesn\u0026rsquo;t have the -B (or the -A) options. Bummer. A bit of awk fixed the former, but the latter I leave as an exercise for the reader as they say!\n1 awk \u0026#39;/^ORA-/ {for (i = 1; i \u0026lt;= x;) print a[i++]; print; print \u0026#34;---\u0026#34;} {for (i = 1; i \u0026lt; x; i++) a[i] = a[i + 1]; a[x] = $0;}\u0026#39; x=3 file_name In my case, setting x=3 allowed me to display the three lines prior to the error line which itself began with ORA-.\nIn use, do this:\nChange the search term between \u0026lsquo;/\u0026rsquo; in the above, to look for your text. Set x to however many lines before the found text you wish to display. Pass the file name to be searched as the last parameter of the command. That\u0026rsquo;s it. Simple, as they say! You can easily build the above into a shell script and simply pass it the three parameters for x, the search text and the file name. Saves getting all those punctuation characters in the right place!\nDos2Unix for Servers That Don\u0026rsquo;t Have it Installed Dos2Unix is very handy when you have to use a secure file transfer system - sftp, scp etc - to send text files from Windows to a Unix server. Because the servers are securely locked down, unlike your home PC, you cannot simply install packages as and when you like.\nThe problem with the secure protocols is that they send in binary, so Windows text files get transferred with the CR/LF end of line characters intact, which is irritating when you subsequently try to edit the file in vi (emacs is also not installed, thankfully!)\nTo fix the problem, you could edit the file manually and remove the visible ^M characters from each line. You could but why would you, sed and vi allow you to do it in one command:\n1 :1,$s/^M//g Of course, if you simply type in the two characters, ^M, shown above, nothing will be replaced. However if you enter them as:\nCTRL-v CTRL-m\nWhere the above means press the CTRL key and while holding it down, press the v (or m) key.\nNow it works!\n","description":"","id":57,"section":"posts","tags":null,"title":"A Couple of Useful Linux \u0026 HP-UX Tricks","uri":"http://localhost:1313/RantsAndRaves/posts/2014/08/a-couple-of-useful-linux-hp-ux-tricks/"},{"content":"Have you ever wanted to use a tool to parse the manually typed up \u0026ldquo;stuff\u0026rdquo; that lives in a tnsnames.ora file, to be absolutely certain that it is correct? Ever wanted some tool to count all the opening and closing brackets match? I may just have the very thing for you.\nDownload the binary file Tnsnames.Parser.zip and unzip it. Source code is also available on Github.\nWhen unzipped, you will see the following files:\nREADME - this should be obvious! tnsnames_checker.sh - Unix script to run the utility. tnsnames_checker.cmd - Windows batch file to run the utility. antlr-4.4-complete.jar - Parser support file. tnsnames_checker.jar - Parser file. tnsnames.test.ora - a valid tnsnames.ora to test the utility with. The README file is your best friend!\nAll the utility does is scan the supplied input file, passed via standard in, and writes any syntax or semantic problems out to standard error.\nWorking Example There are no errors in the tnsnames.test.ora file, so the output looks like the following:\n1 2 3 4 5 6 ./tnsnames_checker.sh \u0026lt; tnsnames.test.ora Tnsnames Checker. Using grammar defined in tnsnames.g4. Parsing .... Done. Non-Working Example After a bit of fiddling, there are now some errors in the tnsnames.test.ora file, so the output looks like the following:\n1 2 3 4 5 6 7 8 ./tnsnames_checker.sh \u0026lt; tnsnames.test.ora Tnsnames Checker. Using grammar defined in tnsnames.g4. Parsing .... line 5:12 missing \u0026#39;)\u0026#39; at \u0026#39;(\u0026#39; line 8:16 extraneous input \u0026#39;address\u0026#39; expecting {\u0026#39;(\u0026#39;, \u0026#39;)\u0026#39;} Done. You can figure out where and what went wrong from the messages produced.\nHave fun.\n","description":"","id":58,"section":"posts","tags":null,"title":"Tnsnames.ora Parser","uri":"http://localhost:1313/RantsAndRaves/posts/2014/08/tnsnames-ora-parser/"},{"content":"For some strange reason, my Linux Mint Cinnamon desktop suddenly stopped showing the running program buttons on the task bar. I could only switch between running programs with ALT+TAB.\nIt\u0026rsquo;s easy when you know how, but for those who don\u0026rsquo;t, this is what you do:\nRight click on the blank task bar. Select the \u0026ldquo;Add to Panel\u0026hellip;\u0026rdquo; option. (Or similar depending on your version of Mint.) Type \u0026ldquo;Window\u0026rdquo; into the search box. Select \u0026ldquo;Window List\u0026rdquo; from the subsequent list. Click \u0026ldquo;Add\u0026rdquo;. Job done! ","description":"","id":59,"section":"posts","tags":null,"title":"Linux Mint (Cinnamon) Not Showing Programs on Task Bar.","uri":"http://localhost:1313/RantsAndRaves/posts/2014/06/linux-mint-cinnamon-not-showing-programs-on-task-bar/"},{"content":"I think this topic has the longest title of all my postings! Never mind. Have you ever started Thunderbird fetching your emails, and encountered a pop-up message that starts off by saying \u0026ldquo;The message could not be filtered for folder \u0026lsquo;whatever\u0026rsquo; because writing to folder failed\u0026rdquo;? Read on for the cure.\nCredit: go here - https://bugzilla.mozilla.org/show_bug.cgi?id=931303 - then scroll down to Comment 39. A gentleman by the name of Craig Lassen deserves all the kudos for this fix. The problem has been bothering me for some time and I have now fixed it, thanks to this person. Thanks.\nThe problem manifests when the messages are being downloaded. At some point you will see the pop-up appear and be invited to press the OK button. You will have to do this for every message intended, by a filter, to be moved to a different folder. Only affected folders will show the message.\nIf you subsequently click Tools-\u0026gt;Run filters on folder, it will work fine.\nThe solution - keeping it simple, the following is all you have to do:\nExpand all your folders in the tree on the left side. Click on the top entry of the tree - probably your inbox. Press the down arrow to visit each folder in turn. Look for \u0026ldquo;Done\u0026rdquo; on the lower left on the status bar. When you see that (or \u0026ldquo;no messages to download\u0026rdquo;), you can move on. That\u0026rsquo;s it! At each affected folder in your tree traversal, you will notice a couple of things:\nThe words \u0026ldquo;Building summary file for \u0026hellip;.\u0026rdquo; may appear briefly on the status bar. Followed by \u0026ldquo;Done\u0026rdquo;. The folder name becoming bold to indicate unread messages. The unread message count updates itself. Apparently, there has been a minor corruption of some sort in the *.msf files for the affected folders, and this has resulted in the message filters refusing to write into the affected folders.\nDon\u0026rsquo;t thank me, thank Craig!\n","description":"","id":60,"section":"posts","tags":null,"title":"Thunderbird - Message Filters stop Working on Email Download - But Work Ok Manually.","uri":"http://localhost:1313/RantsAndRaves/posts/2014/06/thunderbird-message-filters-stop-working-on-email-download-but-work-ok-manually/"},{"content":"A simple exercise to refresh a schema in a test database caused no end of problems when it hung at 99% complete. The last message on screen indicated that it was importing the Materialized Views (yes, with a \u0026lsquo;Z\u0026rsquo;). After a long time, the DBA running the import killed it, cleaned out, and restarted the whole process. Exactly the same happened.\nBackground The databases in question were both 11.2.0.3 Enterprise Edition. The materialized views were created and refreshed from a table in another database, utilising a database link. Investigation While the impdp was running, breaking into the session and running the status command, repeatedly shows that the import was at 99% completion and so many bytes had been processed. Using the command status=120 we could see that this was not moving on at all as time went by. (The above command runs the status command every 2 minutes.)\nChecking the server for the processes doing the import, DW00, we were able to extract the SID for the process:\n1 2 3 4 5 select sid, serial# from v$session s where paddr = ( select addr from v$process where spid = \u0026amp;PID ); Running the above in SQL*Plus, and entering the Unix process id of the DW00 process for the database, we were able to find the SID and SERIAL# for the hung process.\nLooking in V$SESSION_WAIT for that process, we could see that it was counting up from around 128 seconds, and was waiting on an event named \u0026ldquo;SINGLE-TASK MESSAGE\u0026rdquo;.\nGoogling around for this event seemed to indicated that it was mainly responsible for a process to create a synonym for a table on the other end of a database link, taking up to 10 minutes to fail. Not quite our problem, but we might as well check.\nIn the importing database, we could see the SQL used to create the materialized views and noted the fact that they were all created from data held in another table, on the far end of a database link. Hmmm, suspicious!\nChecking DBA_DB_LINKS we made a note of the HOST column, and in a shell session, tried a tnsping - no response.\nA quick edit to the tnsnames.ora file to add in the appropriate details for these \u0026ldquo;hosts\u0026rdquo; and suddenly, the impdp session completed with no errors. This is good, but what exactly was going on?\nWhat Impdp Does Down a DB Link A test session was set up whereby a materialized view was created with a data source at the far end of a database link. This was refreshed, checked, and exported before being dropped.\nThe database at the far end of the link was set up with a trigger that fired \u0026ldquo;after logon\u0026rdquo; and if the user in question was being logged into, set event 10046 at level 12 - might as well get more data than we need!\nWhen we re-ran the import of the materialized view, we could see in the generated trace file that Oracle was connecting to the database and parsing the SQL statement that was used to refresh the data for the materialized view. Note, it was never executed or fetched from, only parsed. Basically, Oracle was checking that the source of the data was correct enough to be used by the materialized view when refresh time came around, when we were creating the materialized view.\nSo, when you are doing this sort of thing in future, make sure that any database links that exists in the schema(s) owning the materialized views, or that are being imported into the schema, are going to be valid and usable at the time the materialized view itself is imported. If not, you will see this wait and your import will never get past the materialised view section.\nThis problem may well also rear its ugly head if you have tables, views, or PL/SQL code in packages etc that make use of database links.\n","description":"","id":61,"section":"posts","tags":null,"title":"Impdp Hangs Importing Materialized Views","uri":"http://localhost:1313/RantsAndRaves/posts/2014/03/impdp-hangs-importing-materialized-views/"},{"content":"As standard, VirtualBox has a menu option (machine-\u0026gt;Insert CTRL+ALT+Backspace or Insert CTRL+ALT+DEL) which is fine if you need these key combinations sending to the guest and not grabbed by the host, however, how can you send CTRL+ALT+F1 through CTRL+ALT+F8 to a Linux guest OS to get it to startup one of its virtual consoles?\nNormally, those keys would be grabbed and actioned by the host OS rather than being passed to the guest. What to do?\nSimple, since very early versions of VirtualBox (around 1.3.7 or 1.3.8) the Host (Virtual) key is a good substitute for CTRL+ALT and can be used for other special key combinations. On my Linux host OS, the host key is the right side CTRL key.\nBy pressing that, holding it down and pressing BackSpace, for example, I can reset the X system in the guest (Linux) OS.\nThe F1 through F8 keys work in a similar way if you press them after the Host key. In fact, Oracle have set VirtualBox up in such a way as to allow you to press any of the function keys from F1 through F12 and with the host key, will be passed as CTRL+ALT+Fn to the guest.\n","description":"","id":62,"section":"posts","tags":null,"title":"VirtualBox - Sending Special Key Combinations to the Guest OS.","uri":"http://localhost:1313/RantsAndRaves/posts/2014/03/virtualbox-sending-special-key-combinations-to-the-guest-os/"},{"content":"While checking out a dataguarded database prior to being handed over into production, I needed to test that both OEM and dgmgrl could carry out a switchover and failover from the (stand-alone) primary db (ORCL_PDB) to the physical standby database (ORCL_SBY), and back again.\nThe Problem Database and server names have been changed, to protect the innocent, and me!\nOEM had no problems, other than the usual \u0026ldquo;bug\u0026rdquo; whereby the credentials used for the standby server were those for the primary server, but hey, that\u0026rsquo;s OEM for you, it\u0026rsquo;s nothing if not inconsistent! However, when I tried to use dgmgrl I found a small problem.\nWhile I could happily switchover to the standby database, from either server, switching back always failed with the following error:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 DGMGRL\u0026gt; **switchover to \u0026#34;ORCL_PDB\u0026#34;** Performing switchover NOW, please wait... New primary database \u0026#34;ORCL_PDB\u0026#34; is opening... Operation requires shutdown of instance \u0026#34;ORCL_SBY\u0026#34; on database \u0026#34;ORCL_SBY\u0026#34; Shutting down instance \u0026#34;ORCL_SBY\u0026#34;... ORACLE instance shut down. Operation requires startup of instance \u0026#34;ORCL_SBY\u0026#34; on database \u0026#34;ORCL_SBY\u0026#34; Starting instance \u0026#34;ORCL_SBY\u0026#34;... **Unable to connect to database ORA-12521: TNS:listener does not currently know of instance requested in connect descriptor Failed. Warning: You are no longer connected to ORACLE. Please complete the following steps to finish switchover: start up and mount instance \u0026#34;ORCL_SBY\u0026#34; of database \u0026#34;ORCL_SBY\u0026#34;** A quick srvctl start database -d $ORACLE_SID -o mount sorted things out while I investigated the problem.\nData Guard requires that there be an entry in tnsnames.ora for both databases and also for a service name consisting of the database and \u0026ldquo;DGMGRL\u0026rdquo;. I checked.\nBoth TNSNAMES.ORA files have the following, and all entries are configured correctly:\nORCL_PDB ORCL_PDB_DGMGRL ORCL_SBY ORCL_SBY_DGMGRL I had no problems running sqlplus sys/password@orcl_whatever as sysdba for any of the above.\nLooking in the listener logfile for the standby server\u0026rsquo;s listener, I noticed that there were entries where the error code shown above (ORA-12521 and also ORA-12514)) were present, however, there was a problem with the host_name.\n1 2 3 4 msg time=\u0026#39;yyyy-mm-ddThh:mm:ss.fff+00:00\u0026#39; org_id=\u0026#39;oracle\u0026#39; comp_id=\u0026#39;tnslsnr\u0026#39; type=\u0026#39;UNKNOWN\u0026#39; level=\u0026#39;16\u0026#39; host_id=\u0026#39;sby_server\u0026#39; host_addr=\u0026#39;x.x.x.x\u0026#39; 15-JAN-2014 12:44:13 * (CONNECT_DATA=(SERVICE_NAME=ORCL_SBY_DGMGRL)(INSTANCE_NAME=**ORCL_XXX**)(SERVER=DEDICATED)(CID=(PROGRAM=dgmgrl)(HOST=sby_server)(USER=oracle))) * (ADDRESS=(PROTOCOL=tcp)(HOST=x.x.x.x)(PORT=53336)) * establish * ORCL_SBY_DGMGRL * 12521 The logfile was showing the instance_name - ORCL_XXX - as something completely unrelated to the instance_name for the standby database - ORCL_SBY. Most confusing, especially when I had already confirmed that tnsnames.ora was correct and also that all the entries functioned correctly, from both servers. Where was this erroneous host name coming from?\nLooking in dgmgrl again, I checked the StaticConnectIdentifier property for both databases.\n1 2 3 4 5 6 7 8 9 DGMGRL\u0026gt; show database \u0026#39;ORCL_PDB\u0026#39; StaticConnectIdentifier StaticConnectIdentifier = \u0026#39;(DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=pdb_server)(PORT=1522))(CONNECT_DATA=(SERVICE_NAME=ORCL_PDB_DGMGRL)(INSTANCE_NAME=ORCL_PDB)(SERVER=DEDICATED)))\u0026#39; DGMGRL\u0026gt; show database \u0026#39;ORCL_SBY\u0026#39; StaticConnectIdentifier StaticConnectIdentifier = \u0026#39;(DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=sby_server)(PORT=1522))(CONNECT_DATA=(SERVICE_NAME=ORCL_SBY_DGMGRL)(INSTANCE_NAME=ORCL_XXX)(SERVER=DEDICATED)))\u0026#39; Bingo! At least, after starting at the screen for a few minutes, it was bingo! I finally spotted that the stand by database\u0026rsquo;s property had the wrong instance name.\nThe Solution A simple property edit for the standby database was carried out in dgmgrl as follows:\n1 2 DGMGRL\u0026gt; edit database \u0026#39;ORCL_SBY\u0026#39; set property StaticConnectIdentifier=\u0026#39;(DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=sby_server)(PORT=1522))(CONNECT_DATA=(SERVICE_NAME=ORCL_SBY_DGMGRL)(INSTANCE_NAME=ORCL_SBY)(SERVER=DEDICATED)))\u0026#39;; The edit database command above is all on one line by the way.\nAnd that was it. After making the change, I was able to run switchovers to and from the standby on eiither server. Job done.\n","description":"","id":63,"section":"posts","tags":null,"title":"Interesting Data Guard Problem","uri":"http://localhost:1313/RantsAndRaves/posts/2014/01/interesting-data-guard-problem/"},{"content":"Here\u0026rsquo;s a short Arduino sketch that will fade and flash an LED on pin 9. Works a treat on a Shrimp as well. Which is what mine runs on - my Breaded Shrimp which stared like on a tiny breadboard like the one at the end of the above link, but which is now permanently soldered onto a piece of copper \u0026ldquo;breadboard\u0026rdquo; as you can see in the image that\u0026rsquo;s around here somewhere.\nWhat does it look like? Well, I can\u0026rsquo;t upload a video for security reason (or so WordPress tells me after I\u0026rsquo;ve uploaded the video!) Imagine, if you will, the LED in the image fading up from nothing to full brightness, flashing 3 times, then fading back down again where it flashes another three times before starting again.\nMaybe it needs a little more work?\nMore LEDs?\nWhat do you think?\nI suppose I could call it minimalist? :-)\nMaybe I\u0026rsquo;m just not cut out for this sort of work?\nHappy Christmas, or whichever Winter Festival type celebration that you follow. Whatever it is, do have a nice one.\nAnyway, here\u0026rsquo;s the code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 /* * This code is a cross between the \u0026#34;Fade\u0026#34; and \u0026#34;Blink\u0026#34; examples * Supplied with the Arduino IDE. Not much has changed, just the * removal of a few \u0026#34;magic\u0026#34; numbers. * * This example code is in the public domain. */ int brightness = 0; // Brightness of the LED, right now. int fadeAmount = 2; // Fade the LED up/down by this amount each time. int led = 9; // The LED is attached to this pin. int flashDelay = 200; // Pause between flashes of the LED. int fadeDelay = 20; // Pause between fadeAmount changes. void setup() { // declare pin 9 to be an output: pinMode(led, OUTPUT); } void loop() { // set the brightness of pin 9: analogWrite(led, brightness); // change the brightness for next time through the loop: brightness = brightness + fadeAmount; // reverse the direction of the fading at the ends of the fade: if (brightness \u0026lt;= 0 || brightness \u0026gt;= 255) { // Flash the LED a few times at each end // of the fading cycle for (int x = 0; x \u0026lt; 4; x++) { digitalWrite(led, HIGH); delay(flashDelay); digitalWrite(led, LOW); delay (flashDelay); } // And now, reverse the fade. fadeAmount = -fadeAmount ; } // Pause a little, to see the fading effect delay(fadeDelay); } Simple stuff, and very Open Source too. Take it, use it, abuse it if you like. Just have fun.\nCheers.\n","description":"","id":64,"section":"posts","tags":null,"title":"Arduino Christmas Light","uri":"http://localhost:1313/RantsAndRaves/posts/2013/12/arduino-christmas-light/"},{"content":"Everybody, it seems is making their own Christmas cards these days. We are no different, except, we are! Read on \u0026hellip;.\nA couple of weeks ago, in preparation for Christmas, Alison (SWMBO!) and I spend a fun morning and bits of the afternoon at a printer\u0026rsquo;s in Skipton. Sadly, with my memory, I can\u0026rsquo;t remember the printer\u0026rsquo;s name, but suffice to say, he\u0026rsquo;s an old fashioned printer, with an upper and lower case full of type faces and lots of different fonts, in the old style - made out of metal.\nThat\u0026rsquo;s how upper and lower case got their names, by the way, from the old printer\u0026rsquo;s habit of putting the capital letters on the upper shelf of the stand, and the non-capitals on the lower shelf. The letters were stored in cases. Hence, upper and lower case.\nEqually, spread about the place, are boxes and boxes of old printing plates of various pictures. The detail is incredible in some of these, and many are a couple of hundred years old, long before mass production, these were hand made.\nWe started with a blank card, 5\u0026quot; by 5\u0026quot;, and had to come up with a message and a design, first of all, hopefully, one that would fit in the \u0026ldquo;stick\u0026rdquo; and in the \u0026ldquo;block\u0026rdquo; when we finished setting up.\nI made the message up. Alison chose the image.\nThe first problem was to pick a few fonts, and layout the top part of the design on a \u0026ldquo;stick\u0026rdquo; Once we had the widest part done, we could then make sure everything else was exactly as wide. None of that desktop publishing here, this is all done in cold, hard steel!\nAfter getting the first part done, we could move on to the next part. The fonts we had available were determined by the width of the first part and not the other way round. After much choosing and testing and rejecting of fonts, we found a couple that we liked, and we were ready to do the stick. This involves taking our design\u0026rsquo;s top half, and putting it into a sliding clamp like system and locking it down to the desired width. In other words, as wide as it took to get everything in, and still fit on the card! Making sure everything was equally spaces involved adding bits of lead (hence, leading) between rows of text and kerning between letters.\nThe design was checked and double-checked, and transferred to the block. The stick stayed locked down and the lower half of the card was ready to be done next.\nWe had to get the lower half of the card to match up with the width of the upper part. More fonts were tested and rejected. Finally, we had a finished design. The lower half was transferred to the block, and locked into place with clamps and a pretty hefty metal frame. Then we had to make sure that everything was held firmly in place, and was at the exact same height so that everything would take ink and transfer it to the cards.\nThe block. Inked and ready for a test print.: Soon we were ready for ink. Alison chose a deep red colour, and a tiny amount was dropped onto a sturdy glass plate. That ink is incredibly viscous - it has the consistency of tarmacadam - aka bitumen. Then, it was rolled out with the roller into a smooth, square arrangement. We were ready to print. You can see a photo of the glass plate, inked up and ready to go, with the roller somewhere close by!\nInky stuff on a glass plate.: For each card, the block was inked by running the roller over the inky glass plate, and then rolling it over the entire block, in both directions, long ways and up and down. A card was placed carefully alongside the marks we made earlier when we measured things out and centred the block in the frame. Then - with both hands - the whats-its-name was dragged slowly, but firmly, across the card. A sheet of paper prevented any crud from the roller or the thingy-ma-jig transferring to our nice clean cards. (Can you tell I\u0026rsquo;ve got all the technical terms off pat?)\nAnd still using both hands, the doo-dah was rolled back again, over the card to the starting position. A quick removal of the paper, and lo, we had a problem. The damned hyphen was in the wrong place. Check out the image for details - did you spot it? Well, try spotting it when everything is frack-to-bont! (Back to front!) We had to break down the block, and move stuff around to get the damned hyphen in the right place. After another lockdown and check for heights and squareness, we were ready to rock and roll. And to print our own cards.\nWe did it wrong!: Success! We had a nice looking card, but all was not complete. Alison had decided on glitter. Printer ink sticks like the proverbial to a blanket, and is a pretty good way of holding glitter in place. Except, printers use flitter as opposed to glitter, it\u0026rsquo;s much finer. And a lot more expensive. Still, it was decided that sparkly was needed, so the tree in the design got some decorating!\nHanging cards up to dry.: I did quite a lot of printing and inking, while Alison did the sparkly bits and hung the cards up to dry. 24 Hours are desired for a decent drying time. Here you can see the cards, in all their hanging up glory!\nThe finished Christmas card.: And here is one of the finished cards, from a pretty bad angle, it has to be said. But you get the drift? And notice how wonderfully placed that rogue hyphen is now. I can tell you, it took some taming!\nSo, tonight we have been writing the cards out and guess what, there\u0026rsquo;s damned glitter all over the house. Not only does it stick like you know what to the cards, but also to everything else.\nHappy Christmas!\n","description":"","id":65,"section":"posts","tags":null,"title":"Making Christmas Cards - The Old Way!","uri":"http://localhost:1313/RantsAndRaves/posts/2013/12/making-christmas-cards-the-old-way/"},{"content":"The Oracle database allows the users to change their passwords as follows:\nSQL\u0026gt; ALTER USER me IDENTIFIED BY my_new_password;\nor, alternatively, to use the PASSWORD command, which prompts for the old and new passwords.\nOf course, if the user has forgotten their old password, the system manager can do the necessary:\nSQL\u0026gt; ALTER USER forgetful_user IDENTIFIED BY a_new_password;\nNow, if there are profiles in use, as there are, and these profiles have a password verification function defined, these passwords will be validated to ensure that they adhere to the installation standards.\nSadly, all is not as it seems.\nThe verification function is passed three parameters:\nUsername Old Password New Password In 12c, the standard verification function has an inbuilt helper function called string_distance which determines how different the new password is from the old one. The problem is, regardless of what you have set for that difference to be, it is not always executed. The code in the verification function, $ORACLE_HOME/rdbms/admin/utlpwmg.sql, has something resembling this check in it:\n1 2 3 if old_password is not null then result := string_distance(old_password, new_password) ... Interesting, if the old password is NULL? How can this be? Well, in reality, it is simple. Here are the cases where the old password will not be NULL:\nWhen the user calls the PASSWORD function. When the user executes ALTER USER me IDENTIFIED BY my_new_password REPLACE old_password; And here are the cases when the old password will be NULL:\nWhen the user executes ALTER USER me IDENTIFIED BY my_new_password; When the SYS user executes ALTER USER forgetful_user IDENTIFIED BY a_new_password; When the SYS user executes ALTER USER forgetful_user IDENTIFIED BY a_new_password REPLACE old_password; And thereby hangs the rub. If the SYSDBA always changes the passwords, then the old password is always NULL, and some of the verification checks will not be carried out. Only when the user affected changes the password using the PASSWORD command or passes the REPLACE clause to the ALTER USER command, will the old password be supplied to the verification function.\nIt actually makes sense, if you think about it, the password is stored, by Oracle, as the result of a one-way hash. This means that there is no way to retrieve the plain text password from the hashed value. However, it appears that regardless of whether the SYSDBA user supplies the old password in the ALTER USER ... REPLACE command, it is not passed through to the verification function.\nJust a little something to watch out for as it can allow your users to get past some of the checks in the verification function - adding a one character suffix to the old password.for example, can get past the checks if the old password is not supplied.\nWhat do you mean, you never knew there was a REPLACE clause on the ALTER USER command? ;-)\n","description":"","id":66,"section":"posts","tags":null,"title":"So, How Do You Change a User's Password","uri":"http://localhost:1313/RantsAndRaves/posts/2013/11/so-how-do-you-change-a-users-password/"},{"content":"In this, the second part of the Introduction to Oracle Datapump mini-series, we take a look at importing dump files using impdp. If you missed the first part which concentrated on exporting with expdp, have a read of it here. Once again, the following is a quick introduction for people like me - running Oracle on Linux and slightly averse to change! ;-)\nIntroduction to Datapump Imports All of the following is based on 11.2.0.2 Enterprise Edition of Oracle, running on Suse Linux, Enterprise Edition version 11 SP 0.\nPlease note. In the following, any Linux commands that need to be executed as the root user, are prefixed with a \u0026lsquo;#\u0026rsquo; prompt. All other commands are prefixed by a \u0026lsquo;$\u0026rsquo; prompt, and these should be executed as the oracle user.\nBefore we start, we need to make sure we have a couple of things (technical term) set up.\nPrerequisites When you come to use Datapump\u0026rsquo;s impdp utility, you need to have a pre-existing Oracle Directory within the database being imported in to. This directory object tells datapump - where to find dump files and where to write log files. As mentioned in the previous article, every database created has a directory already set up for datapump to use. It is called DATA_PUMP_DIR and defaults to the location $ORACLE_HOME/rdbms/log.\nThis isn\u0026rsquo;t normally the best location, so you have a choice of amending (ie dropping and recreating) the current one, or creating a new one for our own use. I find it best to create a new one. I also like to set up a special datapump_admin user dedicated to carrying out exports and imports. Instructions on creating Oracle Directories and setting up the datapump_admin user, and its required privileges, were covered in the the previous article, and will not be repeated here.\nImporting This article concentrates mainly on the importing of data from a database using the impdp utility which replaces the old imp utility we know and love so much!\nbefore we start looking at specifics, be aware that when we Luddites use imp we need to be aware that it appends data to existing tables (provided ignore=y of course) and if we didn\u0026rsquo;t want this to happen, we either had to login and drop the tables in question, or at the very least, truncate them. With impdp we don\u0026rsquo;t need to do this! We have two options to replicate the ignore=y parameter of imp, CONTENT and TABLE_EXISTS_ACTION.\nThe following sections describe each of these parameters in turn, and further down the page, there is a section describing what exactly happens when these parameters are used in certain combinations. Beware, some combinations produce misleading error messages - in 11gR2 at least.\nContent The content parameter takes the following values:\nAll - the default. Impdp attempts to create the objects it finds in the dump file, and load the data into them.\nMetadata_only - impdp will only attempt to create the objects it finds in the dump file. It will not attempt to bring in the data. This is equivalent to rows=n in imp.\nData_only - impdp will not try to create the objects it finds in the dump file, but it will attempt to load the data. This is equivalent to ignore=y in imp.\nIf there are objects in the dump file, which are not in the database/schemas/tablespaces being imported, then there will be errors listed to the log file, and screen, and those missing tables will not be imported.\nTable_Exists_Action This parameter tells impdp what to do when it encounters a table with or without data already in it. It takes the following values:\nAppend - impdp will simply append the data from the dump file to the table(s) in question. Existing data will remain, untouched. This is the default option if content=data_only.\nReplace - impdp will drop the object, recreate it and then load the data. Useful, for example, if the definition of a table has been changed, this option will ensure that the new definition - in the dump file - is used. This value cannot be used if the content=data_only parameter is in use.\nSkip - the default, unless content=data_only. Impdp will not attempt to load the data and will skip this table and move on to the next one. This is very handy if an import went wrong, for example, and after sorting out the failing table, you simply restart the import and have it skip the tables it has already completed.\nTruncate - impdp will truncate any tables it finds, but not drop them, and then load the data.\nLegacy Mode Parameters Of course, if you can\u0026rsquo;t be bothered to learn the new parameters for impdp, then from 11gR2 onwards, you can specify a number of the old imp parameters and have impdp convert them to the new ones for you. It\u0026rsquo;s best to learn the new ones though, you never know if Oracle will drop legacy mode at some future date.\nRight, onwards, we have data to import!\nImport a Full Database Dump File We created a full database export file - full.dmp - last time, which we can use to import back into the same, or a different database. As before, it is usually wise to put the required parameters in a parameter file, and specify that on the command line - this is especially true if you need to have double or single quotes in some of the parameters - as these will need escaping on the command line, but not in the parameter file.\n1 $ cat fullimp.par 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=my_datapump_dir dumpfile=full.dmp logfile=full.imp.log **full=y** content=all table_exists_action=truncate If you omit the password from the userid parameter, impdp will prompt you.\nRunning a full import is now a simple matter of:\n1 $ impdp parfile=fullimp.par What happens next is that a pile of \u0026ldquo;stuff\u0026rdquo; scrolls up the screen, some of which is useful, some not so. Straight away, you will notice (because I\u0026rsquo;ve highlighted a couple!) a number of errors about objects already existing. This is because we have content=all. Had we simply had content=data_only these errors would not have appeared.\nHere is an excerpt with some of the good bits highlighted:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 Import: Release 11.2.0.2.0 - Production on Sat Aug 31 16:47:13 2013 Copyright (c) 1982, 2009, Oracle and/or its affiliates. All rights reserved. Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.2.0 - 64bit Production With the Partitioning option Master table \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_FULL_01\u0026#34; successfully loaded/unloaded **Starting \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_FULL_01\u0026#34;**: datapump_admin/******** parfile=fullimp.par Processing object type DATABASE_EXPORT/TABLESPACE **ORA-31684: Object type TABLESPACE:\u0026#34;SYSAUX\u0026#34; already exists ORA-31684: Object type TABLESPACE:\u0026#34;UNDOTBS1\u0026#34; already exists** ... Processing object type DATABASE_EXPORT/PASSWORD_VERIFY_FUNCTION ... Processing object type DATABASE_EXPORT/SCHEMA/USER **ORA-31684: Object type USER:\u0026#34;OUTLN\u0026#34; already exists ORA-31684: Object type USER:\u0026#34;ORACLE\u0026#34; already exists ORA-31684: Object type USER:\u0026#34;NORMAN\u0026#34; already exists** ... Processing object type DATABASE_EXPORT/GRANT/SYSTEM_GRANT/PROC_SYSTEM_GRANT Processing object type DATABASE_EXPORT/SCHEMA/GRANT/SYSTEM_GRANT Processing object type DATABASE_EXPORT/SCHEMA/ROLE_GRANT Processing object type DATABASE_EXPORT/SCHEMA/DEFAULT_ROLE ... Processing object type DATABASE_EXPORT/SCHEMA/TABLE/TABLE **ORA-39120: Table \u0026#34;SYSTEM\u0026#34;.\u0026#34;DEF$_DESTINATION\u0026#34; can\u0026#39;t be truncated, data will be skipped. Failing error is: ORA-02266: unique/primary keys in table referenced by enabled foreign keys** ... . . imported \u0026#34;APP_OWNER\u0026#34;.\u0026#34;TEST\u0026#34; 5.031 KB 5 rows . . imported \u0026#34;NORMAN\u0026#34;.\u0026#34;NORM\u0026#34; 5.023 KB 4 rows ... Processing object type DATABASE_EXPORT/SCHEMA/TABLE/POST_TABLE_ACTION Processing object type DATABASE_EXPORT/SCHEMA/TABLE/TRIGGER Processing object type DATABASE_EXPORT/SCHEMA/POST_SCHEMA/PROCACT_SCHEMA Processing object type DATABASE_EXPORT/AUDIT **Job \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_FULL_01\u0026#34; completed with 700 error(s) at 16:50:18** Well, that went well - but full imports - even with imp - usually do create masses of errors. They are probably best avoided. I suppose they might be useful when you have created a brand new database, with only the Oracle required tablespaces etc, and you are happy to have impdp bring in a clone, almost, of another database. Maybe! ;-)\nYou will notice that because of all the extra errors caused by numerous objects that already exist, that checking the log file for real errors will be a little difficult.\nIf you already know that all the objects, present in the dump file, exist in the database that you are importing into, then use the content=data_only parameter which will prevent these errors from appearing. Remember to set a suitable value for the table_exists_action parameter as well, otherwise the default action is to skip whichever objects it finds already existing.\nYou will not be allowed to use replace because there will be no metadata in the import, so there will be no commands to recreate the objects. You only have append, truncate or skip available. If you try to use a forbidden option, you will see the following error:\nORA-39137: cannot specify a TABLE_EXISTS_ACTION of REPLACE for a job with no metadata\nRunning the above full import again, but with a data_only import this time, resulted in far fewer errors:\n\u0026hellip;\nJob \u0026ldquo;DATAPUMP_ADMIN\u0026rdquo;.\u0026ldquo;SYS_IMPORT_FULL_01\u0026rdquo; completed with 21 error(s) at 16:52:41\nAnd this time, these were due to dropping tables that were in use as the parent of a foreign key constraint.\nYou will see highlighted above, the usual job name details. The format of the job names was described in the previous article in this mini-series, and will not be discussed further here.\nFollowing the job details - and as already mentioned, a table of the same name will be created within the datapump_admin user while the job is running - we start to see the list of various objects that already exist. Then, eventually, in amongst all the errors, we see the tables actually being imported. Phew!\nNote, on the final line, the number of errors I have to check to ensure that they are not critical. Hmmm, I really do not like database full imports, never have done and I doubt I ever will. I think we should move on, quickly, and take a look at schema imports instead.\nImporting Schemas Importing a scheme or schemas is a better way to bring data into an existing database. You can, if you wish, import a particular schema from a full dump - you don\u0026rsquo;t have to have exported the schemas specifically - the example below will demonstrate this.\nAs with expdp, you simply need to specify the schemas= parameter in your parameter file or on the command line:\n1 $ cat schema_imp.par 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=my_datapump_dir dumpfile=full.dmp logfile=schema.imp.log **schemas=norman** content=all table_exists_action=replace 1 $ impdp parfile=schema_imp.par 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Import: Release 11.2.0.2.0 - Production on Mon Sep 2 16:17:24 2013 Copyright (c) 1982, 2009, Oracle and/or its affiliates. All rights reserved. Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.2.0 - 64bit Production With the Partitioning option Master table \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_SCHEMA_01\u0026#34; successfully loaded/unloaded S**tarting \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_SCHEMA_01\u0026#34;**: datapump_admin/******** parfile=schema_imp.par Processing object type DATABASE_EXPORT/SCHEMA/USER **ORA-31684: Object type USER:\u0026#34;NORMAN\u0026#34; already exists** Processing object type DATABASE_EXPORT/SCHEMA/GRANT/SYSTEM_GRANT Processing object type DATABASE_EXPORT/SCHEMA/ROLE_GRANT Processing object type DATABASE_EXPORT/SCHEMA/DEFAULT_ROLE Processing object type DATABASE_EXPORT/SCHEMA/PROCACT_SCHEMA Processing object type DATABASE_EXPORT/SCHEMA/TABLE/TABLE Processing object type DATABASE_EXPORT/SCHEMA/TABLE/TABLE_DATA . . imported \u0026#34;NORMAN\u0026#34;.\u0026#34;NORM\u0026#34; 5.023 KB 4 rows Processing object type DATABASE_EXPORT/SCHEMA/TABLE/STATISTICS/TABLE_STATISTICS **Job \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_SCHEMA_01\u0026#34; completed with 1 error(s) at 16:17:35** What happens if you specify the schema only dump file that was previously created, but supply the full=y parameter? Exactly as you would expect, the full dump file is imported and, in the following example, the norman schema is once more imported - and existing objects are replaced.\n1 $ cat schema2_imp.par 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=my_datapump_dir **dumpfile=norman.dmp full=y** logfile=schema.imp.log content=all table_exists_action=replace 1 $ impdp parfile=schema2_imp.par 1 2 3 4 5 6 7 8 9 10 Import: Release 11.2.0.2.0 - Production on Mon Sep 2 16:24:39 2013 ... **Starting \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_FULL_01\u0026#34;**: datapump_admin/******** parfile=schema2_imp.par ... Processing object type SCHEMA_EXPORT/USER ORA-31684: Object type USER:\u0026#34;NORMAN\u0026#34; already exists ... . . imported \u0026#34;NORMAN\u0026#34;.\u0026#34;NORM\u0026#34; 5.023 KB 4 rows ... Job \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_FULL_01\u0026#34; completed with 1 error(s) at 16:24:49 So, the old imp behaviour is still available to use!\nImporting Tablespaces As before, when you wish to import a tablespace, it can be from a full dump file, or a tablespace level dump file - provided that the tablespace(s) you want to import are actually present in the file. A table or schema level dump file will not be suitable.\nTo import at the tablespace level, add the tablespaces= parameter in your parameter file or on the command line:\nWhat will happen if, for some reason, a tablespace - call it tools - exists in the dump file, but not in the database? Well, it will not be recreated - impdp will not recreate the tablespaces, only the objects within them.\n1 $ cat tablespace_imp.par 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=my_datapump_dir **dumpfile=full.dmp** logfile=tablespace.imp.log **tablespaces=users** content=all table_exists_action=replace 1 $ impdp parfile=tablespace_imp.par 1 2 3 4 5 6 7 8 9 10 Import: Release 11.2.0.2.0 - Production on Mon Sep 2 20:05:19 2013 ... Master table \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_TABLESPACE_01\u0026#34; successfully loaded/unloaded **Starting \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_TABLESPACE_01\u0026#34;**: datapump_admin/******** parfile=tablespace_imp.par Processing object type DATABASE_EXPORT/SCHEMA/TABLE/TABLE Processing object type DATABASE_EXPORT/SCHEMA/TABLE/TABLE_DATA . . imported \u0026#34;APP_OWNER\u0026#34;.\u0026#34;TEST\u0026#34; 5.031 KB 5 rows . . imported \u0026#34;NORMAN\u0026#34;.\u0026#34;NORM\u0026#34; 5.023 KB 4 rows Processing object type DATABASE_EXPORT/SCHEMA/TABLE/STATISTICS/TABLE_STATISTICS Job \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_TABLESPACE_01\u0026#34; successfully completed at 20:05:26 Importing Tables And finally, importing specific tables is as easy as the above. You should be completely at home with the parameter file and parameters by now:\n1 $ cat tables_imp.par 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=my_datapump_dir dumpfile=tables.dmp logfile=tables.imp.log **tables=norman.norm,app_owner.test** content=all table_exists_action=replace 1 $ impdp parfile=tables_imp.par 1 2 3 4 5 6 7 8 9 10 11 12 Import: Release 11.2.0.2.0 - Production on Thu Sep 5 09:09:01 2013 Copyright (c) 1982, 2009, Oracle and/or its affiliates. All rights reserved. Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.2.0 - 64bit Production With the Partitioning option Master table \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_TABLE_01\u0026#34; successfully loaded/unloaded **Starting \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_TABLE_01\u0026#34;**: datapump_admin/******** parfile=tables_imp.par Processing object type TABLE_EXPORT/TABLE/TABLE Processing object type TABLE_EXPORT/TABLE/TABLE_DATA . . imported \u0026#34;APP_OWNER\u0026#34;.\u0026#34;TEST\u0026#34; 5.031 KB 5 rows . . imported \u0026#34;NORMAN\u0026#34;.\u0026#34;NORM\u0026#34; 5.023 KB 4 rows Processing object type TABLE_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS Job \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_IMPORT_TABLE_01\u0026#34; successfully completed at 09:09:05 Content and Table_Exists_Action Parameters The following is a summary of the actions or errors you will see when various combinations of these two parameters are used together.\nContent = All If table_exists_action is append, then you will see error messages for any objects that currently exist in the database plus, existing data will be left untouched and the contents of the dump file will be appended to those tables present in both the dump file and the current database.\nIf table_exists_action is truncate, then then you will see error messages for any objects that currently exist in the database plus, any object that is being imported, which exists in the database, will be truncated prior to the data from the dump file being imported.\nIf table_exists_action is Replace, then then you will see error messages for any objects that currently exist but those objects will subsequently be dropped and recreated prior to the data being imported.\nIf table_exists_action is Skip, then you will see error messages for any objects that currently exist in the database and nothing will be imported for those existing objects. Objects which exists in the dump file but not in the database, will be created. But not tablespaces.\nContent = Data_Only If table_exists_action is append, then provided that the definition of the objects in the dump file matches that in the database, data will be appended and no error messages shown for existing objects. If an existing table has a different definition to that in the dump file, errors will be shown. Any objects in the dump file that do not exist in the database will report an error and will not be created.\nIf table_exists_action is truncate, then no errors will be shown for existing objects and those tables that exists in both the database and dump file will first be truncated prior to the data being imported. Any objects in the dump file that do not exist in the database will report an error and will not be created.\nIf table_exists_action is Replace, then you will receive an error as this is an invalid parameter for this option for the content parameter. You cannot replace an object when the metadata, used to recreate it, is not being imported.\nIf table_exists_action is Skip, then no data are imported for existing objects. Any objects in the dump file that do not exist in the database will report an error and will not be created.\nContent = Metadata_only If table_exists_action is append, then objects which do not exist in the database, but do in the dump file will be created. Tables which already exist in the database will not be affected in any way, however, the following misleading message will be displayed:\nTable \u0026ldquo;USER\u0026rdquo;.\u0026ldquo;TABLE_NAME\u0026rdquo; exists. Data will be appended to existing table but all dependent metadata will be skipped due to table_exists_action of append\nThis is misleading as no data will be loaded at all. We are specifying contant=metadata_only after all!\nIf table_exists_action is truncate, then existing tables will be truncated, no new data will be loaded. New objects will be created from the dump file. Existing tables will cause the following message to be displayed:\nTable \u0026ldquo;USER\u0026rdquo;.\u0026ldquo;TABLE_NAME\u0026rdquo; exists and has been truncated. Data will be loaded but all dependent metadata will be skipped due to table_exists_action of truncate\nThis message is misleading as there will be no data loaded, the table will be empty after the import. If the dump file\u0026rsquo;s metadata is different from the existing table definition, then the existing table will remain in force.\nIf table_exists_action is Replace, then no messages will be displayed for existing tables. These will be dropped and recreated using the metadata from the dump file. No data will be loaded, so they will be empty after the import. New objects in the dump file will be created.\nIf table_exists_action is Skip, then nothing will be done for existing objects. New objects, in the dump file, will be created. No data will be loaded. The following - correct - message will be produced for existing tables:\nTable \u0026ldquo;USER\u0026rdquo;.\u0026ldquo;TABLE_NAME\u0026rdquo; exists. All dependent metadata and data will be skipped due to table_exists_action of skip.\nImporting - Cheat Sheet The following is a list of \u0026ldquo;cheats\u0026rdquo; - basically, a list of the parameters you would find useful in doing a quick import at any level from full database right down to individual tables. As before, I have listed each one in the form of a parameter file - for ease of (your) copy and paste.\nThe following parameter files assume that you have set up a suitable Oracle Directory object within the database being exported, however, the first part of the cheat sheet summarises the commands required to create one, and a suitably endowed user to carry out the exports.\nCreate an Oracle Directory The following is executed in SQL*Plus, or similar, as a SYSDBA enabled user, or SYS:\n1 create directory my_datapump_dir as \u0026#39;/your/required/location\u0026#39;; This location is where all the dump files need to be copied into before running an impdp. The log files for the imports will be created here as the jobs run.\nThe following must executed as root, unless the location is already owned by the oracle account of course!\n1 2 mkdir -p /your/required/location chown oracle:dba /your/required/location Create a Datapump User Account and Privileges The following is executed in SQL*Plus, or similar, as a SYSDBA enabled user, or SYS:\n1 2 3 4 5 6 7 8 create user datapump_admin identified by secret_password default tablespace users quota unlimited on users; grant create table to datapump_admin; grant datapump_exp_full_database to datapump_admin; grant datapump_imp_full_database to datapump_admin; grant read, write on directory **my_datapump_dir** to datapump_admin; Full Import This one is suitable for a database where the objects already exist. Tables will be trunccated prior to the data replacing the existing contents.\n1 2 3 4 5 6 7 userid=datapump_admin/secret directory=**my_datapump_dir** dumpfile=full.dmp logfile=full.imp.log **full=y** content=data_only table_exists_action=truncate Schema Import 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=**my_datapump_dir** dumpfile=full.dmp logfile=schema.imp.log **schemas=user_a,user_b** content=all table_exists_action=replace Tablespaces Import 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=**my_datapump_dir** dumpfile=full.dmp logfile=tablespace.imp.log **tablespaces=users** content=all table_exists_action=replace Table Import 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=**my_datapump_dir** dumpfile=full.dmp logfile=tables.imp.log **tables=[user.]table_a,[user.]table_b** content=all table_exists_action=replace Remapping Schemas and/or Tablespaces In the old days, we could specify parameters such as from_user and to_user to change the resulting owner of the imported objects. We can do this too with impdp as follows.\nTablespaces On the command line, or in the parameter file, simply add one of the following for each tablespace you wish to remap:\n1 remap_tablespace=export_tablespace:import_tablespace Any objects in the export_tablespace will be mapped into the import_tablespace instead. If you have more than one tablespace to map from, then simply use more than one remap_tablespace parameter.\n1 2 remap_tablespace=old_users:users remap_tablespace=old_data:archived_data Schemas On the command line, or in the parameter file, simply add one of the following for each schema you wish to remap:\n1 remap_schema=from_user:to_user Any objects in the from_user will be mapped into the to_user instead. If you have more than one tablespace to map from, then simply use more than one remap_schema parameter.\n1 2 3 remap_schema=other_user_1:norman remap_schema=other_user_2:norman remap_schema=other_user_3:norman ","description":"","id":67,"section":"posts","tags":null,"title":"Introduction to Oracle Datapump - Part 2","uri":"http://localhost:1313/RantsAndRaves/posts/2013/09/introduction-to-oracle-datapump-part-2/"},{"content":"Oracle Datapump, aka expdp and impdp were introduced at Oracle 10g to replace the old faithful exp and imp utilities. Many DBAs around the world find that it\u0026rsquo;s hard to change from what we know like the back of our hand, to something new. We need to change because exp is deprecated from 10g onwards and might even already have vanished from 12c - which I have to install as one of my upcoming tasks. The following is a quick introduction for people like me - running Oracle on Linux and slightly averse to change! ;-)\nIntroduction to Datapump All of the following is based on 11.2.0.2 Enterprise Edition of Oracle, running on Suse Linux, Enterprise Edition version 11 SP 0.\nOracle datapump, as mentioned, is a replacement for the old exp and imp utilities. It comes with numerous benefits, and a couple of minor niggles! All will be revealed.\nInternally, datapump uses a couple of PL/SQL packages:\nDBMS_DATAPUMP DBMS_METADATA Normally, you won\u0026rsquo;t need to bother with these - for taking logical backups of the database, but you can, if you wish, call them explicitly. Doing so is beyond the scope of this article - as they say - but if you wish to find out more, have a look in the Oracle Database PL/SQL Packages and Types Reference Guide for more details.\nDatapump has two modes:\nCommand line Interactive The former is similar to using the old exp or imp utilities, while the latter allows you to connect to long running datapump jobs, and enquire as to progress and add files or similar. This will be discussed later, but for the rest of us Luddites, command line mode will probably be most familiar to start with.\nBefore we start, we need to make sure we have a couple of things (technical term) set up.\nPlease note. In the following, some Linux commands need to be executed as the root user, these are prefixed with a \u0026lsquo;#\u0026rsquo;. All other commands are prefixed by a \u0026lsquo;$\u0026rsquo; prompt, and these are executed as the oracle user.\nPrerequisites When you come to use Datapump utilities, you need to have a pre-existing Oracle Directory within the database being exported or imported. This directory object tells datapump - where to write or read dump files to/from. By default, every database created has a directory already set up for datapump to use. It is called DATA_PUMP_DIR and defaults to the location $ORACLE_HOME/rdbms/log.\n1 2 3 4 5 6 7 8 9 10 SQL\u0026gt; !echo $ORACLE_HOME /srv/oracle/product/11gR1/db/ SQL\u0026gt; select owner, directory_name, directory_path 2 from dba_directories 3 where directory_name = \u0026#39;DATA_PUMP_DIR\u0026#39;; OWNER DIRECTORY_NAME DIRECTORY_PATH ------ -------------- ---------------------------------------- SYS DATA_PUMP_DIR /srv/oracle/product/11gR1/db//rdbms/log/ This isn\u0026rsquo;t normally the best location, so you have a choice of amending (ie dropping and recreating) the current one, or creating a new one for our own use. I find it best to create a new one:\n1 2 3 4 5 SQL\u0026gt; connect / as sysdba Connected. SQL\u0026gt; create directory my_datapump_dir as \u0026#39;/srv/oracle/datapump\u0026#39;; Directory created. The location pointed to need not exist when the above command is executed, but it must exist when you attempt to use it and the oracle user must be able to read from and write to the specified location.\n1 2 3 4 5 6 # mkdir /srv/oracle/datapump # chown oracle:dba /srv/oracle/datapump/ # ls -l /srv/oracle ... drwxr-xr-x 2 oracle dba 4096 Aug 29 15:03 datapump ... If you only ever intend to run datapump jobs as the SYSDBA enabled users, then this is all we need. However, if you intend to set up another user for this purpose, the following needs to be carried out or the user in question won\u0026rsquo;t be able to run the datapump utilities.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SQL\u0026gt; create user datapump_admin identified by secret 2 default tablespace users 3 quota unlimited on users; User created. SQL\u0026gt; grant create table to datapump_admin; Grant succeeded. SQL\u0026gt; grant datapump_exp_full_database to datapump_admin; Grant succeeded. SQL\u0026gt; grant datapump_imp_full_database to datapump_admin; Grant succeeded. SQL\u0026gt; grant read, write on directory my_datapump_dir to datapump_admin; Grant succeeded. That\u0026rsquo;s it, we are ready to do some exporting - new style! In case you were wondering, we need the create table privilege because datapump utilities need to create a table for each job executed. More on this later.\nExporting This article concentrates mainly on the exporting of data from a database using the expdp utility which replaces the old exp utility we know and love so much!\nExporting to a Lower Version of Oracle There is a nice new parameter that allows easy exporting of data from a higher version of Oracle so that it can be imported into a lower version. It is not used in the details below, but just in case you ever need to export from 12.2 and import into 11.2.0.4, for example, then the VERSION parameter will be your best friend.\n1 $ expdp ..... VERSION=11.2 It can, of course, be added to a parameter file too. Details below on the use of parameter files.\nExport Full Database The command to export a full database is as follows:\n1 $ expdp datapump_admin/secret directory=my_datapump_dir dumpfile=full.dmp logfile=full.exp.log full=y However, we can put these parameters into a parameter file - just like when we used exp!\n1 $ cat fulldp.par 1 2 3 4 5 userid=datapump_admin/secret directory=my_datapump_dir dumpfile=full.dmp logfile=full.exp.log full=y If you omit the password from the userid parameter, expdp and impdp will prompt you.\nRunning a full export is now a simple matter of:\n1 $ expdp parfile=fulldp.par What happens next is that a pile of \u0026ldquo;stuff\u0026rdquo; scrolls up the screen, some of which is useful, some not so. Here is an excerpt with the good bits highlighted:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Export: Release 11.2.0.2.0 - Production on Thu Aug 29 15:20:12 2013 Copyright (c) 1982, 2009, Oracle and/or its affiliates. All rights reserved. Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.2.0 - 64bit Production With the Partitioning option Starting **\u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_FULL_01\u0026#34;**: datapump_admin/******** parfile=fulldp.par Estimate in progress using BLOCKS method... Processing object type DATABASE_EXPORT/SCHEMA/TABLE/TABLE_DATA **Total estimation using BLOCKS method: 13.75 MB** Processing object type DATABASE_EXPORT/TABLESPACE Processing object type DATABASE_EXPORT/PASSWORD_VERIFY_FUNCTION Processing object type DATABASE_EXPORT/PROFILE Processing object type DATABASE_EXPORT/SYS_USER/USER ... Processing object type DATABASE_EXPORT/SCHEMA/TABLE/TABLE ... The job name created to carry out the export is DATAPUMP_ADMIN.SYS_EXPORT_FULL_01. Other jobs will have a different numeric suffix to keep them unique. If something goes wrong with the export, we will need that job name to allow us to fix things. The job, while it is in progress, also creates a table within the schema specified in the userid parameter. That table\u0026rsquo;s name is also called SYS_EXPORT_FULL_01.\nWe can also see that an estimate of the amount of disc space we will need in the location where the dump file is being written to. In my case, 13.75 Mb (It\u0026rsquo;s not a big database!) is required.\nThen we get a list of the objects being exported as they occur. Nothing much here that\u0026rsquo;s new, it\u0026rsquo;s very similar to that output by a full exp in the old days!\nOf course, like many things, it doesn\u0026rsquo;t always go to plan:\n1 2 ORA-39171: Job is experiencing a resumable wait. ORA-01691: unable to extend lob segment DATAPUMP_ADMIN.SYS_LOB0000015147C00045$$ by 128 in tablespace USERS This LOB is part of the above mentioned table. Expdp will sit there (for two hours by default) until I fix the problem. I need to use SQL*Plus (or Toad etc) to fix the underlying problem with space, then expdp can continue.\n1 2 3 4 5 6 7 8 9 10 SQL\u0026gt; select file_id, bytes/1024/1024 as mb 2 from dba_data_files 3 where tablespace_name = \u0026#39;USERS\u0026#39;; FILE_ID\tMB ---------- ---------- 6\t10 SQL\u0026gt; alter database datafile 6 resize 50m; Database altered. As soon as the extra space is added, the datapump job resumes automatically. There is no need to tell it to continue. The screen output starts scrolling again:\n1 2 3 4 5 6 7 8 ... . . exported \u0026#34;SYSTEM\u0026#34;.\u0026#34;REPCAT$_USER_PARM_VALUES\u0026#34; 0 KB 0 rows . . exported \u0026#34;SYSTEM\u0026#34;.\u0026#34;SQLPLUS_PRODUCT_PROFILE\u0026#34; 0 KB 0 rows **Master table \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_FULL_01\u0026#34; successfully loaded/unloaded** ****************************************************************************** Dump file set for DATAPUMP_ADMIN.SYS_EXPORT_FULL_01 is: /srv/oracle/datapump/full.dmp Job \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_FULL_01\u0026#34; **completed with 5 error(s)** at 15:46:57 The message about the master table means that the table has now been dropped as the datapump job completed. The final message, indicating a number of errors can be quite threatening, but is simply the number of times that the utility output a message to the screen (and log file) telling you that there is a problem:\n1 2 3 4 5 6 7 8 9 10 ORA-39171: Job is experiencing a resumable wait. ORA-01691: unable to extend lob segment DATAPUMP_ADMIN.SYS_LOB0000015147C00045$$ by 128 in tablespace USERS ORA-39171: Job is experiencing a resumable wait. ORA-01691: unable to extend lob segment DATAPUMP_ADMIN.SYS_LOB0000015147C00045$$ by 128 in tablespace USERS ORA-39171: Job is experiencing a resumable wait. ORA-01691: unable to extend lob segment DATAPUMP_ADMIN.SYS_LOB0000015147C00045$$ by 128 in tablespace USERS ORA-39171: Job is experiencing a resumable wait. ORA-01691: unable to extend lob segment DATAPUMP_ADMIN.SYS_LOB0000015147C00045$$ by 128 in tablespace USERS ORA-39171: Job is experiencing a resumable wait. ORA-01691: unable to extend lob segment DATAPUMP_ADMIN.SYS_LOB0000015147C00045$$ by 128 in tablespace USERS As you can see, 5 \u0026ldquo;errors\u0026rdquo; that are not really errors, they are merely hints that something needed attending to a bit quicker than I managed!\nThe full log file for the job is created in the output area, as is the dump file itself.\n1 2 3 4 5 $ ls -l /srv/oracle/datapump/ total 18360 -rw-r----- 1 oracle users 18751488 2013-08-29 15:46 full.dmp -rw-r--r-- 1 oracle users 23410 2013-08-29 15:46 full.exp.log One thing that a number of DBAs will be miffed at, you cannot perform compression of the dump file \u0026ldquo;on the fly\u0026rdquo; as we used to do in the old days of exp:\n1 2 3 $ mknop exp.pipe p $ cat exp.pipe | gzip -9 - \u0026gt; full.dmp.gz \u0026amp; $ exp ... file=exp.pipe log=full.log .... This is no longer allowed, but expdp does have a compression parameter however, you need to have paid extra for the Advanced Compression Option. Not good! You are allowed, without extra costs, to compress the metadata only when exporting. Thanks Oracle. Add the following to the parameter file:\n1 compression=metadata_only The default is compression=none.\nAnd, also, if you run the same export again, then expdp will crash out because it doesn\u0026rsquo;t like overwriting files that already exist.\n1 $ expdp parfile=fulldp.par 1 2 3 4 5 6 7 8 9 Export: Release 11.2.0.2.0 - Production on Thu Aug 29 15:59:29 2013 Copyright (c) 1982, 2009, Oracle and/or its affiliates. All rights reserved. Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.2.0 - 64bit Production With the Partitioning option ORA-39001: invalid argument value ORA-39000: bad dump file specification ORA-31641: unable to create dump file \u0026#34;/srv/oracle/datapump/full.dmp\u0026#34; ORA-27038: created file already exists Additional information: 1 There are two ways to avoid this issue:\nDon\u0026rsquo;t put the file names in the parameter file. Take them out and specify them on the command line, explicitly:\n1 $ cat fulldp.par 1 2 3 userid=datapump_admin/secret directory=my_datapump_dir full=y 1 $ expdp parfile=fulldp.par dumpfile=full.\\`date +%Y%m%d\\`.dmp logfile=full.\\`date +%Y%m%d\\`.exp.log ... Add the following to the parameter file (or command line):\n1 reuse_dumpfiles=y The default is not to reuse the existing dump files.\nI find that putting all the common parameters into the parameter file, while keeping the changeable ones on the command line is probably the best idea.\nWhat about the old consistent=y parameter? Can\u0026rsquo;t you do that with expdp? Well, yes, you can. Since 11gR2 you can anyway. If you add the legacy parameters to the parameter file, or command line as follows:\n1 consistent=y Then expdp will notice that you are a Luddite like me, and tell you what to do next time in order to get a proper expdp consistent export. As follows:\n1 2 3 4 5 6 7 Export: Release 11.2.0.2.0 - Production on **Fri Aug 30 17:05:33 2013** ... **Legacy Mode Active due to the following parameters:** Legacy Mode **Parameter: \u0026#34;consistent=TRUE\u0026#34;** Location: Parameter File, **Replaced with: \u0026#34;flashback_time=TO_TIMESTAMP(\u0026#39;2013-08-30 17:05:33\u0026#39;, \u0026#39;YYYY-MM-DD HH24:MI:SS\u0026#39;)\u0026#34;** **Legacy Mode has set reuse_dumpfiles=true parameter.** Starting \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_FULL_01\u0026#34;: datapump_admin/******** parfile=fulldp.par ... Note the time that the export started, and note how the consistent parameter was converted to a expdp flashback_time parameter set to the same time as the start of the job. That\u0026rsquo;s how to get a consistent export. You can also use the flashback_scn parameter if yo happen to knwo the desired SCN of course.\nNote also that legacy mode turns on, automatically, the ability to overwrite existing dump files, even if you don\u0026rsquo;t specify it in the parameter file or command line.\nYou can, if you wish, add the following to your parameter files, or the command line, according to what you are using, to get an export equivalent to an old consistent=y one from exp:\n1 flashback_time=to_timestamp(sysdate) or:\n1 flashback_time=systimestamp Export Schemas Exporting a schema, or more than one, is equally as simple. Simply specify the schemas= parameter in your parameter file or on the command line:\n1 $ cat user_norman.par 1 2 3 4 userid=datapump_admin/secret directory=my_datapump_dir schemas=norman reuse_dumpfiles=y 1 $ expdp parfile=user_norman.par dumpfile=norman.dmp logfile=norman.exp.log If you have more than one schema to export, simply specify them all with commas between. For example, you might have the following in a parameter file (or on the command line):\n1 schemas=barney,fred,wilma,betty,bambam,dino The output is similar to that for the full database:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Export: Release 11.2.0.2.0 - Production on Thu Aug 29 16:39:30 2013 ... Starting **\u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_SCHEMA_01\u0026#34;**: datapump_admin/******** parfile=user_norman.par Estimate in progress using BLOCKS method... Processing object type SCHEMA_EXPORT/TABLE/TABLE_DATA Total estimation using BLOCKS method: 64 KB Processing object type SCHEMA_EXPORT/USER ... Processing object type SCHEMA_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS . . exported \u0026#34;NORMAN\u0026#34;.\u0026#34;NORM\u0026#34; 5.023 KB 4 rows Master table \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_SCHEMA_01\u0026#34; successfully loaded/unloaded ****************************************************************************** Dump file set for DATAPUMP_ADMIN.SYS_EXPORT_SCHEMA_01 is: /srv/oracle/datapump/norman.01.dmp Job \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_SCHEMA_01\u0026#34; successfully completed at 16:40:04 Export Tablespaces Exporting a tablespace, or more than one, is just as simple as exporting schemas. Simply specify the tablespaces= parameter in your parameter file or on the command line:\n1 $ cat user_tablespace.par 1 2 3 4 5 6 userid=datapump_admin/secret directory=my_datapump_dir dumpfile=users_ts.dmp logfile=users_ts.exp.log tablespaces=users reuse_dumpfiles=y 1 $ expdp parfile=user_tablespace.par This time, I don\u0026rsquo;t care about having a unique name, so I\u0026rsquo;ve specified the logfile and dumpfile parameters within the parameter file.\nIf you have more than one tablespace to export, simply specify them all with commas between. For example, you might have the following in a parameter file (or on the command line):\n1 tablespaces=bedrock_data,bedrock_index The output is similar to that for the full database:\n1 $ expdp parfile=user_tablespace.par 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Export: Release 11.2.0.2.0 - Production on Thu Aug 29 16:45:01 2013 ... Starting **\u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_TABLESPACE_01\u0026#34;**: datapump_admin/******** parfile=user_tablespace.par Estimate in progress using BLOCKS method... Processing object type TABLE_EXPORT/TABLE/TABLE_DATA Total estimation using BLOCKS method: 128 KB Processing object type TABLE_EXPORT/TABLE/TABLE ... Processing object type TABLE_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS . . exported \u0026#34;APP_OWNER\u0026#34;.\u0026#34;TEST\u0026#34; 5.031 KB 5 rows . . exported \u0026#34;NORMAN\u0026#34;.\u0026#34;NORM\u0026#34; 5.023 KB 4 rows Master table \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_TABLESPACE_01\u0026#34; successfully loaded/unloaded ****************************************************************************** Dump file set for DATAPUMP_ADMIN.SYS_EXPORT_TABLESPACE_01 is: /srv/oracle/datapump/users_ts.dmp Job \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_TABLESPACE_01\u0026#34; successfully completed at 16:45:27 Export Tables And finally, for now, exporting a list of tables is simple too. Once again, and as with exp, you are best to fill a parameter file with the required tables.\n1 $ cat tables.par 1 2 3 4 5 6 userid=datapump_admin/secret directory=my_datapump_dir dumpfile=tables.dmp logfile=tables.exp.log tables=norman.norm,app_owner.test reuse_dumpfiles=y As before, I\u0026rsquo;m only interested in these particular tables, so I name them in the parameter file. Also, dumpfile and logfile are in there too.\nYou should be quite familiar with the output by now:\n1 expdp parfile=tables.par 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Export: Release 11.2.0.2.0 - Production on Thu Aug 29 16:49:05 2013 ... Starting **\u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_TABLE_01\u0026#34;**: datapump_admin/******** parfile=tables.par Estimate in progress using BLOCKS method... Processing object type TABLE_EXPORT/TABLE/TABLE_DATA Total estimation using BLOCKS method: 128 KB Processing object type TABLE_EXPORT/TABLE/TABLE Processing object type TABLE_EXPORT/TABLE/STATISTICS/TABLE_STATISTICS . . exported \u0026#34;APP_OWNER\u0026#34;.\u0026#34;TEST\u0026#34; 5.031 KB 5 rows . . exported \u0026#34;NORMAN\u0026#34;.\u0026#34;NORM\u0026#34; 5.023 KB 4 rows Master table \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_TABLE_01\u0026#34; successfully loaded/unloaded ****************************************************************************** Dump file set for DATAPUMP_ADMIN.SYS_EXPORT_TABLE_01 is: /srv/oracle/datapump/tables.dmp Job \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_TABLE_01\u0026#34; successfully completed at 16:49:12 Points to Note Job names. The job names created for an expdp or impdp job are made up as follows:\nschema_name + \u0026ldquo;.\u0026rdquo; + \u0026ldquo;SYS_\u0026rdquo; + \u0026ldquo;EXPORT_\u0026rdquo; or \u0026ldquo;IMPORT_\u0026rdquo; + Level + \u0026ldquo;_\u0026rdquo; + Unique Identifier.\nExpdp uses \u0026ldquo;EXPORT\u0026rdquo; while impdp uses \u0026ldquo;IMPORT\u0026rdquo;, for obvious reasons. The level part is one of:\nFULL SCHEMA TABLESPACE TABLE The unique identifier is simply a numeric suffix, starting at 01 and increasing for each concurrent datapump job at that level.\nSo, a job running under the schema of datapump_admin, exporting a schema level dump, would have the full job name of:\nDATAPUMP_ADMIN.SYS_EXPORT_SCHEMA_01\nWhile the same user, exporting a full database would have the job name of\nDATAPUMP_ADMIN.SYS_EXPORT_FULL_01\nTables created for jobs. As mentioned above, datapump jobs create a table in the default tablespace of the user running the utility. The table names are exactly the same as the running job names.\nWhen the job completes, the tables are dropped.\nEstimating space requirements. As seen above, expdp produces an estimate of the space it will need to carry out the requested export. However, it might be nice to have this information well in advance of running the export - so that you can be sure that the export will work without problems. This can be done:\n1 $ expdp datapump_admin/secret full=y estimate_only=y 1 2 3 4 5 6 7 8 9 10 11 12 Export: Release 11.2.0.2.0 - Production on Thu Aug 29 16:27:26 2013 ... Starting \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_FULL_01\u0026#34;: datapump_admin/******** full=y estimate_only=y Estimate in progress using BLOCKS method... Processing object type DATABASE_EXPORT/SCHEMA/TABLE/TABLE_DATA ... . estimated \u0026#34;SYSTEM\u0026#34;.\u0026#34;REPCAT$_TEMPLATE_TARGETS\u0026#34; 0 KB . estimated \u0026#34;SYSTEM\u0026#34;.\u0026#34;REPCAT$_USER_AUTHORIZATIONS\u0026#34; 0 KB . estimated \u0026#34;SYSTEM\u0026#34;.\u0026#34;REPCAT$_USER_PARM_VALUES\u0026#34; 0 KB . estimated \u0026#34;SYSTEM\u0026#34;.\u0026#34;SQLPLUS_PRODUCT_PROFILE\u0026#34; 0 KB **Total estimation using BLOCKS method: 13.75 MB** Job \u0026#34;DATAPUMP_ADMIN\u0026#34;.\u0026#34;SYS_EXPORT_FULL_01\u0026#34; successfully completed at 16:27:39 Now, armed with the above information, you can make sure that the destination for the dump file(s) has enough free space. Don\u0026rsquo;t forget, there will be a log file as well.\nThe next article in this short series will feature the corresponding imports using impdp.\nExporting - Cheat Sheet The following is a list of \u0026ldquo;cheats\u0026rdquo; - basically, a list of the parameters you would find useful in doing a quick export at any level from full down to individual tables. I have listed each one in the form of a parameter file - for ease of copy and paste, which you are free to do by the way, assuming you find it interesting and/or useful!\nThe following assumes you have set up a suitable Oracle Directory object within the database being exported, however, the first part of the cheat sheet summarises the commands required to create one, and a suitably endowed user to carry out the exports.\nAll the following default to consistent exports, there\u0026rsquo;s really no reason why an export should be taken any other way!\nCreate an Oracle Directory The following is executed in SQL*Plus, or similar, as a SYSDBA enabled user, or SYS:\n1 create directory **my_datapump_dir** as \u0026#39;/your/required/location\u0026#39;; The following is executed as root:\n1 2 mkdir -p /your/required/location chown oracle:dba /your/required/location Create a Datapump User Account and Privileges The following is executed in SQL*Plus, or similar, as a SYSDBA enabled user, or SYS:\n1 2 3 4 5 6 7 8 create user datapump_admin identified by secret_password default tablespace users quota unlimited on users; grant create table to datapump_admin; grant datapump_exp_full_database to datapump_admin; grant datapump_imp_full_database to datapump_admin; grant read, write on directory **my_datapump_dir** to datapump_admin; Full, Consistent Export 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=**my_datapump_dir** dumpfile=full.dmp logfile=full.exp.log **full=y** reuse_dumpfiles=y flashback_time=\u0026#34;TO_TIMESTAMP(\u0026#39;your_date_time_here\u0026#39;, \u0026#39;YYYY-MM-DD HH24:MI:SS\u0026#39;)\u0026#34; You may wish to use consistent=y rather than the flashback_time, it will default to the timestamp of the start of the expdp job.\nConsistent Schema(s) Export 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=**my_datapump_dir** dumpfile=schema.dmp logfile=schema.exp.log **schemas=user_a,user_b** reuse_dumpfiles=y flashback_time=\u0026#34;TO_TIMESTAMP(\u0026#39;your_date_time_here\u0026#39;, \u0026#39;YYYY-MM-DD HH24:MI:SS\u0026#39;)\u0026#34; You may wish to use consistent=y rather than the flashback_time, it will default to the timestamp of the start of the expdp job.\nConsistent Tablespace(s) Export 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=**my_datapump_dir** dumpfile=tablespaces.dmp logfile=tablespaces.exp.log **tablespaces=ts_a,ts_b** reuse_dumpfiles=y flashback_time=\u0026#34;TO_TIMESTAMP(\u0026#39;your_date_time_here\u0026#39;, \u0026#39;YYYY-MM-DD HH24:MI:SS\u0026#39;)\u0026#34; You may wish to use consistent=y rather than the flashback_time, it will default to the timestamp of the start of the expdp job.\nConsistent Table(s) Export 1 2 3 4 5 6 7 userid=datapump_admin/secret directory=**my_datapump_dir** dumpfile=tables.dmp logfile=tables.exp.log **tables=user.table_a,user.table_b** reuse_dumpfiles=y flashback_time=\u0026#34;TO_TIMESTAMP(\u0026#39;your_date_time_here\u0026#39;, \u0026#39;YYYY-MM-DD HH24:MI:SS\u0026#39;)\u0026#34; You may wish to use consistent=y rather than the flashback_time, it will default to the timestamp of the start of the expdp job.\nAdding Compression By default there is no compression. You add it as follows in 10g and higher with no further options purchased:\n1 compression=metadata_only Or off with this:\n1 compression=none Adding Even More Compression If you have purchased a license for Oracle\u0026rsquo;s Advanced Compression Option, in Oracle 11g and higher, then you have the options of adding compression as follows:\nNothing is compressed:\n1 compression=none Only the metadata is to be compressed:\n1 compression=metadata_only Compress the data only:\n1 compression=none Compress the metadata and the data:\n1 compression=all Even More Compression, in 12c From 12c, a new parameter named compression_algorithm allows you to specify which level of compression you would like:\n1 2 3 4 5 6 7 compression_algorithm=basic compression_algorithm=low compression_algorithm=medium compression_algorithm=high These options may well incur extra CPU costs as the data are required to be compressed for export and uncompressed on import.\nAdding Encryption If you have Enterprise Edition and have paid the extra cost license fee for the Oracle Advanced Security option, then you can encrypt the data in the dump files.\nEncrypt the metadata and the data:\n1 encryption=all Encrypt the data only:\n1 encryption=data_only Encrypt just the metadata:\n1 encryption=data_only No encryption (the default):\n1 encryption=none Encrypt the data in columns that are defined as encrypted in the database:\n1 encryption=encrypted_columns_only ","description":"","id":68,"section":"posts","tags":null,"title":"Introduction to Oracle Datapump - Part 1","uri":"http://localhost:1313/RantsAndRaves/posts/2013/08/introduction-to-oracle-datapump-part-1/"},{"content":"A slight variation on the incremental backups. In this (short) article, I demonstrate the use of database file copy backups which are themselves updated on a regular basis to avoid having to restore and recover using numerous incremental backups.\nWhat\u0026rsquo;s Going On Here? This took me a wee while to get my head around. We can take a backup of the database, in incremental form, and then, use our nightly incremental backups to update the backup itself to the latest state of the database. This means that although we are taking a regular incremental backup, a restore will require much less work as the backup is nearly up to date.\nThis requires the use of file copy backups, as opposed to backupsets, and was hinted at in the previous article where I introduced incremental backups.\nImage copies of the database are usually full copies. You would take a copy of the data files and do that every night, however, with RMAN, we can take an initial file copy of the database and then, incrementally, update that with each day\u0026rsquo;s changes. It\u0026rsquo;s not quite as simple as that, as we shall see, but bear with me!\nFile Copy Updates In order to perform this type of backup, we need to use the following commands in a run block:\n1 2 3 4 5 6 7 RMAN\u0026gt; run { 2\u0026gt; recover copy of database with tag \u0026#34;Daily\u0026#34;; 3\u0026gt; backup incremental level 1 4\u0026gt; for recover of copy 5\u0026gt; with tag \u0026#34;Daily\u0026#34; 6\u0026gt; database; 7\u0026gt; } So, what happens?\nDay 1 - there is nothing to recover and no file copy backup of the database exists. The commands will therefore create a new file copy of the database. The autobackup of the controlfile etc creates a backup as normal. The list backup summary command will show the latter while the list copy of database command will show details of the former. Unfortunately, there is no summary option on the list copy command.\nDay 2 - Yesterday\u0026rsquo;s file copy still cannot be recovered because there is, as yet, no incremental backups to apply. However, this pass through will create a new level 1 incremental backup. As ever, whatever has been configured to autobackup, will do so.\nSubsequent days - Yesterday\u0026rsquo;s level 1 backup will be used to recover the file copy of the database taken on Day 1. A new level 1 incremental backup will be created afterwards and finally, the usual autobackup requirements will be carried out.\nWith the exception of the first two runs of these commands, there will be a file copy of the database as of the end of yesterday\u0026rsquo;s level 1 backup and a new level 1 backup from today. A recovery from this backup situation will involve restoring from the file copy and recovering from the level 1 backup - and any online-redo logs as required.\nA Worked Example As ever, set NLS_DATE_FORMAT in the shell prior to running RMAN - this gives better quality date formatting in the output and listings.\n1 2 3 $ export NLS_DATE_FORMAT=\u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39; $rman target / In this demonstration, assume that this is the first time this set of commands has ever been run - or, at least, the first time for this tag value.\n1 2 3 4 5 6 7 8 RMAN\u0026gt;### DAY ONE ### 2\u0026gt; run { recover copy of database with tag \u0026#34;Daily Backup\u0026#34;; backup incremental level 1 for recover of copy with tag \u0026#34;Daily Backup\u0026#34; database; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Starting recover at 13/08/2013 10:02:53 using channel ORA_DISK_1 no copy of datafile 1 found to recover ... Finished recover at 13/08/2013 10:02:53 Starting backup at 13/08/2013 10:02:53 using channel ORA_DISK_1 no parent backup or copy of datafile 1 found ... channel ORA_DISK_1: starting datafile copy input datafile file number=00004 name=/srv/nffs/oradata/ant12/data/perfstat01_01.dbf output file name=/srv/nffs/flashback_area/ant12/ANT12/datafile/o1_mf_perfstat_90mxky4f_.dbf tag=DAILY BACKUP RECID=118 STAMP=823341801 channel ORA_DISK_1: datafile copy complete, elapsed time: 00:00:35 ... Finished backup at 13/08/2013 10:04:53 Starting Control File and SPFILE Autobackup at 13/08/2013 10:04:53 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2013_08_13/o1_mf_s_823341893_90mxopfw_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 13/08/2013 10:04:56 1 RMAN\u0026gt; list backup summary; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 List of Backups =============== Key TY LV S Device Type Completion Time #Pieces #Copies Compressed Tag ------- -- -- - ----------- ------------------- ------- ------- ---------- --- ... 117 B F A DISK 13/08/2013 10:04:55 1 1 NO TAG20130813T100453 RMAN\u0026gt; list copy; ... List of Datafile Copies ======================= Key File S Completion Time Ckp SCN Ckp Time ------- ---- - ------------------- ---------- ------------------- 119 1 A 13/08/2013 10:03:52 682467 13/08/2013 10:03:29 Name: /srv/nffs/flashback_area/ant12/ANT12/datafile/o1_mf_system_90mxm1cv_.dbf Tag: DAILY BACKUP ... So, we can see that the recover command did nothing as there was no file copy to be recovered. The backup command started by noting that the requested level 1 incremental backup didn\u0026rsquo;t have a parent (level zero) backup and so created a file copy backup with our requested tag.\nWhen we look at the list of backups, we see only one - the tag matches the date and time of the controlfile autobackup and this is confirmed if we list backupset 117.\nThe following day, we execute exactly the same set of commands.\n1 2 3 4 5 6 7 8 RMAN\u0026gt; ### DAY TWO ### 2\u0026gt;run { recover copy of database with tag \u0026#34;Daily Backup\u0026#34;; backup incremental level 1 for recover of copy with tag \u0026#34;Daily Backup\u0026#34; database; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Starting recover at 13/08/2013 10:05:53 using channel ORA_DISK_1 no copy of datafile 1 found to recover ... Finished recover at 13/08/2013 10:05:53 Starting backup at 13/08/2013 10:05:53 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental level 1 datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set input datafile file number=00004 name=/srv/nffs/oradata/ant12/data/perfstat01_01.dbf ... channel ORA_DISK_1: starting piece 1 at 13/08/2013 10:05:53 channel ORA_DISK_1: finished piece 1 at 13/08/2013 10:05:54 piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_13/o1_mf_nnnd1_DAILY_BACKUP_90mxqkq9_.bkp tag=DAILY BACKUP comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:01 Finished backup at 13/08/2013 10:05:54 Starting Control File and SPFILE Autobackup at 13/08/2013 10:05:54 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2013_08_13/o1_mf_s_823341954_90mxqm3y_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 13/08/2013 10:05:55 1 RMAN\u0026gt; list backup summary; 1 2 3 4 5 6 7 8 List of Backups =============== Key TY LV S Device Type Completion Time #Pieces #Copies Compressed Tag ------- -- -- - ----------- ------------------- ------- ------- ---------- --- ... 117 B F A DISK 13/08/2013 10:04:55 1 1 NO TAG20130813T100453 118 B 1 A DISK 13/08/2013 10:05:54 1 1 NO DAILY BACKUP 119 B F A DISK 13/08/2013 10:05:55 1 1 NO TAG20130813T100554 1 RMAN\u0026gt; list copy; 1 2 3 4 5 6 7 8 9 10 11 ... List of Datafile Copies ======================= Key File S Completion Time Ckp SCN Ckp Time ------- ---- - ------------------- ---------- ------------------- 119 1 A 13/08/2013 10:03:52 682467 13/08/2013 10:03:29 Name: /srv/nffs/flashback_area/ant12/ANT12/datafile/o1_mf_system_90mxm1cv_.dbf Tag: DAILY BACKUP ... At the end of day two, we see that the file copy backup has still not been recovered. This is because there wasn\u0026rsquo;t a level one incremental backup created yesterday. Remember, the recovery applies the latest incremental backup to the file copy and that incremental backup has not been created yet.\nWe then see that a new level one incremental backup is being created with our requested tag value, followed by the normal autobackup stuff.\nListing the backups shows that backups 118 and 119 have been created today. The latter is the autobackup and the former is our new (first) incremental backup. It is a coincidence that the file copy is also 119 - this need not always be the case, as tomorrow will show.\nAnd the third, and subsequent, runs produce output similar to that shown below.\n1 2 3 4 5 6 7 8 RMAN\u0026gt; ### DAY THREE AND SUBSEQUENT DAYS ### 2\u0026gt; run { recover copy of database with tag \u0026#34;Daily Backup\u0026#34;; backup incremental level 1 for recover of copy with tag \u0026#34;Daily Backup\u0026#34; database; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Starting recover at 13/08/2013 10:06:45 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental datafile backup set restore channel ORA_DISK_1: specifying datafile copies to recover recovering datafile copy file number=00001 name=/srv/nffs/flashback_area/ant12/ANT12/datafile/o1_mf_system_90mxm1cv_.dbf ... channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_13/o1_mf_nnnd1_DAILY_BACKUP_90mxqkq9_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_13/o1_mf_nnnd1_DAILY_BACKUP_90mxqkq9_.bkp tag=DAILY BACKUP channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 Finished recover at 13/08/2013 10:06:46 Starting backup at 13/08/2013 10:06:47 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental level 1 datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set input datafile file number=00004 name=/srv/nffs/oradata/ant12/data/perfstat01_01.dbf ... channel ORA_DISK_1: starting piece 1 at 13/08/2013 10:06:47 channel ORA_DISK_1: finished piece 1 at 13/08/2013 10:06:48 piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_13/o1_mf_nnnd1_DAILY_BACKUP_90mxs7ob_.bkp tag=DAILY BACKUP comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:01 Finished backup at 13/08/2013 10:06:48 Starting Control File and SPFILE Autobackup at 13/08/2013 10:06:48 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2013_08_13/o1_mf_s_823342008_90mxs90z_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 13/08/2013 10:06:49 1 RMAN\u0026gt; list backup summary; 1 2 3 4 5 6 7 8 9 10 List of Backups =============== Key TY LV S Device Type Completion Time #Pieces #Copies Compressed Tag ------- -- -- - ----------- ------------------- ------- ------- ---------- --- ... 117 B F A DISK 13/08/2013 10:04:55 1 1 NO TAG20130813T100453 118 B 1 A DISK 13/08/2013 10:05:54 1 1 NO DAILY BACKUP 119 B F A DISK 13/08/2013 10:05:55 1 1 NO TAG20130813T100554 120 B 1 A DISK 13/08/2013 10:06:47 1 1 NO DAILY BACKUP 121 B F A DISK 13/08/2013 10:06:49 1 1 NO TAG20130813T100648 1 RMAN\u0026gt; list copy; 1 2 3 4 5 6 7 8 9 10 11 ... List of Datafile Copies ======================= Key File S Completion Time Ckp SCN Ckp Time ------- ---- - ------------------- ---------- ------------------- 131 1 A 13/08/2013 10:06:45 682600 13/08/2013 10:05:53 Name: /srv/nffs/flashback_area/ant12/ANT12/datafile/o1_mf_system_90mxm1cv_.dbf Tag: DAILY BACKUP ... We see from the above that we have finally recovered the image copy of the database from yesterday\u0026rsquo;s level one backup. So that takes the image copy of the data files up to date until the end of yesterday\u0026rsquo;s backup. We also see that a brand new level one backup was taken with today\u0026rsquo;s changes copied. This will be recovered into the file copy backup on tomorrow\u0026rsquo;s run of these commands. The file copy of the database will always be as up to date as of yesterday and not as of today.\nLooking at the list of backups, we can see the most recent ones are the tagged daily backup which is the level one created just now and the usual autobackup one.\nLooking at the file copies, we see that yesterday\u0026rsquo;s key numbers have gone, and new ones are in place as are the checkpoint SCN numbers - the files are updated in other words.\nRunning Recoveries So, what happens when we need to recover a tablespace, for example?\n1 2 3 4 RMAN\u0026gt; sql \u0026#34;alter tablespace users offline\u0026#34;; sql statement: alter tablespace users offline RMAN\u0026gt; restore tablespace users; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Starting restore at 13/08/2013 10:57:59 using channel ORA_DISK_1 channel ORA_DISK_1: restoring datafile 00006 input datafile copy RECID=140 STAMP=823343078 file name=/srv/nffs/flashback_area/ant12/ANT12/datafile/o1_mf_users_90mxonmc_.dbf destination for restore of datafile 00006: /srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: copied datafile copy of datafile 00006 output file name=/srv/nffs/oradata/ant12/data/users01.dbf RECID=0 STAMP=0 Finished restore at 13/08/2013 10:58:00 RMAN\u0026gt; recover tablespace users; Starting recover at 13/08/2013 10:58:06 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set destination for restore of datafile 00006: /srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_13/o1_mf_nnnd1_DAILY_BACKUP_90mytqy3_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_13/o1_mf_nnnd1_DAILY_BACKUP_90mytqy3_.bkp tag=DAILY BACKUP channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:00 starting media recovery media recovery complete, elapsed time: 00:00:00 Finished recover at 13/08/2013 10:58:06 1 2 RMAN\u0026gt; sql \u0026#34;alter tablespace users online\u0026#34;; sql statement: alter tablespace users online The first thing we see is that the file with recid = 140 is being restored as a file copy. We can see what this means with the following:\n1 RMAN\u0026gt; list datafilecopy 140; 1 2 3 4 5 6 7 8 List of Datafile Copies ======================= Key File S Completion Time Ckp SCN Ckp Time ------- ---- - ------------------- ---------- ------------------- 140 6 A 13/08/2013 10:24:38 682690 13/08/2013 10:06:47 Name: /srv/nffs/flashback_area/ant12/ANT12/datafile/o1_mf_users_90mxonmc_.dbf Tag: DAILY BACKUP Or, we could have found out what files would have been restored for the tablespace as follows:\n1 RMAN\u0026gt; list copy of tablespace users; 1 2 3 4 5 6 7 8 List of Datafile Copies ======================= Key File S Completion Time Ckp SCN Ckp Time ------- ---- - ------------------- ---------- ------------------- 140 6 A 13/08/2013 10:24:38 682690 13/08/2013 10:06:47 Name: /srv/nffs/flashback_area/ant12/ANT12/datafile/o1_mf_users_90mxonmc_.dbf Tag: DAILY BACKUP Either way, we now know that a file copy was restored. The recovery phase isn\u0026rsquo;t so easy to decode, but we can see which backuppiece was used. It is highlighted in bold above. Checking what that actually is gives the following:\n1 RMAN\u0026gt; list backuppiece \u0026#34;/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_13/o1_mf_nnnd1_DAILY_BACKUP_90mytqy3_.bkp\u0026#34;; 1 2 3 4 List of Backup Pieces BP Key BS Key Pc# Cp# Status Device Type Piece Name ------- ------- --- --- ----------- ----------- ---------- 122 122 1 1 AVAILABLE DISK /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_13/o1_mf_nnnd1_DAILY_BACKUP_90mytqy3_.bkp This gives the backupset key number, which also happens to be 122. We can interrogate this next:\n1 RMAN\u0026gt; list backupset 122; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 List of Backup Sets =================== BS Key Type LV Size Device Type Elapsed Time Completion Time ------- ---- -- ---------- ----------- ------------ ------------------- 122 Incr 1 464.00K DISK 00:00:01 13/08/2013 10:24:40 BP Key: 122 Status: AVAILABLE Compressed: NO Tag: DAILY BACKUP Piece Name: /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_13/o1_mf_nnnd1_DAILY_BACKUP_90mytqy3_.bkp List of Datafiles in backup set 122 File LV Type Ckp SCN Ckp Time Name ---- -- ---- ---------- ------------------- ---- 1 1 Incr 683288 13/08/2013 10:24:39 /srv/nffs/oradata/ant12/data/system01.dbf 2 1 Incr 683288 13/08/2013 10:24:39 /srv/nffs/oradata/ant12/data/sysaux01.dbf 3 1 Incr 683288 13/08/2013 10:24:39 /srv/nffs/oradata/ant12/data/undotbs01.dbf 4 1 Incr 683288 13/08/2013 10:24:39 /srv/nffs/oradata/ant12/data/perfstat01_01.dbf 5 1 Incr 683288 13/08/2013 10:24:39 /srv/nffs/oradata/ant12/data/tools01.dbf 6 1 Incr 683288 13/08/2013 10:24:39 /srv/nffs/oradata/ant12/data/users01.dbf 7 1 Incr 683288 13/08/2013 10:24:39 /srv/nffs/oradata/ant12/data/audit01_01.dbf 8 1 Incr 683288 13/08/2013 10:24:39 /srv/nffs/oradata/ant12/data/utility01_01.dbf The output from the above shows that we are applying a level 1 incremental backup. Phew! At least we now know. There must be an easier way that this to find out though, surely?\nRestoring individual data files, the whole database, or just a block or two is as easy as shown in previous examples. Don\u0026rsquo;t forget, tablespaces and data files need to be offline before they can be restored and if the tablespace or the datafiles are SYSTEM or UNDO, then the whole database needs to be in mount mode.\n","description":"","id":69,"section":"posts","tags":null,"title":"Oracle RMAN for Beginners â Part 10","uri":"http://localhost:1313/RantsAndRaves/posts/2013/08/oracle-rman-for-beginners-part-10/"},{"content":"It\u0026rsquo;s been a while since the previous post in this series, but I\u0026rsquo;m back again. This time out, we are looking at incremental backups. What they are, how they work, and how - of course - to take them and use the to restore and recover your databases.\nMore Terminology What exactly is an incremental backup? Previously, this series has shown you how to take a full backup be that of the database, archived logs, tablespaces and data files. Each time, for example, you back up a full database, you copy everything - even data that have not changed. This does make it simpler to restore and recover the database as you only require the most recent backup, but it can take quite some time if you are backing up a multi-terabyte database every night.\nAn incremental backup works on the principle that if you already have a backup, and only some of your data has changed, then surely it will be quicker to just backup the changed data? This is exactly how it works.\nIncremental Backup Levels There are two incremental backup levels:\nLevel 0 (zero) - this is almsot the equivalent of a full backup, but it prepares the way for any following incremental backups, of any kind (see below), by giving you a starting position from which to work.\nYou must start from a level zero incremental backup and not a previous full backup. Although both backup everything, only the level zero incremental one can be used as a parent backup for future level one backups.\nLevel 1 (one) - this is your normal incremental backup level. It only backs up blocks which changed recently. Which blocks it backs up are dependent on the backup type - which is described below. A level 1 incremental backup should be smaller and faster than a full or level 0 backup as it normally wouldn\u0026rsquo;t copy the entire database. However, the final size does depend on the number of changes, and as mentioned, the backup type.\nIncremental Backup Types There are two different incremental backup types:\nDifferential - the default incremental backup type. This backs up only those blocks which have been changed since the previous level 0 or level 1 incremental backups. Cumulative - this incremental backup will backup all changed blocks since the previous level 0 incremental backup. All incremental backups use backup sets, not file copies. However, see the next article in the series for details of something which may, at first glance, appear to contradict this statement! (Merged file copy incremental backups.)\nA Contrived Example The following backup strategy should hopefully clarify things:\nOn Monday, you take a level 0 incremental backup. On Tuesday, you take a differential level 1 incremental backup. This means that only those blocks which changed since the previous level 0 or level 1 backup will be copied. This means, changes since Monday\u0026rsquo;s level 0 backup. On Wednesday, you take another differential level 1 incremental backup. This means that only those blocks which changed since the previous level 0 or level 1 backup will be copied. This means, changes since Tuesday\u0026rsquo;s level 1 backup. On Thursday, you take a cumulative level 1 incremental backup. This backup will copy all changed blocks from the previous level 0 backup - which took place on Monday. On Friday, you take a differential level 1 incremental backup. This backup will copy all changed blocks from the previous level 0 or level 1 backup - which took place on Thursday - because you can mix and match the level 1 backup types without any worries. On Saturday, you take another differential level 1 incremental backup. This backup will copy all changed blocks from the previous level 0 or level 1 backup - which took place on Friday. You have Sunday off to be with your family! Why would you want to take a cumulative backup at certain points when you are already taking differential backups? Well, even though you are taking incremental backups, it is possible that there will be a lot of changes. Cumulative backups help with the restoration of the database after a failure - they can reduce the amount of work you need to do, tapes to be located etc.\nHere\u0026rsquo;s what RMAN would do in the event of a database failure on any of the days when the failure is prior to the backups being successfully completed. Assume that the entire database will require restoring and recovery.\nTuesday - Restore Monday\u0026rsquo;s backup and use the archived logs to recover the database. Wednesday - As for Tuesday, but Tuesday\u0026rsquo;s level 1 backup would be required in addition to the archived logs for recovery. Thursday - As for Wednesday, but Wednesday\u0026rsquo;s level 1 backup would be required in addition to the archived logs for recovery. Friday - Restore Monday\u0026rsquo;s backup, recover using Thursday\u0026rsquo;s cumulative backup, in addition to the archived logs needed to bring us back up to date. Saturday - As per Friday, but also requires Friday\u0026rsquo;s backup as part of the recovery. Sunday - As per Saturday but add in Saturday\u0026rsquo;s backup as part of the recovery. Monday - As per Sunday, but you better make sure that you have all the archived logs that were created after the Saturday backup in order that you can recover bang up to date! Maybe you should consider reviewing the above backup strategy? ;-) So you can see, using cumulative backups reduces the number of incremental backups you need to use to recover the database. RMAN will choose the best backup it knows about - in the controlfile or the catalogue - to restore the database - this may be a level 0 incremental backup.\nWhen you recover the database, RMAN will choose to use archived logs or an incremental level 1 backup or a mixture of both, as required, to get the database back to where you requested it.\nIncremental backups take less time to produce, but can require a little more thought when trying to restore and recover a database (tablespace etc) as you may need to keep more backups in the backup location to allow an adequate recovery window.\nFull backups are easier, RMAN will most likely restore the most recent backup and use the archived logs to recover the database, but as mentioned, these backups may be very much larger.\nSlightly Technical Stuff Under normal conditions, RMAN will perform an incremental backup by reading each and every (non-virgin) block into the buffer and checking if the SCN is higher (or the same according to Kuhn, Alapati and Nanda) than the SCN of the most recent level 0 (or level 1 depending on the incremental backup type) backup. If the block\u0026rsquo;s SCN is higher, this block requires backup.\nYou can make this process a lot quicker, especially with large databases, by turning on Block Change Tracking. This process uses a binary file to record the blocks that have changed over the normal day to day running of the database, since the most recent level 0/level 1 backup.\nIf this feature is turned on, when RMAN performs an incremental backup at level 1, it doesn\u0026rsquo;t need to read each and every (non-virgin) block into the buffer as it knows already which blocks need copying. You can turn on block change tracking as follows - assuming that we are not using Oracle Managed files:\nSQL\u0026gt; alter database enable block change tracking using file \u0026lsquo;/path/to/file/change_tracking.chg\u0026rsquo; reuse;\nDatabase altered.\nThe filename will be created if it doesn\u0026rsquo;t exist, but the path should already exists or the command will fail. If the file already exists, it will be overwritten. This could cause the loss of existing change tracking data, so check first, to see if it is already enabled:\n1 SQL\u0026gt; select * from v$block_change_tracking; 1 2 3 STATUS FILENAME BYTES ---------- ------------------------------------------------------------------ ----------- ENABLED /srv/nffs/flashback_area/ant12/change_tracking/change_tracking.chg 11599872 Block change tracking can be turned off, if desired, as follows:\n1 2 SQL\u0026gt; alter database disable block_change_tracking; Database altered. You cannot specify a size for the change tracking file. Oracle will allocate the required space itself, based on the database size, the number of data files in the database and the number of redo threads that are enabled. In 11gR2, the file is initially created at 10Mb and grows in 10Mb chunks as new space is required. There is 320 Kb allocated in the file for each data file in the database. (Information from Kuhn, Alapati \u0026amp; Nanda.).\nEnough Talking, Lets Do Backups! As ever, we should consider setting NLS_DATE_FORMAT in the shell before we begin. This way, we get better date\u0026rsquo;s when we look at the dates and times of our backups. You cannot set this within RMAN itself, sadly.\n1 2 3 $ export NLS_DATE_FORMAT=\u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39; $ rman target / Once we are in RMAN and connected to the target database (and catalogue, if one is in use) we can start the incremental process by taking an initial level 0 incremental backup. You will note from the following that I am including the archived logs too. You can\u0026rsquo;t incrementally back those up, so they get copied in the normal manner.\n1 RMAN\u0026gt; backup incremental level 0 database tag \u0026#34;DB Level 0\u0026#34;; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Starting backup at 12/08/2013 15:51:59 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental level 0 datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set input datafile file number=00004 name=/srv/nffs/oradata/ant12/data/perfstat01_01.dbf input datafile file number=00001 name=/srv/nffs/oradata/ant12/data/system01.dbf input datafile file number=00002 name=/srv/nffs/oradata/ant12/data/sysaux01.dbf input datafile file number=00007 name=/srv/nffs/oradata/ant12/data/audit01_01.dbf input datafile file number=00008 name=/srv/nffs/oradata/ant12/data/utility01_01.dbf input datafile file number=00003 name=/srv/nffs/oradata/ant12/data/undotbs01.dbf input datafile file number=00005 name=/srv/nffs/oradata/ant12/data/tools01.dbf input datafile file number=00006 name=/srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: starting piece 1 at 12/08/2013 15:52:00 channel ORA_DISK_1: finished piece 1 at 12/08/2013 15:52:15 piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd0_DB_LEVEL_0_90kxnj9z_.bkp tag=DB LEVEL 0 comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:15 Finished backup at 12/08/2013 15:52:15 Starting Control File and SPFILE Autobackup at 12/08/2013 15:52:15 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2013_08_12/o1_mf_s_823276335_90kxnzkh_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 12/08/2013 15:52:16 Don\u0026#39;t forget the archived logs though! Even though we are doing an incremental backup, the archived logs are still required. 1 RMAN\u0026gt; backup incremental level 0 archivelog all delete input tag \u0026#34;ARC Level 0\u0026#34;; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Starting backup at 12/08/2013 15:54:14 current log archived using channel ORA_DISK_1 channel ORA_DISK_1: starting archived log backup set channel ORA_DISK_1: specifying archived log(s) in backup set input archived log thread=1 sequence=57 RECID=57 STAMP=823275596 input archived log thread=1 sequence=58 RECID=58 STAMP=823275613 input archived log thread=1 sequence=59 RECID=59 STAMP=823276422 input archived log thread=1 sequence=60 RECID=60 STAMP=823276454 channel ORA_DISK_1: starting piece 1 at 12/08/2013 15:54:14 channel ORA_DISK_1: finished piece 1 at 12/08/2013 15:54:15 piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_annnn_ARC_LEVEL_0_90kxrpvq_.bkp tag=ARC LEVEL 0 comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:01 channel ORA_DISK_1: deleting archived log(s) archived log file name=/srv/nffs/flashback_area/ant12/ANT12/archivelog/2013_08_12/o1_mf_1_57_90kwxw9z_.arc RECID=57 STAMP=823275596 archived log file name=/srv/nffs/flashback_area/ant12/ANT12/archivelog/2013_08_12/o1_mf_1_58_90kwyfhj_.arc RECID=58 STAMP=823275613 archived log file name=/srv/nffs/flashback_area/ant12/ANT12/archivelog/2013_08_12/o1_mf_1_59_90kxqpck_.arc RECID=59 STAMP=823276422 RMAN-08138: WARNING: archived log not deleted - must create more backups archived log file name=/srv/nffs/flashback_area/ant12/ANT12/archivelog/2013_08_12/o1_mf_1_60_90kxrpl4_.arc thread=1 sequence=60 Finished backup at 12/08/2013 15:54:15 Starting Control File and SPFILE Autobackup at 12/08/2013 15:54:15 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2013_08_12/o1_mf_s_823276456_90kxrr72_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 12/08/2013 15:54:17 Yes, I know, it looks silly! How on earth can you incrementally backup an archived log? Well, the whole log file will be copied because each and every block has changed! But in effect, it\u0026rsquo;s a normal archived log backup.\nNow, you may be wondering why I didn\u0026rsquo;t just use the following command:\n1 2 3 RMAN\u0026gt; backup incremental level 0 database 2\u0026gt; plus archivelog all delete input 3\u0026gt; tag \u0026#34;DB Level 0\u0026#34;; The command does work, and does backup both the database and the archived logs, however, the archived logs will be copied first and will take the requested tag - \u0026ldquo;DB Level 0\u0026rdquo;, then the archived logs on disc will be deleted as normal - according to the deletion policy configuration. When the database is backed up, the tag is not used, so the actual database backup gets an RMAN generated tag and not the one we wanted. The current redo log is then archived and backed up - getting the requested tag again - and finally the controlfile and spfile are backed up according to the autobackup policy and again, get an RMAN generated tag.\nIt appears that whatever tag is requested is only applied to the first backupset in the backup. If you wish the database and archived log backups to be tagged in this way, then run separate backups with an appropriate tag. The autobackup of the controlfile and/or spfile will not use the requested tag.\nThat\u0026rsquo;s the initial level 0 backup. We can now do some work in the database - I shall not show that here, but you can be assured that I have made some changes to tables in the working schema I\u0026rsquo;m using - and then backup only the changes.\n1 RMAN\u0026gt; backup incremental level 1 database tag \u0026#34;DB LEvel 1\u0026#34;; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Starting backup at 12/08/2013 16:07:18 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental level 1 datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set input datafile file number=00004 name=/srv/nffs/oradata/ant12/data/perfstat01_01.dbf input datafile file number=00001 name=/srv/nffs/oradata/ant12/data/system01.dbf input datafile file number=00002 name=/srv/nffs/oradata/ant12/data/sysaux01.dbf input datafile file number=00007 name=/srv/nffs/oradata/ant12/data/audit01_01.dbf input datafile file number=00008 name=/srv/nffs/oradata/ant12/data/utility01_01.dbf input datafile file number=00003 name=/srv/nffs/oradata/ant12/data/undotbs01.dbf input datafile file number=00005 name=/srv/nffs/oradata/ant12/data/tools01.dbf input datafile file number=00006 name=/srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: starting piece 1 at 12/08/2013 16:07:18 channel ORA_DISK_1: finished piece 1 at 12/08/2013 16:07:19 piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_DB_LEVEL_1_90kyk6qw_.bkp tag=DB LEVEL 1 comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:01 Finished backup at 12/08/2013 16:07:19 Starting Control File and SPFILE Autobackup at 12/08/2013 16:07:19 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2013_08_12/o1_mf_s_823277239_90kyk81j_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 12/08/2013 16:07:20 That\u0026rsquo;s it done, it took all of 1 second to backup the changes I made. Because I didn\u0026rsquo;t specify to perform a cumulative backup, RMAN defaulted to differential, as described above. If I now carry out some more work, and take another level 1 differential backup, I\u0026rsquo;ll get only the changed blocks since the backup we just carried out.\n1 RMAN\u0026gt; backup incremental level 1 database tag \u0026#34;DB Level 1 - part 2\u0026#34;; I have omitted the output from this one as it\u0026rsquo;s almost identical to the one above. Also, you will note, I am not backing up the archived logs - you can assume that I am, because I should be, I\u0026rsquo;m not showing the output though.\nAnd now, we shall take a cumulative incremental backup, which will create a new backup consisting of all the blocks that changed since the previous level zero backup. This is effectively, everything in the two differential backups taken above.\n1 RMAN\u0026gt; backup incremental level 1 cumulative database tag \u0026#34;DB Level 1 - cumulative\u0026#34;; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Starting backup at 12/08/2013 16:12:21 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental level 1 datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set input datafile file number=00004 name=/srv/nffs/oradata/ant12/data/perfstat01_01.dbf ... channel ORA_DISK_1: starting piece 1 at 12/08/2013 16:12:21 channel ORA_DISK_1: finished piece 1 at 12/08/2013 16:12:22 piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_DB_LEVEL_1___CUMULAT_90kytoof_.bkp tag=DB LEVEL 1 - CUMULATIVE comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:01 Finished backup at 12/08/2013 16:12:22 Starting Control File and SPFILE Autobackup at 12/08/2013 16:12:22 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2013_08_12/o1_mf_s_823277542_90kytq64_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 12/08/2013 16:12:23 You will note that there is little indication that this is a cumulative backup, other than the tag I have used to show me what it is! How can we discover what level a backup is, what type and so on?\nListing Backups We can see the backups, and their tags, as follows:\n1 RMAN\u0026gt; list backup summary; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 List of Backups =============== Key TY LV S Device Type Completion Time #Pieces #Copies Compressed Tag ------- -- -- - ----------- ------------------- ------- ------- ---------- --- 62 B A A DISK 12/08/2013 15:38:01 1 1 NO FULL BACKUP 63 B F A DISK 12/08/2013 15:38:10 1 1 NO TAG20130812T153802 64 B A A DISK 12/08/2013 15:38:18 1 1 NO FULL BACKUP 65 B F A DISK 12/08/2013 15:38:20 1 1 NO TAG20130812T153819 70 B 0 A DISK 12/08/2013 15:52:07 1 1 NO DB LEVEL 0 71 B F A DISK 12/08/2013 15:52:16 1 1 NO TAG20130812T155215 72 B A A DISK 12/08/2013 15:53:42 1 1 NO ARC LEVEL 0 73 B F A DISK 12/08/2013 15:53:44 1 1 NO TAG20130812T155343 74 B A A DISK 12/08/2013 15:54:14 1 1 NO ARC LEVEL 0 75 B F A DISK 12/08/2013 15:54:16 1 1 NO TAG20130812T155416 76 B 1 A DISK 12/08/2013 16:07:19 1 1 NO DB LEVEL 1 77 B F A DISK 12/08/2013 16:07:20 1 1 NO TAG20130812T160719 78 B 1 A DISK 12/08/2013 16:09:29 1 1 NO DB LEVEL 1 - PART 2 79 B F A DISK 12/08/2013 16:09:31 1 1 NO TAG20130812T160931 80 B 1 A DISK 12/08/2013 16:12:21 1 1 NO DB LEVEL 1 - CUMULATIVE 81 B F A DISK 12/08/2013 16:12:23 1 1 NO TAG20130812T161222 82 B A A DISK 12/08/2013 16:13:31 1 1 NO ARC LEVEL 1 83 B F A DISK 12/08/2013 16:13:33 1 1 NO TAG20130812T161332 In the above, KEY is the backupset key, TY is the backup type which in these examples is all Backupset types, LV is the backup level - 0 (zero) is incremental level 0, 1 (one) is incremental level 1, A is archived logs and F is a full backup (or autobackup of spfile and/or controlfile). The S column is the status where A indicates an available backup, and finally, the TAG column displays our requested tags.\nYou can see from the various columns how the backups do appear to match up to our tags. With the exception of the RMAN generated tags for the autobackups of course. You will also see that regardless of how the tag was originally specified in the backup command, it is converted to upper case.\nBacking Up Parts of the Database You do not have to carry out an incremental backup of the entire database. As with full backups, you can backup at the tablespace or data file level. You cannot backup individual data blocks though!\nA tablespace backup would be as follows:\n1 2 3 RMAN\u0026gt; backup incremental level 1 2\u0026gt; tablespace users 3\u0026gt; tag \u0026#34;TS USERS - Level 1\u0026#34;; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Starting backup at 12/08/2013 16:34:59 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental level 1 datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set input datafile file number=00006 name=/srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: starting piece 1 at 12/08/2013 16:34:59 channel ORA_DISK_1: finished piece 1 at 12/08/2013 16:35:00 piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_TS_USERS___LEVEL_1_90l053hb_.bkp tag=TS USERS - LEVEL 1 comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:01 Finished backup at 12/08/2013 16:35:00 Starting Control File and SPFILE Autobackup at 12/08/2013 16:35:00 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2013_08_12/o1_mf_s_823278900_90l054z0_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 12/08/2013 16:35:01 While a couple of data files (in different tablespaces, as it happens) would be thus:\n1 2 RMAN\u0026gt; backup incremental level 1 2\u0026gt; datafile 5,7; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Starting backup at 12/08/2013 16:36:23 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental level 1 datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set input datafile file number=00007 name=/srv/nffs/oradata/ant12/data/audit01_01.dbf input datafile file number=00005 name=/srv/nffs/oradata/ant12/data/tools01.dbf channel ORA_DISK_1: starting piece 1 at 12/08/2013 16:36:23 channel ORA_DISK_1: finished piece 1 at 12/08/2013 16:36:24 piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_TAG20130812T163623_90l07qc1_.bkp tag=TAG20130812T163623 comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:01 Finished backup at 12/08/2013 16:36:24 Starting Control File and SPFILE Autobackup at 12/08/2013 16:36:24 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2013_08_12/o1_mf_s_823278984_90l07rml_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 12/08/2013 16:36:25 Restoring Incremental Backups You can, as mentions previously, restore from incremental backups. Given the choice, RMAN will choose the most appropriate backup to restore from, and when you recover, then either a level 1 incremental or the archived logs will be used to recover back to the most recent state of the database. Assuming you didn\u0026rsquo;t specify an SCN or an until time etc of course!\nEqually, as with full backups, you can restore to the database, tablespace, datafile or block level, as well as restoring the archived logs.\nThe following examples show each of these recoveries.\nDatabase Recovery To restore the entire database, it must be shut down to the mounted state. This is no different from carrying out a restore from a full backup.\n1 RMAN\u0026gt; shutdown 1 2 3 database closed database dismounted Oracle instance shut down 1 RMAN\u0026gt; startup mount 1 2 3 4 5 6 connected to target database (not started) Oracle instance started database mounted Total System Global Area 768331776 bytes ... 1 RMAN\u0026gt; restore database; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Starting restore at 12/08/2013 16:42:48 using channel ORA_DISK_1 channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00001 to /srv/nffs/oradata/ant12/data/system01.dbf ... channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd0_DB_LEVEL_0_90kxnj9z_.bkp ... channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:01:16 Finished restore at 12/08/2013 16:44:04 1 RMAN\u0026gt; recover database; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 Starting recover at 12/08/2013 16:46:45 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set destination for restore of datafile 00001: /srv/nffs/oradata/ant12/data/system01.dbf ... channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_DB_LEVEL_1___CUMULAT_90kytoof_.bkp ... channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 channel ORA_DISK_1: starting incremental datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set destination for restore of datafile 00006: /srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_TS_USERS___LEVEL_1_90l053hb_.bkp ... channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:00 channel ORA_DISK_1: starting incremental datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set destination for restore of datafile 00005: /srv/nffs/oradata/ant12/data/tools01.dbf destination for restore of datafile 00007: /srv/nffs/oradata/ant12/data/audit01_01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_TAG20130812T163623_90l07qc1_.bkp ... channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:00 starting media recovery media recovery complete, elapsed time: 00:00:01 Finished recover at 12/08/2013 16:46:48 1 2 RMAN\u0026gt; alter database open; database opened So, RMAN chose to use the Incremental level zero backup to restore everything, then in the recovery phase, applied the cumulative level one backup as opposed to two separate differential level one backups, followed by the tablespace backup of the users tablespace and the data file backup of the data files we backed up above, rather than using archived log backups.\nTablespace Recovery To recover one or more tablespaces, the database can be open unless either (or both) the SYSTEM or UNDO tablespaces needs to be restored and recovered. In that case, the database must be mounted only.\nThe first example restores a single tablespace, USERS, and this can be done with the database open, but the tablespace must be taken off line. Users will only be affected if their work requires access to the tablespace(s) being restored.\n1 2 3 4 RMAN\u0026gt; sql \u0026#34;alter tablespace users offline\u0026#34;; sql statement: alter tablespace users offline RMAN\u0026gt; restore tablespace users; 1 2 3 4 5 6 7 8 9 10 11 Starting restore at 12/08/2013 20:03:12 using channel ORA_DISK_1 channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00006 to /srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd0_DB_LEVEL_0_90kxnj9z_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd0_DB_LEVEL_0_90kxnj9z_.bkp tag=DB LEVEL 0 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 Finished restore at 12/08/2013 20:03:14 1 RMAN\u0026gt; recover tablespace users; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Starting recover at 12/08/2013 20:03:20 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set destination for restore of datafile 00006: /srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_DB_LEVEL_1___CUMULAT_90kytoof_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_DB_LEVEL_1___CUMULAT_90kytoof_.bkp tag=DB LEVEL 1 - CUMULATIVE channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 channel ORA_DISK_1: starting incremental datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set destination for restore of datafile 00006: /srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_TS_USERS___LEVEL_1_90l053hb_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_TS_USERS___LEVEL_1_90l053hb_.bkp tag=TS USERS - LEVEL 1 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:00 starting media recovery media recovery complete, elapsed time: 00:00:01 Finished recover at 12/08/2013 20:03:22 1 2 RMAN\u0026gt; sql \u0026#34;alter tablespace users online\u0026#34;; sql statement: alter tablespace users online You can see, once again, that RMAN chose to restore the initial level zero incremental backup of the database, but only the USERS tablespace was restored from it. The recovery phase used the cumulative level one incremental backup plus the USERS tablespace backup that we took earlier. No archived logs were used in this recovery either.\nThe next example, restores the SYSTEM tablespace and as such, will require the database to be mounted.\n1 RMAN\u0026gt; shutdown; 1 2 ... Oracle instance shut down 1 RMAN\u0026gt; startup mount; 1 2 3 4 5 ... database mounted Total System Global Area 768331776 bytes ... 1 RMAN\u0026gt; restore tablespace system; 1 2 3 4 5 6 7 8 9 10 11 12 Starting restore at 12/08/2013 20:08:15 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=18 device type=DISK channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00001 to /srv/nffs/oradata/ant12/data/system01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd0_DB_LEVEL_0_90kxnj9z_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd0_DB_LEVEL_0_90kxnj9z_.bkp tag=DB LEVEL 0 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:26 Finished restore at 12/08/2013 20:08:41 1 RMAN\u0026gt; recover tablespace system; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Starting recover at 12/08/2013 20:11:03 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set destination for restore of datafile 00001: /srv/nffs/oradata/ant12/data/system01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_backup incremental level 0 database tag \u0026#34;DB Level 0\u0026#34;;area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_DB_LEVEL_1___CUMULAT_90kytoof_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_DB_LEVEL_1___CUMULAT_90kytoof_.bkp tag=DB LEVEL 1 - CUMULATIVE channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 starting media recovery media recovery complete, elapsed time: 00:00:01 Finished recover at 12/08/2013 20:11:05 1 2 RMAN\u0026gt; alter database open; database opened This time RMAN restored from the same level zero backup as previously and recovered from the cumulative level one backup. Again, neither of the differential backups were used simply because the cumulative backup was the best choice for the recovery.\nData File Recovery This example shows the restoration and recovery of a data file that is not a member of the SYSTEM or UNDO tablespaces. For this, the data file(s) need to be taken offline, but the database can remain open. Users will only be affected if their work requires access to the data files being restored.\n1 2 3 4 RMAN\u0026gt; sql \u0026#34;alter database datafile 5 offline\u0026#34;; sql statement: alter database datafile 5 offline RMAN\u0026gt; restore datafile 5; 1 2 3 4 5 6 7 8 9 10 11 Starting restore at 12/08/2013 20:16:39 using channel ORA_DISK_1 channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00005 to /srv/nffs/oradata/ant12/data/tools01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd0_DB_LEVEL_0_90kxnj9z_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd0_DB_LEVEL_0_90kxnj9z_.bkp tag=DB LEVEL 0 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 Finished restore at 12/08/2013 20:16:41 1 RMAN\u0026gt; recover datafile 5; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Starting recover at 12/08/2013 20:16:47 using channel ORA_DISK_1 channel ORA_DISK_1: starting incremental datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set destination for restore of datafile 00005: /srv/nffs/oradata/ant12/data/tools01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_DB_LEVEL_1___CUMULAT_90kytoof_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_DB_LEVEL_1___CUMULAT_90kytoof_.bkp tag=DB LEVEL 1 - CUMULATIVE channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 channel ORA_DISK_1: starting incremental datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set destination for restore of datafile 00005: /srv/nffs/oradata/ant12/data/tools01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_TAG20130812T163623_90l07qc1_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2013_08_12/o1_mf_nnnd1_TAG20130812T163623_90l07qc1_.bkp tag=TAG20130812T163623 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:00 starting media recovery media recovery complete, elapsed time: 00:00:01 Finished recover at 12/08/2013 20:16:49 1 2 RMAN\u0026gt; sql \u0026#34;alter database datafile 5 online\u0026#34;; sql statement: alter database datafile 5 online You can see, as above, the backups used in restoration and recovery have been chosen by RMAN in order to most efficiently get the data file in question, back up to date.\nHad we required to restore and recover data file(s) which are part of the SYSTEM or UNDO tablespaces, we would, once again, have had to have the database in mounted mode as opposed to open. This would, of course, affect all users of this particular database.\nAs this is very similar to the tablespace example given above for SYSTEM, I shall omit the example here.\nData Block Recovery RMAN incremental backups allow the recovery of a single, or multiple, data blocks. Why recover a whole data file when you can make matter much quicker by recovering only the affected blocks - assuming you know which ones are affected of course!\nYou can do this with the database open, even if the blocks are in the SYSTEM or UNDO tablespaces. You will notice below, that there is no need to restore the data blocks in question, you cannot! You simply recover them.\nThis first example recovers a data block in a normal tablespace:\n1 RMAN\u0026gt; recover datafile 7 block 17; 1 2 3 4 5 6 7 Starting recover at 12/08/2013 20:22:23 using channel ORA_DISK_1 starting media recovery media recovery complete, elapsed time: 00:00:01 Finished recover at 12/08/2013 20:22:25 And now, one block in the SYSTEM tablespace:\n1 RMAN\u0026gt; recover datafile 1 block 9; 1 2 3 4 5 6 7 Starting recover at 12/08/2013 20:25:13 using channel ORA_DISK_1 starting media recovery media recovery complete, elapsed time: 00:00:00 Finished recover at 12/08/2013 20:25:13 ","description":"","id":70,"section":"posts","tags":null,"title":"Oracle RMAN for Beginners â Part 9","uri":"http://localhost:1313/RantsAndRaves/posts/2013/08/oracle-rman-for-beginners-part-9/"},{"content":"This post has also been categorised under \u0026ldquo;rants and raves\u0026rdquo; as you will see below! Oracle 10g was the first time that proxy users could be used easily from SQL. Prior to that only Java and/or OCI programs could use them. They\u0026rsquo;ve been around since 8i, but not (well) documented. Want to know more? Read on\u0026hellip;.\nA Bit of Background Many years ago, a software company I worked in - as a DBA - was taken over and we inherited a system (no names - you will see why later) which allowed numerous users the ability to use the system, and some of them got to create documents from within the application. I have no idea to this day, which version of Oracle was the first to be used for the system, but it was apparent (from discussions with their technical people) that it was once a COBOL program using Indexed files as the \u0026ldquo;database\u0026rdquo;. Apparently a straight conversion to Oracle was carried out, replacing each indexed file with an Oracle table.\nThe system was a bit of a nightmare. There were a number - at least three - of application owners in the database. Each of these had privileges and synonyms pointing to the other two, and in a few cases, User_a has a synonym that pointed to one of User_b\u0026rsquo;s objects, and that turned out to be a synonym back to User_a! Go figure. As you can imaging, this did make running a full database import (only exp and imp in those days) a bit of a nightmare with all those circular references back and forth between the application owners.\nThe worst part, and if you are security conscious in any way, I suggest you sit down now and take a deep breath before reading on, was this. The users in the system, who were able to create documents, had the following two privileges assigned in addition to their others:\nCREATE_ANY_PROCEDURE EXECUTE_ANY_PROCEDURE Yes that\u0026rsquo;s correct, in order to create a document, the end users had to be able to create a procedure in the application user\u0026rsquo;s schema, then execute it! End users had their own login of course, to the database, this allowed auditing to work correctly.\nWhen I demonstrated this problem to the head of IT one day, He saw me show how it was simple for an end user to connect to the database and wipe out anything s/he desired, with only those two privileges, plus CREATE SESSION of course. His advice? Do not tell the customers about this!. I didn\u0026rsquo;t.\nI never did get the chance to dig down to discover the reason why the documentation enabled users had to have those abilities, unfortunately, I might have been able to suggest something else instead.\nMoving On - Proxy Users Oracle\u0026rsquo;s proxy users could have been a solution to this massive security problem. The application logged in as each end user and used the two privileges above to create a procedure in the application owner schema, executed it, then dropped the procedure again. That is how the documents were produced.\nHowever, had the application been a little more up to date, and using Oracle 10g, we could have still had the same abilities as above, but without the need to have those nasty \u0026ldquo;ANY\u0026rdquo; privileges. Here\u0026rsquo;s how we could have done it with Proxy Users.\nAssume the following:\nApp_owner is the application owner, at least, the one responsible for document creation. All document creation will be done, within this user, using procedures owned by this user. All other users who require to connect to the database will do so, and will be able to effectively become the app_owner user, but using it as a proxy rather than logging in directly as app_owner. For the purpose of this demonstration, we shall refer to these users, collectively, as doc_user although there can be more than one, obvioulsy. We do not want to have those \u0026ldquo;ANY\u0026rdquo; privileges granted to anyone. Creating APP_OWNER The application owner would be created as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 SQL\u0026gt; create user app_owner 2 identified by app_password 3 default tablespace users 4 quota unlimited on users; User created. SQL\u0026gt; create role app_owner_role; Role created. SQL\u0026gt; grant 2 create session, 3 create table, 4 create procedure, 5 create trigger 6 to app_owner_role; Grant succeeded. SQL\u0026gt; grant app_owner_role to app_owner; Grant succeeded. In the real world, there would be more privileges granted, but these will do for now.\nAt this point in time, the application would be initialised by the creation of tables, procedures, functions, packages and so on. All done under the app_owner user. Once the application has been set up, we can consider creating the doc_user account(s). Before we do so, we need to create a role that defines only the privileges that the doc_user requires when connected to the application as the app_owner:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SQL\u0026gt; connect / as sysdba Connected. SQL\u0026gt; create role doc_user_role; Role created. SQL\u0026gt; grant 2 create procedure, 3 create session 4 to doc_user_role; Grant succeeded. SQL\u0026gt; grant doc_user_role to app_owner; Grant succeeded. Remember, we wish to connect as the doc_user but become the app_owner for the duration of our document production. Therefore, the role we create needs to be granted to the app_owner user and not to the doc_user user(s).\nIn addition, because the app_owner user already has create session via the app_owner_role, you may be wondering why we also grant it to the doc_user_role. Are you wondering? I\u0026rsquo;ll tell you soon. Read on!\nObviously, the role could be enhanced with other privileges, as required, to allow the application requirements to be achieved. For this demonstration, create procedure is enough as we need the doc_user to be able to create and execute a procedure within the app_owner schema.\nCreating DOC_USER The application users, able to create documents, would be created as follows:\n1 2 3 SQL\u0026gt; create user doc_user 2 identified by doc_password; User created. That is all that is required. The doc_user(s) will not be creating tables etc, merely logging into the system, in this case, by becoming the app_owner and using only privileges granted to that user via the doc_user_role. If the doc_users required to connect as themselves for certain parts of the application that didn\u0026rsquo;t involve document production, they would obviously require the appropriate privileges, such as create session\nAs above, the real application would require some other privileges, but these will do for this demonstration.\nSo far, so normal. But, in order to allow the doc_user the ability to login and become the app_owner user, we need to tell Oracle that the app_owner can be connected to, through the doc_user and to only allow the privileges granted to the role doc_user_role:\n1 2 3 4 SQL\u0026gt; alter user app_owner 2 grant connect through doc_user 3 with role doc_user_role; User altered. This is why we had to grant create session to the doc_users_role earlier. If we had not done so, we would have seen the following error when we tried to do a proxy connection:\n1 2 ERROR: ORA-01045: user APP_OWNER lacks CREATE SESSION privilege; logon denied If you see this, make sure that your user - app_owner in this case - has create session granted directly or to the role that will be enabled when proxy connections take place.\nIt is permitted to allow the app_owner to connect through numerous doc_users, it need not be just the one. If you have doc_user_1 through doc_user_n, then execute an alter user as above for each, and any of those will be able to become the app_owner for the purpose of creating documents.\nWhat Magic is This? The alter user statement above had done two things, doc_user is now able to login using app_owner as a proxy and, when it does so, it will actually have logged in as the app_owner and will only have the privileges granted to the doc_user_role available. Had we omitted the with role clause, doc_user would have had all the privileges of app_owner - and this is not as secure as we would like. Oracle applications and thus, users, should operate on the least privilege basis.\nThe doc_user account doesn\u0026rsquo;t even need create session any more, unless it requires to login for other reasons.\nProxy logging in is as follows:\n1 2 3 4 5 SQL\u0026gt; connect doc_user[app_owner]/doc_password Connected. SQL\u0026gt; show user USER is \u0026#34;APP_OWNER\u0026#34; You will hopefully notice two things above:\nEven though the doc_user has no create session privileges, it logged in quite happily with that username and password. Although we logged in as doc_user, we are connected as app_owner. You can see how the doc_user logs in, effectively as itself, but specifies the user that it wants to become after login in square brackets. Because the app_user has been told to use the role doc_user_role, then after becoming app_owner, only that role will be enabled:\n1 2 3 4 5 SQL\u0026gt; select role from session_roles; ROLE ------------------------------ DOC_USER_ROLE Now, can we create a procedure? Remember, the doc_user has not been given any privileges that allow it to do so, however, the enabled role of doc_user_role does have this ability:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SQL\u0026gt; -- Create a \u0026#34;document\u0026#34; via a procedure. SQL\u0026gt; create procedure doc_user_document 2 as 3 begin 4 null; -- This would normally do stuff to create a document. 5 end; 6 / Procedure created. SQL\u0026gt; -- Do the document \u0026#34;creation\u0026#34; by executing said procedure. SQL\u0026gt; exec doc_user_document; PL/SQL procedure successfully completed. SQL\u0026gt; -- Tidy up again. SQL\u0026gt; drop procedure doc_user_document; Procedure dropped. We can be sure that we don\u0026rsquo;t have any of app_owner\u0026rsquo;s other privileges active, by trying to create a table, for example:\n1 2 3 4 5 SQL\u0026gt; create table test(a number); create table test(a number) * ERROR at line 1: ORA-01031: insufficient privileges That looks fine â even though app_owner has create table etc, via the app_owner_role, that role isn\u0026rsquo;t active when doc_user proxies in as app_owner.\nFinding Proxy Users You can find details of proxy users in the PROXY_USERS view:\n1 2 3 4 5 6 7 8 SQL\u0026gt; conn / as sysdba Connected. SQL\u0026gt; select * from proxy_users; PROXY CLIENT\tAUT FLAGS ---------------- --------------- --- ----------------------- DOC_USER APP_OWNER NO PROXY MAY ACTIVATE ROLE This shows you that the doc_user is a proxy user which is permitted to become the client user, app_owner, and that a role may/will be activated at login. It doesn\u0026rsquo;t tell you anything about which role will be activated at login though. To discover that information, you should use either USER_PROXIES or DBA_PROXIES where the ROLE column has the details you need:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SQL\u0026gt; desc dba_proxies Name\tNull? Type ----------------------------------------- -------- ---------------------------- PROXY\tVARCHAR2(30) CLIENT NOT NULL VARCHAR2(30) AUTHENTICATION VARCHAR2(3) AUTHORIZATION_CONSTRAINT\tVARCHAR2(35) ROLE\tVARCHAR2(30) PROXY_AUTHORITY\tVARCHAR2(9) SQL\u0026gt; desc user_proxies Name\tNull? Type ----------------------------------------- -------- ---------------------------- CLIENT NOT NULL VARCHAR2(30) AUTHENTICATION VARCHAR2(3) AUTHORIZATION_CONSTRAINT\tVARCHAR2(35) ROLE\tVARCHAR2(30) As usual, DBA_PROXIES gives you details of all the proxy users in the database while USER_PROXIES only shows the ones that your currently logged in user can become, as per this example:\n1 2 3 4 5 6 7 8 SQL\u0026gt; conn doc_user/doc_password Connected. SQL\u0026gt; select client, role from user_proxies; CLIENT\tROLE ------------------------------ ------------------------------ APP_OWNER\tDOC_USER_ROLE We can see that our current user - doc_user - can proxy connect as the app_user with the role doc_user_role enabled. It cannot proxy login to any other database user account.\nWhat About Other Roles, Privileges and PL/SQL? We have seen above that when a user is granted \u0026ldquo;connect through \u0026hellip; with role\u0026rdquo; then only the privileges granted to that specific role are enabled at proxy login. What about privileges granted directly to the user we are becoming?\n1 2 3 4 5 6 7 8 9 10 11 SQL\u0026gt; conn / as sysdba Connected. SQL\u0026gt; grant create table to app_owner; Grant succeeded. SQL\u0026gt; conn doc_user[app_owner]/doc_password Connected. SQL\u0026gt; create table test(a number); Table created. Whoops! So, privileges granted directly to the app_owner are also enabled when we proxy login to it, even though a role was specified. This could be something to consider when setting up your proxy users.\nWho owns the new table then?\n1 2 3 4 5 SQL\u0026gt; select owner from all_tables where table_name = \u0026#39;TEST\u0026#39;; OWNER ------------------------------ APP_OWNER So, that\u0026rsquo;s another thing to consider, when you have become another user via a proxy login, the other user owns any objects you create. As owner, privileges to INSERT, DELETE, UPDATE and SELECT will also exists on these tables, as well as any other that it owns and to which SELECT etc may not have been granted to the doc_user, in this case.\nBear in mind also, that roles enabled in a session are disabled when PL/SQL is being compiled or executed.\nHave fun!\nTerminology In the discussions above, I\u0026rsquo;ve tended to stay away from the various terminology that Oracle uses in an effort to try and make things a little more clear. However, before I go, here\u0026rsquo;s the information you may require:\nProxy User - is the user that is allowed to become another user. In the above, the proxy user is the doc_user as it is permitted to become the app_owner.\nClient User - is the user that the proxy user is allowed to become. In the above, the client user is app_owner.\nProxy Login - is a special format connect string where the proxy user\u0026rsquo;s name and password is used, as normal, but with the addition of the client user\u0026rsquo;s name in square brackets after the proxy user\u0026rsquo;s name.\nconnect proxy_user[client_user]/proxy_user_password@\u0026hellip;..\n","description":"","id":71,"section":"posts","tags":null,"title":"Oracle Proxy Users - What Are They Used For?","uri":"http://localhost:1313/RantsAndRaves/posts/2013/08/oracle-proxy-users-what-are-they-used-for/"},{"content":"Registration is Suspended - For Now This blog post applied to my Wordpress blog, of which, this is a migrated copy. It no longer applies, but I\u0026rsquo;ve left it here for old time sake! Until further notice, subscribing to this blog, by self-registration, has been suspended. I can and will subscribe you on request - but see below for terms and conditions.\nWhile, I was on holiday for a couple of weeks, over 4,000 spam users \u0026ldquo;registered\u0026rdquo; and I spent a lot of time devoted to cleaning up the mess. In addition I\u0026rsquo;m currently receiving about 100 new users per day - the vast majority are spam users.\nUntil further notice, user self-registration has been suspended so if you wish to register with this site, please email me a request to:\nqdosmsq@ qdosmsq.dunbar-it .co.uk Obviously, you will need to string all three items above to make a valid email address - sorry, another attempt at reducing spam.\nI will need all of the following to register you:\nYour First name. Your Last name. Your desired nickname or username. (Will be displayed) Your email address. Will not be displayed. Note: for the time being, hotmail.com email addresses will not be accepted unless you can give me a pretty spectacular reason why I should. 99.99% of the spam registrations I get come from hotmail.com - so for now, I apologise if you are a valid hotmail user, but the scum have spoiled it for you I\u0026rsquo;m afraid. :-(\n","description":"","id":72,"section":"posts","tags":null,"title":"Spam, Spam, Spam!","uri":"http://localhost:1313/RantsAndRaves/posts/2013/07/spam-spam-spam/"},{"content":"Continuing with the setting up and such like, using Arch Linux on the Raspberry Pi. This is the second blog post on the subject.\nIn the previous posting, we managed to set up our locales, languages, keyboards, and so on. We have internet connection vie a wired Ethernet connection using either a dynamic IP address or a Static one. Time to move on.\nAs before, if you see a command with a \u0026lsquo;\u0026gt;\u0026rsquo; prompt it means that you should be in the root user, or have prefixed the command with sudo. If the prompt is \u0026lsquo;$\u0026rsquo; then that runs as your normal \u0026lsquo;pi\u0026rsquo; user - which we created last time.\nWiFi Access to the Internet Setting up a WiFi access is just as simple as setting up the Wired access, actually, it\u0026rsquo;s easier as there is a utility that helps out.\nMake sure your WiFi dongle is plugged in and then:\n1 \u0026gt; wifi-menu -o The last parameter is a lower case letter o, not a zero.\nAfter a short delay, you will see a message telling you that the system is \u0026ldquo;scanning for networks\u0026rdquo;. Eventually, a list of available networks will be displayed - if yours is not showing, make sure that your wireless router is broadcasting the SSID. Follow the router\u0026rsquo;s instructions if you need to make changes.\nUse the arrow keys to move up and down the list of networks until you have highlighted the desired one. Press the return key to select it.\nIf the network is password protected, you will be prompted for the password. Type it in and press return to accept it. If the network is not protected - you are asking for trouble!\nAfter a pause, you will get the shell prompt back again. You should now be connected to the internet which you can check, as before, by pinging 8.8.8.8 to see if the connection is talking to the internet and then ping google.com to ensure that you have name resolution too. Both are required.\nThe wifi-menu utility has created a new profile for your WiFi link. It is called wlan0-SSID where the SSID part of the name is the name of the network you connected to.\nIt has been configured for a dynamic IP address, and you might not want this.\nTo set up a static IP address, proceed as follows:\n1 2 \u0026gt; cd /etc/netctl \u0026gt; vi wlan-SSID # Where SSID is your network name You will see something like the following:\n1 2 3 4 5 6 7 Description=\u0026#39;Automatically generated profile by wifi-menu\u0026#39; Interface=wlan0 Connection=wireless Security=wpa ESSID=your_network_name IP=dhcp Key=abc1234dhdjfkfkfkgjflsdjjbskbk.bkjbvkbkjbvkjvkbvkohu You need to change it to something like the following:\n1 2 3 4 5 6 7 8 9 10 11 12 13 Description=\u0026#39;Automatically generated profile by wifi-menu\u0026#39; Interface=wlan0 Connection=wireless Security=wpa ESSID=your_network_name # # Changes below here. # IP=**static** Key=abc1234dhdjfkfkfkgjflsdjjbskbk.bkjbvkbkjbvkjvkbvkohu Address=(\u0026#39;192.168.1.36/24\u0026#39;) gateway=(\u0026#39;192.168.1.1\u0026#39;) DNS=(\u0026#39;8.8.8.8\u0026#39; \u0026#39;194.168.4.100\u0026#39; \u0026#39;194.168.8.100\u0026#39;) You will note that the IP address I\u0026rsquo;m using here is exactly the same as the one I set up in the last article. This is fine as I only ever user the WiFi or the wired connection, never both.\nPlus, I don\u0026rsquo;t have the network connection enabled to auto start at boot time.\nOnce the file is saved, you can start the WiFi network by:\n1 \u0026gt; netctl start wlan0-your_network_name If you always want the WiFi network to be started when the Pi boots, then enable it, as follows:\n1 \u0026gt; netctl enable wlan0-your_network_name If you sometimes use a WiFi with a static IP address - say at home, but use a dynamic one when out and about - say at a coding class or Raspberry Jam etc, then you can set up two separate profiles.\nUse the wifi-menu command to set up the dynamic profile, and rename it to wlan0-your_network_name.dynamic then copy it to wlan0-your_network_name.static and add in the bits you need, as above, for a static IP address.\nThe netctl list command will list both and you can start and stop either one at will.\nObviously, this only applies if the two profiles relate to the same network. If you have a home network called home and a work one called work they will already be different files!\nUpdate the System Now that we have a working system, it\u0026rsquo;s time to update the system. The package system on Arch Linux is called pacman and is quite simple to use:\n1 \u0026gt; pacman -Syu --ignore filesystem --ignore ca-certificates The above command first of all updates the current database with any changes since the release of the version you are running, then checks to see what needs updating. After a few seconds, you are presented with a list of packages that are installed, but need updating. Type Y to update the lot. Time for a coffee I suspect!\nNote: When I upgraded my system, I saw two errors. One in package filesystem and the other in ca-certificates. Everything else installed correctly. The above command actually ignores those two packages completely.\nOnce everything else installed, I simply ran the following command and the two ignored packages were upgraded without error:\n1 \u0026gt; pacman -Syu If you see a message, during the upgrading of the system, which mentions that \u0026ldquo;/etc/pacman.d/mirrorlist has been saved as /etc/pacman.d/mirrolist.pacnew\u0026rdquo; then you might need to do a manual correction.\n1 2 3 4 5 6 7 8 \u0026gt; cd /etc/pacman.d \u0026gt; grep -v ^# mirrorlist Server = http://mirror.archlinuxarm.org/armv6h/$repo \u0026gt; grep -v ^# mirrorlist.new Server = http://mirror.archlinuxarm.org/armv6h/$repo As you can see, both files contain a single uncommented line, which is the pacman repository to use on a Raspberry Pi. As they are the same, we can delete the old file and rename the new one with no problems:\n1 2 \u0026gt; rm mirrorlist \u0026gt; mv mirrorlist.pacnew mirrorlist If there were other uncommented lines in either of the files, you would need to make the mirrorlist.pacnew file match the existing mirrorlist file prior to deleting and renaming.\nYou now have an up to date mirrorlist file.\nGraphical User Interface Arch doesn\u0026rsquo;t come with much already installed, just the base system. This keeps bloat to a minimum, and allows you to decide what goes into your own setup. Arch also doesn\u0026rsquo;t come with a GUI system at all, so now, we can install one if we wish.\nThe Linux GUI system comes in three parts:\nX-Org Window Manager Desktop Environment The file /usr/share/X11/xkb/rules/xorg.lst contains details of various settings that you can use for your keyboard model, variant and language (aka layout) and such like.\nThe default for my keyboard model and a possible variant of that model (see the above file for details), is correct, but my layout is wrong. It is set to US while I want GB. (Yes, in the shell it\u0026rsquo;s UK and in X it\u0026rsquo;s GB, go figure!)\nTo fix the problem of a \u0026ldquo;broken\u0026rdquo; layout, we need to create a file called /etc/X11/xorg.conf.d/10-keyboard.conf and add the following to it:\n1 2 3 4 5 Section \u0026#34;InputClass\u0026#34; Identifier \u0026#34;Keyboard Defaults\u0026#34; MatchIsKeyboard\t\u0026#34;yes\u0026#34; Option\t\u0026#34;XkbLayout\u0026#34; \u0026#34;gb\u0026#34; EndSection Obviously, if you want a French keyboard layout, you would set it accordingly. I need \u0026ldquo;gb\u0026rdquo; for mine.\nIf your keyboard model and/or variant isn\u0026rsquo;t correct, you may also add one or both of the following lines after the existing \u0026ldquo;option\u0026rdquo;:\n1 2 Option \u0026#34;XkbModel\u0026#34; \u0026#34;xxxxxx\u0026#34; Option \u0026#34;XkbVariant\u0026#34; \u0026#34;yyyyyy\u0026#34; Where \u0026lsquo;xxxxxx\u0026rsquo; is one of the models, and \u0026lsquo;yyyyyy\u0026rsquo; is one of the variants listed in /usr/share/X11/xkb/rules/xorg.lst. If you now run the startx command and start typing shift-3 etc, you should see that your keyboard is correctly configured. If not, make sure you spelt the various options correctly - and not as I originally did - for example, there is no \u0026lsquo;kbd\u0026rsquo; in \u0026lsquo;Xkblayout\u0026rsquo; etc, it\u0026rsquo;s just \u0026lsquo;kb\u0026rsquo;!\nAnother way to set the keyboard layout and such like is to run the following command in the X session when you have started it, or add it to the file ~/.xinitrc in your particular user:\n1 \u0026gt; setxkbmap -layout \u0026#34;gb\u0026#34; -model \u0026#34;xxxxxx\u0026#34; -variant \u0026#34;yyyyyy\u0026#34; Where the layout, model and variant are as desired. You may omit one or other of these if the default settings are correct.\nNext, we need a Window Manager, and for this experiment in Arch, I\u0026rsquo;m installing Openbox - it\u0026rsquo;s lightweight and \u0026ldquo;fun\u0026rdquo;.\n1 \u0026gt; pacman -S openbox obconf obmenu lxappearance When the above has completed, running startx will have apparently made no difference. We need to tell the X system to run Openbox on startup, so, edit the file .xinitrc in your home directory (/home/root as we are working as the root user at present) and make it thus:\n1 openbox-session Also, in order to have at least something on the openbox context (ie, right-click) menu, do the following as well:\n1 2 3 \u0026gt; mkdir -p ~/.config/openbox/ \u0026gt; cp /etc/xdg/openbox/menu.xml ~/.config/openbox \u0026gt; cp /etc/xdg/openbox/rc.xml ~/.config/openbox When you create a new user on the system, you will need to do a similar exercise for those user accounts, like pi, that you wish to run a GUI on login.\nNow, run the startx command and be amazed! You get, as I did, a totally black screen with an arrow pointer on it. Congratulations, you are now running an openbox window manager.\nRight-click the black area to see the default context menu, select terminals and select xterm from the list. You should now see a new window appearing running a terminal session. You can click on the caption bar and move it around and such like.\nWhen you exit from xterm, you are back at the blank desktop. The context menu allows you to configure open box, if you wish to do so, right-click, select System and Openbox Configuration Manager to play around.\nSelect Log Out and Exit, when prompted, to exit from openbox.\n","description":"","id":73,"section":"posts","tags":null,"title":"Beginner's Guide to Arch Linux on the Raspberry Pi - Part 2","uri":"http://localhost:1313/RantsAndRaves/posts/2013/06/beginners-guide-to-arch-linux-on-the-raspberry-pi-part-2/"},{"content":"The ARCH Linus distro for the Raspberry Pi is not the normal one used by the masses, but the benefits of ARCH are good in that it is a rolling release distro. That means, you never have to reinstall it to be on the latest version.\nThe information that follows assumes that you have installed ARCH from the NOOBS installer. The latest version of ARCH has changed the networking system in use.\nIn addition, where you see a bash prompt that looks like \u0026gt; then the commands that follow whould be executed as the root user. Normal user commands have a $ prompt.\nSorting Out Your Keybord and Locale By default, your keyboard is defined to be a US one. That\u0026rsquo;s fine if you live in the USA, but for us Brits, and anyone else, that\u0026rsquo;s no good.\nYou can check if it\u0026rsquo;s configured correctly, by typing SHIFT+3 to get \u0026lsquo;Â£\u0026rsquo;, you will most likely get \u0026lsquo;#\u0026rsquo; if the US keyboard is in use. For a quick fix for the current session, this will work:\n1 \u0026gt; loadkeys uk If you try typing a \u0026lsquo;Â£\u0026rsquo; again, it should be a \u0026lsquo;Â£\u0026rsquo; this time. if you are in another country, try entering your country code. You might find it useful to have a peek at the contents of the directory /usr/share/kbd/keymaps/i386/ where you will find further directories related to your keyboard layout - qwerty, azerty, dvorak etc. The file names will give you a clue as to the code to use.\nTo make the UK keyboard, or whatever you have chosen to use, stick over a reboot, you need to edit the /etc/vconsole.conf file using vi, or nano or whatever editor you prefer. You should change only the line that begins with \u0026lsquo;KEYMAP\u0026rsquo; as follows:\n1 KEYMAP=uk Don\u0026rsquo;t change any of the other lines. Save the file and exit from the editor.\nMoving on from the keyboard, we now need to fix the system language. By default it is configured to be US English plus UK English. Well, speaking as a Brit, I\u0026rsquo;m not having any of that! ;-)\nFirst let\u0026rsquo;s check:\n1 \u0026gt; grep -v ^# /etc/locale.gen The command above lists all the uncommented lines from the /etc/local.gen file. That shows the locales that have been set up by default. On my Raspberry Pi, I see the following:\n1 2 en_US.UTF-8 UTF-8 en_GB.UTF-8 UTF-8 Edit the /etc/locale.gen file and comment out the en_US line by prefixing it with a \u0026lsquo;#\u0026rsquo;. If you are in France, for example, and wish to change the locale to your own, then comment out both lines showing above, and uncomment the one that reads \u0026ldquo;fr_FR.UTF-8 UTF-8\u0026rsquo;.\nSave the file and exit from the editor. Now generate the locale files as follows:\n1 \u0026gt; locale-gen You should see something like the following:\n1 2 3 Generating locales... en_GB.UTF-8... done Generation complete. Now, set the current session\u0026rsquo;s locale to the one just generated:\n1 \u0026gt; export LANG=en_GB.UTF-8 And finally, to make sure that LANG is correctly set over a reboot, edit the /etc/locale.conf file to the following:\n1 2 LANG=en_GB.UTF-8 LC_COLLATE=C Change the Root Password Now that the keyboard and languages has been sorted out, we really must change the default root password. The current password is hugely insecure.\nType the command passwd and follow the prompts, as shown below:\nEnter new UNIX password:\nRetype new UNIX password:\npasswd: password updated successfully\nThat\u0026rsquo;s that taken care of. Make sure you choose a secure password and I advise staying away from \u0026lsquo;raspberry\u0026rsquo; or \u0026lsquo;root\u0026rsquo;, just in case.\nConfiguring the Network (Wired Ethernet) The latest release of Arch changed the way that networking is used from netcfg to netctl. In order to get a working network, you need to set up a parameter file.\nI found on my own set up, that networking wasn\u0026rsquo;t even enabled after I booted up, which isn\u0026rsquo;t very useful.\nTest if networking is running:\n1 \u0026gt; ping -c 3 8.8.8.8 If you get a response telling you that you received \u0026lsquo;64 bytes from 8.8.8.8 etc\u0026rsquo; then you do already have a network that is running.\nNow, check if you have DNS working:\n1 \u0026gt; ping -c 3 google.com This time, you should get a response telling you that you received \u0026lsquo;64 bytes from (173.194.41.135) etc. If so, your networking is already set up and you need do no more.\nIf neither of the above worked, then, fix the problem:\n1 2 \u0026gt; cd /etc/netctl \u0026gt; cp examples/ethernet-dhcp ./eth0 You should now be able to start the network with:\n1 \u0026gt; netctl start eth0 On my system, that always returns an error, but the networking actually begins working. Go figure!\nThe name of the network profile, eth0 in my case, must match the name of the file you created when you copied the example configuration. You can see which profiles are available by:\n1 \u0026gt; netctl list Which gives me \u0026rsquo;eth0\u0026rsquo; on my system. You might have different or additional ones.\nIf, on the other hand, you need to set your Raspberry Pi up with a static IP address, so that you know where to find it on your network all the time, proceed as follows:\n1 2 \u0026gt; cd /etc/netctl \u0026gt; cp examples/ethernet-static ./eth0 This time we need to edit the configuration file, eth0, and add in the settings we need. Before you proceed, you will need the following:\nStatic IP address - I\u0026rsquo;m using 192.168.1.36. The netmask I\u0026rsquo;m using is 255.255.255.0 which is defined as \u0026lsquo;/24\u0026rsquo; or the first 24 bits. An Interface name - I\u0026rsquo;m using \u0026rsquo;eth0\u0026rsquo;. A default gateway - I use 192.168.1.1. Some DNS names servers - I\u0026rsquo;m using 8.8.8.8 and 194.168.4.100 and 194.168.8.100 - which are Google and Virgin Media\u0026rsquo;s DNS servers. You should use the ones you have been told to use by your own ISP. When you have finished editing the file, it should look like the following, using my settings above:\n1 2 3 4 5 6 7 Description=\u0026#39;Anything you like\u0026#39; Interface=eth0 Connection=ethernet IP=static Address=(\u0026#39;192.168.1.36/24\u0026#39;) Gateway=(\u0026#39;192.168.1.1\u0026#39;) DNS=(\u0026#39;8.8.8.8\u0026#39; \u0026#39;194.168.4.100\u0026#39; \u0026#39;194.168.8.100\u0026#39;) Save the file and exit the editor. Try starting the network:\n1 \u0026gt; netctl start eth0 On my system, that always returns an error, but the networking actually begins working. Go figure!\nIt appears that the ethernet connection is running under control of the ifplug daemon. It will be started whenever a cable is inserted. Hmmm.\nThe file in question that controls what is started by ifplugd is /etc/ifplugd/ifplugd.conf - have a look and see if your \u0026ldquo;eth0\u0026rdquo; is listed in the interfaces list.\nAfter this, I can ping 8.8.8.8 and also ping google.com and in both cases, get a correct response.\nIf your network starts ok, without error, and works, then you can configure it to always start on boot by running the following command:\n1 \u0026gt; netctl enable eth0 If you ever decide to disable it again, it\u0026rsquo;s equally as simple:\n1 \u0026gt; netctl disable eth0 Set Your Timezone So far so good. We need to correctly set the timezone next. If it isn\u0026rsquo;t already done so that is.\nFirst we check the current setting:\n1 \u0026gt; ls -l /etc/localtime If there is no file named /etc/localtime then you need to set it up. If there is a file, but it comes back something like the following:\n1 ... /etc/localtime -\u0026gt; /usr/share/zoneinfo/Europe/London And you don\u0026rsquo;t want to be in this timezone, then you need to change it. If the file already exists but is wrong, delete it:\n1 \u0026gt; rm /etc/localtime Now recreate the file by sym-linking to an existing timezone file:\n1 \u0026gt; ln -s /usr/share/zoneinfo/Australia/Brisbane /etc/localtime The example above, obviously, sets the timezone to be suitable for someone living in the Brisbane, Australia timezone.\nSet the Hostname The Arch installation on your raspberry Pi comes with a strange default hostname - alarmpi. We should change it to something meaningful.\nIn the following, I\u0026rsquo;ve set my hostname to \u0026ldquo;raspberrypi\u0026rdquo; - what else? ;-)\n1 \u0026gt; echo raspberrypi \u0026gt; /etc/hostname The new hostname will take effect after a reboot. If you wish to set it for the current session then:\n1 \u0026gt; hostname raspberrypi Your shell session prompt will still be showing the old hostname, but if you run the command 'su -' then the prompt will change to display the new hostname.\nAdd a New Pi User So far we have been working in the root user, but the time will come when we need to work in a \u0026ldquo;safe\u0026rdquo; user that doesn\u0026rsquo;t have the ability to completely trash the entire system! We shall create a \u0026lsquo;pi\u0026rsquo; user with a password, as follows:\n1 \u0026gt; useradd -m -g users -s /bin/bash -G audio,games,lp,optical,power,scanner,storage,video pi The above command adds the pi user to the system, with its main group being users. It uses the bash shell and is a member of a whole list of subordinate groups. it does not yet have a password, so, as before:\n1 \u0026gt; passwd pi And follow the prompts:\nEnter new UNIX password:\nRetype new UNIX password:\npasswd: password updated successfully\nInstall Sudo The new pi user cannot run root\u0026rsquo;s privileged commands, until we install the sudo option. As root:\n1 \u0026gt; pacman -S sudo You will be prompted to confirm your wish and the sudo command will be installed.\nNow we need to tell sudo that pi is allowed to use it. We will do this by adding the pi user to the sudo group. We still need the /etc/sudoers file updating though, so use the visudo command to edit it.\n1 \u0026gt; visudo Locate the lines that are currently commented out as follows:\n1 2 ## Uncomment to allow members of group sudo to execute any command # %sudo ALL=(ALL) NOPASSWD: ALL Uncomment the second of the above lines, so that it reads as follows:\n1 2 ## Uncomment to allow members of group sudo to execute any command %sudo ALL=(ALL) NOPASSWD: ALL Use the ESC key, followed by :wq to write the file and exit.\nNow add a new group named sudo to the system and add the pi user to it:\n1 2 \u0026gt; groupadd sudo \u0026gt; usermod -a -G sudo pi And we can check if it stuck, with the following command:\n1 \u0026gt; groups pi If all went well, you should see the full list of groups we set up when we created the pi user, and the new sudo group as well:\n1 lp games video audio optical storage scanner power sudo users When the pi user next logs in, or starts a new shell, then it will be allowed to use the sudo command.\nDate and Time If you execute the date command before setting up networking, you will most likely find that the date is way back in the 1970\u0026rsquo;s. Not helpful at all.\nHowever, once you get the internet working, you will find that the same command now returns the correct date and time. This is because there is an NTP daemon running in Arch that connects to a time server and sets your Pi\u0026rsquo;s clock to the correct time.\nAs the Pi doesn\u0026rsquo;t have a battery backed clock as standard, although you can add one, this means that the clock will be wrong each time you boot up until ntp has done its thing. Beware, especially if you are creating source code files that make is used to build - if the dates are out, then unnecessary compilations might be done.\nReboot That\u0026rsquo;s it for this first instalment. A quick reboot and everything should be in order.\nreboot You will note, however, that the system is still in console only mode. Want a GUI? Tune in for the next exciting instalment\u0026hellip;.\nHave fun.\n","description":"","id":74,"section":"posts","tags":null,"title":"Beginner's Guide to Arch Linux on the Raspberry Pi","uri":"http://localhost:1313/RantsAndRaves/posts/2013/06/beginners-guide-to-arch-linux-on-the-raspberry-pi/"},{"content":"Updated 11th January 2015 to document NOOBS 1.3.11.\nNOOBS is the latest user friendly installation system from the Raspberry Pi. It allows you the ability to choose one of 7 (currently) Operating Systems to run on your Pi and a separate data partition to save your possibly shared data. You can pick and choose and change your OS at any time you wish simply by rebooting and holding the SHIFT key down. However, any user data on the SD card will be lost each time.\nNOOBS is \u0026ldquo;New Out Of Box Software\u0026rdquo; by the way. Want to know more? Read on\u0026hellip;\nThere are plenty of places on the web that tell you how to use NOOBS to set up a system, but so far, they all assume one thing, that you are using some flavour of Windows. I\u0026rsquo;m not.\nOne fine example is this video on You Tube - http://www.youtube.com/watch?v=TyFDaMpdh2c\nSo, what follows is a quick and easy guide for the Linux user to get a new SD card ready for use in their Raspberry Pi.\nInitialise Your SD Card The SD card needs to be formatted as FAT32 initially. Most cards come with this formatting, but if you have stolen a card from a camera or similar device, it might be best to properly initialise it. Just in case.\nYou will need a 4GB or larger card. NOOBS adds a recovery partition to the card which uses up some space, and holds the various distros and support files for the \u0026ldquo;recovery\u0026rdquo; process. Basically, the installer lives on the card.\nThe following assumes you have root privileges, so either su - or prefix all commands, unless otherwise advised, with sudo If you see commands prefixed by a \u0026lsquo;#\u0026rsquo; prompt, that means that the command should be run with sudo or as root. If the commands have a \u0026lsquo;$\u0026rsquo; prompt, then you can run those commands as your normal user.\nMy laptop comes with an SD card slot, which is always device /dev/mmcblk0 and partitions on the SD card are named as /dev/mmcblk0pn where the \u0026rsquo;n\u0026rsquo; part starts at 1 and increases as desired.\nFirst we need to find the device, assuming your system is different to mine. The easiest way is to run the following as root:\n1 fdisk -l The parameter is a lower case L for \u0026ldquo;list\u0026rdquo;. This command lists all the mounted and unmounted devices on your computer. Make sure you don\u0026rsquo;t have the SD card in the slot yet though!\nNow, insert the card, and run the command again. If you are prompted to mount the card or open it in a file manager etc, ignore it. We don\u0026rsquo;t want or need the card to be mounted.\nAs mentioned, some Linux distros may auto-mount the SD card when you put it in the slot. If yours does, then unmount it or \u0026ldquo;safely remove\u0026rdquo; it - whichever option you are given.\nOn my Linux Mint laptop, under KDE, I get prompted with a pop-up telling me what partitions are on the newly inserted card, and offering me the choice to open in Dolphin or just to mount the partition. If I ignore the pop-up for 5 seconds or so, it vanishes, and the card remains unmounted.\nIf you use KDE and Dolphin, and if the card is auto-mounted, open Dolphin, find the SD card on the list of devices down the left side, right click it and the top option is to \u0026ldquo;safely remove\u0026rdquo; the device.\nYou might need to \u0026ldquo;safely remove\u0026rdquo; all the partitions on the SD Card, not just one, if all of them have been auto-mounted.\nCompare the two listings and you should see something like the following:\n1 2 3 4 5 6 7 8 9 10 11 ... Disk /dev/mmcblk0: 7948 MB, 7948206080 bytes 4 heads, 16 sectors/track, 242560 cylinders, total 15523840 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000dbfc6 Device Boot Start End Blocks Id System /dev/mmcblk0p1 8192 122879 57344 c W95 FAT32 (LBA) /dev/mmcblk0p2 122880 15523839 7700480 83 Linux In my case, I know the SD card is 8GB in size, so the above difference in before and after leads me to conclude that /dev/mmcblk0 is indeed my SD card.\nThe above listing is from a card that already has been used in my Pi, it has two existing partitions - which I\u0026rsquo;m about to wipe!\nNow that we have a device, we can create the single partition we require:\n1 fdisk /dev/mmcblk0 The above command takes you into the fdisk utility that allows you to manipulate (and destroy!) your SD card. It must be run as root, or using sudo.\nIf you have existing partitions on the device, delete them using the \u0026rsquo;d\u0026rsquo; command. (Use the \u0026lsquo;m\u0026rsquo; command to get a list of available commands if you need to.)\n1 2 Command (m for help): d Selected partition 1 You will be prompted for the partition number each time, so repeat until you have deleted them all. Listing the existing partitions afterwards should show the following:\n1 2 3 4 5 6 7 8 9 10 Command (m for help): p Disk /dev/mmcblk0: 7948 MB, 7948206080 bytes 4 heads, 16 sectors/track, 242560 cylinders, total 15523840 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000 Device Boot Start End Blocks Id System Now we can create a single new partition, the \u0026lsquo;n\u0026rsquo; command does this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Command (m for help): n Partition type: p primary (0 primary, 0 extended, 4 free) e extended Select (default p): p Partition number (1-4, default 1): 1 First sector (2048-15523839, default 2048): Using default value 2048 Last sector, +sectors or +size{K,M,G} (2048-15523839, default 15523839): Using default value 15523839 The above creates a primary partition, numbered 1, with the default start and end sector values. This creates a single partition over the entire SD card, with a bit of space left - as required - at the start.\nUse the \u0026lsquo;p\u0026rsquo; command to check all is well:\n1 2 3 4 5 6 7 8 9 10 11 Command (m for help): p Disk /dev/mmcblk0: 7948 MB, 7948206080 bytes 4 heads, 16 sectors/track, 242560 cylinders, total 15523840 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000 Device Boot Start End Blocks Id System /dev/mmcblk0p1 2048 15523839 7760896 83 Linux So far so good, except the partition type is Linux and not FAT32. We need to change it. The \u0026lsquo;t\u0026rsquo; command is our friend:\n1 2 3 4 5 6 7 8 9 10 11 Command (m for help): t Selected partition 1 Hex code (type L to list codes): l ... b W95 FAT32 51 OnTrack DM6 Aux 9f BSD/OS e4 SpeedStor c W95 FAT32 (LBA) 52 CP/M a0 IBM Thinkpad hi eb BeOS fs ... Hex code (type L to list codes): b Changed system type of partition 1 to b (W95 FAT32) Again, use the \u0026lsquo;p\u0026rsquo; command to check all is well:\n1 2 3 4 5 6 7 8 9 10 11 Command (m for help): p Disk /dev/mmcblk0: 7948 MB, 7948206080 bytes 4 heads, 16 sectors/track, 242560 cylinders, total 15523840 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000 Device Boot Start End Blocks Id System /dev/mmcblk0p1 2048 15523839 7760896 b W95 FAT32 Some people may have problems getting their card to boot in the Raspberry Pi when wiped and re-partitioned like this. If you have that problem, simply make the new partition bootable using the a command.\nI didn\u0026rsquo;t need to do this on my card(s) but I have heard from people who do. Strange that some cards need bootable partitions while others don\u0026rsquo;t.\nWhen NOOBS\u0026rsquo; own installer takes over, it will partition the card to suit itself, and set bootable flags as desired. And it won\u0026rsquo;t be the partition you created that NOOBS makes bootable, it will be a small one, located right at the very end of the card, which holds the NOOBS installer.\nAll that remains now is to write the new partition table to the SD card. The \u0026lsquo;w\u0026rsquo; command does this:\n1 2 3 4 5 6 7 8 9 Command (m for help): w The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: If you have created or modified any DOS 6.x partitions, please see the fdisk manual page for additional information. Syncing disks. Fdisk will exit when we write the partition table.\nSo far, all we have done is to create a new, blank, partition and although we told fdisk that it would be FAT32 we have yet to format it. We need to do this now, however, be careful to format the partition, mmcblk0p1, and not the device, mmcblk0.\n1 2 3 mkfs.vfat /dev/mmcblk0p1 mkfs.vfat 3.0.12 (29 Oct 2011) That\u0026rsquo;s done when you get back to the prompt. Now we can get the software.\nDownload the NOOBS Software Go to http://www.raspberrypi.org/downloads and at the top of the page, you will notice the section entitled \u0026ldquo;New Out Of the Box Software (Recommended)\u0026rdquo; - that\u0026rsquo;s where we need to be.\nClick on either the torrent or direct download links. To save bandwidth on the main servers use the torrent link - if you have a torrent client installed, otherwise, click the direct link.\nWhen prompted, save the file to a good location, I use Downloads/RaspberryPi/distros in my home directory. Your location might possibly be different!\nIt takes about 10 minutes to download, depending on your bandwidth and the busyness of the server. I was getting around 1800 Kb per second download speed, and the file itself is around 760 Mb in size, which is smaller than previous versions but it no longer has 6 different distros inside it!\nWhile it\u0026rsquo;s downloading, have a cuppa!\nWhen it is finished downloading, it is best to verify that what you have is exactly what you should have. The download page shows a SHA1 checksum of, currently, \u0026ldquo;7bc2220f9c6a63cbcc6cafb039a3f0b24055cf23\u0026rdquo; so, open a shell session and verify the download:\n1 2 $ # The following command is \u0026#34;sha(one)sum\u0026#34; and not \u0026#34;sha(ell)sum\u0026#34; :-) $ sha1sum /home/norman/Downloads/RaspberryPi/distros/NOOBS_v1_3_11.zip 7bc2220f9c6a63cbcc6cafb039a3f0b24055cf23 /home/norman/Downloads/RaspberryPi/distros/NOOBS_v1_3_11.zip You don\u0026rsquo;t need to be root to do this, so sudo is not required. Compare the two checksums and if they are both the same, carry on, otherwise, the download is probably corrupt. Download it again.\nCopy to the SD Card Now we need to mount the SD card, so eject it and put it back in again. This time, if you are prompted to open it in a file browser, do so. You can find out where it was mounted by running the mount command, as your normal user, and search from the partition name as above:\n1 $ mount | grep -i mmcblk0p1 You should see something like:\n1 /dev/mmcblk0p1 on /media/F493-CE5C type .... We need to be in a shell session in the above location, so :\n1 $ cd /media/F493-CE5C And now we are ready to extract the software to the SD card. This is a simple unzip command as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 $ unzip /home/norman/Downloads/RaspberryPi/distros/NOOBS_v1_3_11.zip Archive: /home/norman/Downloads/RaspberryPi/distros/NOOBS_v1_3_11.zip inflating: bootcode.bin inflating: BUILD-DATA creating: defaults/ creating: defaults/slides/ inflating: defaults/slides/A.png inflating: INSTRUCTIONS-README.txt creating: os/ creating: os/Raspbian/ inflating: os/Raspbian/os.json inflating: os/Raspbian/root.tar.xz inflating: os/Raspbian/partitions.json inflating: os/Raspbian/partition_setup.sh inflating: os/Raspbian/release_notes.txt creating: os/Raspbian/slides_vga/ inflating: os/Raspbian/slides_vga/C.png inflating: os/Raspbian/slides_vga/A.png inflating: os/Raspbian/slides_vga/D.png inflating: os/Raspbian/slides_vga/G.png inflating: os/Raspbian/slides_vga/B.png inflating: os/Raspbian/slides_vga/E.png inflating: os/Raspbian/slides_vga/F.png extracting: os/Raspbian/Raspbian_-_Boot_to_Scratch.png inflating: os/Raspbian/flavours.json inflating: os/Raspbian/boot.tar.xz extracting: os/Raspbian/Raspbian.png creating: os/Data_Partition/ inflating: os/Data_Partition/os.json inflating: os/Data_Partition/partitions.json extracting: os/Data_Partition/data.tar.xz extracting: os/Data_Partition/Data_Partition.png inflating: recovery.cmdline inflating: recovery.elf extracting: RECOVERY_FILES_DO_NOT_EDIT inflating: recovery.img inflating: recovery.rfs inflating: riscos-boot.bin That\u0026rsquo;s it. The software is installed on the SD card, and is ready to be used. We need to get out of the SD card though, and unmount it before we can use it in the RaspberryPi.\n1 2 $ cd $ sudo umount /dev/mmcblk0p1 Installation of Your Chosen OS Insert the SD card into your RaspberryPi, with the power disconnected and then connect or turn on the power.\nA mini-system will appears and start reorganising the SD card layout. Partitions will be resized and new ones created and so on. You will end up with an SD card with a different set of partitions to the one single partition that we created above, this is ok. NOOBS is doing its thing and getting the SD card ready for installation.\nOnce the partitions have been resized and created as required, you are presented with a menu screen where you may select up to 7 different Operating Systems to install, plus an option to create a separate 512 Mb data partition.\nAt the bottom of the screen, there is an option to set the language to be used for the installation, and you may also select a keyboard layout here - if yours differs from the default.\nCan\u0026rsquo;t see the screen? Press one of the following number keys to assist, it may be best to press a digit key on the top row of the keyboard instead of the numeric keypad, which might not be activated by NOOBS by default.\n\u0026ldquo;1\u0026rdquo; to select output to the HDMI socket, and potentially, from there, to a VGA adaptor. This is the default output for NOOBS. \u0026ldquo;2\u0026rdquo; to select output to \u0026ldquo;Safe HDMI\u0026rdquo;. Choose this if the default HDMI isn\u0026rsquo;t showing anything and you are definitely using an HDMI connected monitor or TV. \u0026ldquo;3\u0026rdquo; to select composite PAL output. This is the \u0026ldquo;normal\u0026rdquo; TV arial socket on the side of your Pi, if your Pi is as old as mine! Later Pis have different sockets for composite output. PAL is used in the UK, Japan and Australia, for example. \u0026ldquo;4\u0026rdquo; to select composite NTSC for the USA TV system. I\u0026rsquo;m told it stands for Never Twice the Same Colour! The Available Operating Systems are:\nRaspbian Boot to Scratch Arch OpenELEC RaspBMC PiDora RiscOS There is an additional option to create a 512 Mb data partition. This could be useful, and If you have sufficient space on the SD card, I would advise selecting this option, especially if you may be installing more than one Operating System.\nNote: The operating systems shown with an SD Card icon are available directly from the NOOBS card. Those with an icon resembling a network cable and network socket are required to be downloaded from the internet, so you will need a working internet connection to install Arch, for example.\nSelect all the desired Operating Systems you wish to install by clicking the small box at the start of each entry in the menu. If a tick or cross appears, it has been selected. Just click it again if you made a mistake, or have changed your mind.\nWhen done selecting Operating Systems, click on the \u0026ldquo;Install\u0026rdquo; button. Take note of the warning that appears next - if you are planning to try another distro, then any existing data on this SD card will be obliterated when the new OS is installed. This isn\u0026rsquo;t a problem on the very first use of the SD card, but might be if you have been using one particular distro for a while but wish to try another.\nIf you need to preserve your data, make a backup of at least your Home directory, and anything in the separate data partition, just to be safe.\nClick the \u0026ldquo;yes\u0026rdquo; button to install the OS. Then go and have another coffee because it takes a wee while to write the OS to the SD card. My own Pi gives me about 1.8 MB per second and there is quite a few Gb to write - depending on how many Operating Systems are being installed.\nThe installation will display a running total of the amount of data written so far, and how much is to be installed in total. Beware that these numbers do not include the Operating Systems that must be downloaded, only those that are on the SD card itself.\nWhen all the installations have completed, a pop-up dialogue will let you know that it\u0026rsquo;s time to reboot. Click the OK button on the dialog that has the title \u0026ldquo;OS(es) Installed\u0026rdquo; and the Pi will reboot. If you installed more than one Operating System, a menu will let you select the one you wish to boot into. Double-click the chosen Operating System and the Pi will boot that one.\nYou might have some initial configuration to do on first use, that depends on the OS in question.\nSelecting a New Operating System on Reboot When the Pi reboots, if you installed more than one Operating System, you will be offered a menu of the installed Operating Systems. As above, double-click the desired one. If you do nothing for 10 seconds, the Operating System you used last will be booted.\nIf you created a separate Data Partition, where is it? A good question. My own install of Raspbian, Arch, OpenElec, Riscos and a Data Partition, resulted in my 16 Gb SD card being formatted with 13 different partitions, set up as follows:\nRiscOS on partitions 5 and 6. Data Partition on partition 7. Raspbian on partitions 8 and 9. Arch Linux on partitions 10 and 11. OpenElec on partitions 12 and 13. So the data partition is partition 7, and we know that it is formatted as an ext4 file system, so where is it mounted when I boot into Rasbian, for example?\nThe mount command is your friend and it can be run from your normal pi user, not just root. Running the command, results in the following:\n1 2 3 $ mount | grep -i p7 /dev/mmcblk0p7 on /media/data type ext4 ... So, it would appear that the data partition is mounted on /media/data. In fact, all the other Operating Systems\u0026rsquo; partitions are automatically mounted by Raspbian, in /media/something_or_other.\nWhen logging in to Arch Linux, the data partition is not automatically mounted, and neither are any of the other Operating Systems\u0026rsquo; partitions. In Arch Linux, you tell it what to do - it makes no assumptions.\nReinstallation and Recovery If you need to reinstall, or recover from a huge foul up, you should first attempt to backup any data that you have been creating. The recovery process will trash it without trace.\nOf course, if the system is so badly trashed, you might not be able to take a backup, but at least try!\nBoot the RaspberryPi from the SD card again, and when prompted on screen (for about 3 seconds) press and hold the SHIFT key. Any one will work.\nThe installation menu will appear again, simply follow the instructions in the Installation section above to recover your OS. Alternatively, choose a new OS to try out - there are 7 to choose from, as listed above.\nHave fun.\n","description":"","id":75,"section":"posts","tags":null,"title":"NOOBS For Raspberry Pi","uri":"http://localhost:1313/RantsAndRaves/posts/2013/06/noobs-for-raspberry-pi/"},{"content":" Astell \u0026amp; Kern AK100: My wife bought me one of these because my old iRiver H340 has begun eating batteries. It\u0026rsquo;s on its third replacement now, plus I have more music than fits the H340\u0026rsquo;s hard drive. I do not buy Apple products, so any iWhatever (other than iRiver) was out of the question. Nothing personal, I just don\u0026rsquo;t like them.\nFinding a suitable replacement music player with decent storage, no hard drive, and Linux compatibility (64 bit essential) was very difficult especially if you are not fond of Apple kit. (Have I mentioned that I\u0026rsquo;m not?)\nThe package contains the device, a nice little fabric pouch, a demo micro SD card (2 Gb) with 5 tracks on it, a screen protector for the touch screen on front and one for the back, and a charging lead. There is no charger - you can use your PC\u0026rsquo;s USB slot, your Kindle charger, your phone charger or whatever you have that is fitted with a standard USB slot to plug the lead into.\nYou must supply your own (decent quality) earbuds with great sound quality - or use the optical output to feed into a quality Hi-Fi system. The headphone socket can also supply line-out levels, so can be fed directly into an amp.\nThe manual is in various languages, and lives on the device itself. Save space by deleting the one(s) you don\u0026rsquo;t need. Or all of them. Don\u0026rsquo;t try and print it off on A4 paper, the pages are tiny - as is the print. Use a PDF reader to view the pages.\nThere is a copy of \u0026ldquo;iRiver Plus 4\u0026rdquo; for Windows on the device. Delete it. (See below!) and download the latest from iRiver.com. If you have a 32 bit Windows computer that is!\nThe supplied software (iRiver Plus 4) will not work on Windows 7 Professional 64 bit. It is 32 bit only. On 64 bit, it will not recognise that the device is plugged in. However, the software is not actually very good, so you are better off just using Windows Explorer and mounting the device as a USB drive. That works.\nThe two micro USB cards are valid up to 32 Gb but with the latest software and careful formatting, 64 Gb micro cards can be used giving a huge 160 Gb of storage. At least, that\u0026rsquo;s what I\u0026rsquo;m told, I haven\u0026rsquo;t got any 64Gb cards to try\u0026hellip;\u0026hellip;yet!\nLinux users, don\u0026rsquo;t worry. The device, and cards is recognised as USB storage. Simply drag and drop files to the (up to three) drives. Works just fine. The manual says that only Windows and Mac (32 bit) are supported by iRiver Plus 4 and even then, only the card in slot 1 is recognised. Go figure.\nThe supplied iRiver Plus 4 software doesn\u0026rsquo;t work under Wine on a 64 bit Linux system. It doesn\u0026rsquo;t work under Windows 64bit, so there\u0026rsquo;s not much chance of it working under Wine on a 64 bit system. 32 bit users may find that it does work under Wine. (Good Luck!)\nUpdating the firmware without iRiver Plus 4 is simple:\nDownload the zip file from http://www.iriver.com/support/download_list.asp. (Release 1.33 is available at the time of writing). (Update 23/02/2023: This link is now a little bit useless!) Unzip it to give you a hex file. Copy the hex file to the root of the main drive of the device - the same location where you see Music, Manual, AlbumArt and System directories. Unmount the USB drive that the device is pretending to be. Windows users, this means \u0026ldquo;safely remove\u0026rdquo;. Don\u0026rsquo;t just pull the plug! It will notice the new hex file and will install it. After a power off and back on, which it does automagically, you are now running the latest release. Make sure you have a fully charged battery though, you do not want to expire half way through the upgrade. That would make an expensive brick!\nSound quality is excellent, I\u0026rsquo;ve heard stuff on my ripped CDs that I\u0026rsquo;ve never heard before. The device plays all sorts of formats, but the best ones are FLAC, WMA and OGG. Take my advice, don\u0026rsquo;t even think about putting MP3 on this device - why would you bother, unless you really need the space savings.\nThe AK100 is solidly built. The chassis is a solid billet of aluminium which gives the device a decent weight. It is certainly not at all plasticy - and at the price, it shouldn\u0026rsquo;t be!\nDon\u0026rsquo;t so as I did - spend ages trying to get the supplied screen protector on the front and the back case protector on the back. There is one already fitted on each side, so the ones you get in the package are spares! However, if you do do as I did, the touch screen works perfectly well through two protectors!\nControls are minimal. On the top is the power switch, input and output sockets.\nThe right side has only the volume knob. When playing, turning that increases or decreases the volume - but, if the screen is on, it shows the volume screen on the device and you can easily adjust the volume using the touch screen. Just drag the pointer where you want it to be.\nThe left side has three tiny buttons. Rewind, play, forward. Each has two functions as per the manual.\nThe bottom has the micro USB socket and the two slots for the micro USB cards under a heavy duty slide to open cover.\nBEWARE! The manual shows only 1 micro card being inserted and shows how to align it. The bottom slot matches the diagram in the manual. However, the top slot requires the card to be inverted (turned upside down!) and put in the other way round. So, in the bottom slot it\u0026rsquo;s as per the manual. In the top slot, turn the card over so that the contacts are showing, and put it in that way. Be very very careful, the slots are very small, and putting the cards in the wrong way round might damage things and prevent you using them.\nAlso, the cards have to be pushed in until they click, and the same to get them out. Use another card to do this, because it\u0026rsquo;s quite fiddly to get them in with fingers like mine. Ladies with decent nails will have no problem!\nBuilding play lists on the device itself is interesting. You have to be playing a track to add it. It\u0026rsquo;s not impossible, just fiddly. The supplied software is apparently better to use, but as it doesn\u0026rsquo;t work on my system, I\u0026rsquo;m a tad up that creek! However, I\u0026rsquo;m a programmer and I\u0026rsquo;ll be writing my own sometime soon.\nActually, I\u0026rsquo;ve determined that the playlist is nothing more than a list of the paths to the files I want in the playlist. So, building one in a text editor is relatively simple. I\u0026rsquo;m still going to build a proper cross platform playlist builder though! I loaded up the internal memory to almost full with a pile of FLACs, and on turning the device back on, the Auto Scan Database Rebuild told me that there wasn\u0026rsquo;t enough room to rescan the tracks to put into the database. This is interesting.\nI moved a pile of directories and files onto one of the SD cards and still got the same error, however, moving a few more directories resolved the problem. I think it\u0026rsquo;s caused by the scan attempting to build the entire database in memory and then write it to the file - or something like that - and it\u0026rsquo;s unable to do it with all the tracks I have loaded.\nUntil I got the scan to work, I couldn\u0026rsquo;t select tracks by Album, Artists, Genre etc, only by Folder. It\u0026rsquo;s not a huge problem, my folders are arranged quite nicely by Artist then Albums beneath that and finally, the tracks.\nI don\u0026rsquo;t do album art, and because I have the screen configured to turn off after 10 seconds anyway, I really can\u0026rsquo;t see the point of Album Art while a track is playing. I wonder if playing the same track on repeat for long enough will cause screen burn after a while? Plus, of course, it hogs valuable space for more music! Mind you, some music formats these days embed the art in the track - so if you were to purchase three tracks from Amazon, for example, you\u0026rsquo;d most likely end up with three duplicates of the same album art, embedded in your MP3s. Wonder if the art can be removed from the tracks. Hmmm.\nThe equaliser function is probably best left turned off. No matter what I did with it, I found the default sound settings better then anything I chose using the equaliser. It updates the sound in real time, when you stop moving your finger on the screen for a second, the sound will change. Keep moving your finger and it will patiently wait for you before changing anything sound wise.\nThe pouch supplied is good for protecting the device from scratches, and such like, but it is quite light and won\u0026rsquo;t take too much stress. It is certainly not suitable for attaching to a belt for portability. This device is better off locked in an inside pocket or similar. I will be on the lookout for a small sturdy case to attach this to my belt.\nTalking of which, if you do want to go portable with this player, and why not, get a headphone that comes out at 90 degrees rather than a straight in plug. It makes better sense, especially if you ever bend over while wearing it around your waist. The sticky out plug gets bent over by your bending body and could knacker the socket and/or headphone plug. My Sony active noise cancelling headphones have a 90 degree plug and work great.\nSo, in summary:\nIt\u0026rsquo;s excellent, a little pricey, but it\u0026rsquo;s not Apple. Quality of build and sound is excellent. It handles optical in and out, if you have it. The software is described on a Hi-Fi forum as a \u0026ldquo;steaming POS\u0026rdquo; - it is. It doesn\u0026rsquo;t work at all on 64 bit Windows systems, but to be fair, the manual does say that it must be 32 bit. It won\u0026rsquo;t run on Linux, but under Linux, you don\u0026rsquo;t actually need it. It doesn\u0026rsquo;t play video, or allow you to read text or word files, it\u0026rsquo;s a quality music player! If you want the other stuff, buy a tablet or a smart phone! Without the software, creating playlists is fiddly. The database rebuild scanner cannot cope with a huge amount of files all at once. I love it! Added 23/02/2023: There are no more software updates. Boo! Hiss! And finally, I used to get around 4-6 hours playback from a fully charged battery on my H340. With this AK100, I\u0026rsquo;m past that already and I still show a full charge! I\u0026rsquo;m still on the very first (5 hour) charge.\nDo you think I\u0026rsquo;ve been going on too long? ;-)\nI did have a problem with the AK100 after a few weeks of almost constant use. The volume knob on the side became intermittent. I contacted my dealer - Sound Fidelity - and they were extremely helpful and had the unit returned for investigation. When it was returned to me, I noticed that the iRiver Service Centre had updated the software to 1.33 and the knob feels a lot more substantial than before in operation.\nI tried the bluetooth function on the device and it hung! I had to do a power off reset (press the ^ button on the side, and the power button on top for 7 seconds) which wiped my settings but not the music or the database. Trying bluetooth again resulted in another hang and another reset. Maybe something in 1.33 is a problem?\nTime to contact iRiver support.\nUpdate 23/02/2023: Well, here we are in 2023. I\u0026rsquo;ve been using this device almost daily since 2013 \u0026ndash; 10 years! The AK100 is still working just fine. I\u0026rsquo;ve worn out another volumn control knob but it still works, just a bit intermittently. Luckily I can adjust the volume on the touch screen.\nBluetooth never worked. I suspect I\u0026rsquo;ve got a borked device, but rather than pay more good money to send it back \u0026ndash; out of warranty of course \u0026ndash; I just bought a dongle to plug into the headphone socket and used that for bluetooth. Works great with my Bose QC32 noise cancelling headphones. A gift/legacy from my late mother, who sadly died in 2015.\nCheers, Norm.\n","description":"","id":76,"section":"posts","tags":null,"title":"Astell \u0026 Kern - AK100 Review","uri":"http://localhost:1313/RantsAndRaves/posts/2013/05/astell-kern-ak100-review/"},{"content":"If, like me, you have suffered from ORA-29902 Error in executing ODCIIndexStart() routine errors where Spatial indexes are involved, the following might help you fix it.\nThe error involved in the following has been extracted from a log file for a system which doesn\u0026rsquo;t use Spatial or Locator itself, but calls out to a separate database which does have Locator installed. This latter database was created using Transportable Tablespaces, exported from 10.2.0.5 Enterprise Edition on HP-UX and imported into 11.2.0.3 Standard Edition on Linux x86-64.\nThere were a number of errors creating a few of the spatial indexes on tables, like the one that follows in the example, that had zero rows in them. Oracle Support assured us that this was not a problem. And we believed them. Sigh!\nThe Problem The following query demonstrates the problem.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 CONNECT CADDBA/password SELECT * FROM TEXT_FORESHORE A WHERE MDSYS.SDO_RELATE( A.GEOM, MDSYS.SDO_GEOMETRY(2003,81989, NULL, MDSYS.SDO_ELEM_INFO_ARRAY(1,1003,3), MDSYS.SDO_ORDINATE_ARRAY(362000,600000,363000,601000)), \u0026#39;MASK=ANYINTERACT QUERYTYPE=WINDOW\u0026#39;) = \u0026#39;TRUE\u0026#39;; * ERROR at line 1: ORA-29902: error in executing ODCIIndexStart() routine ORA-13203: failed to read USER_SDO_GEOM_METADATA view ORA-13203: failed to read USER_SDO_GEOM_METADATA view ORA-06512: at \u0026#34;MDSYS.SDO_INDEX_METHOD_10I\u0026#34;, line 333 Working Out I am definitely not a Spatial guru, but the above doesn\u0026rsquo;t look right to me. Looking at Google, the problem is caused by the Spatial Index being not there, missing, absent. Ok, let\u0026rsquo;s create it.\n1 2 3 4 5 6 7 8 9 CREATE INDEX IDX_T142_GEOM ON TEXT_FORESHORE(GEOM) INDEXTYPE IS MDSYS.SPATIAL_INDEX PARAMETERS(\u0026#39;TABLESPACE=CAD_PRSN_IDX_SPAT SDO_RTR_PCTFREE=0\u0026#39;) NOPARALLEL; CREATE INDEX IDX_T142_GEOM ON TEXT_FORESHORE * ERROR at line 1: ORA-00955: name is already used by an existing object Ok, to me, that says that the index is actually present. DBA_INDEXES shows this to be the case. Apparently, it needs to be dropped and recreated, so I carry on:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 DROP INDEX IDX_T142_GEOM ; Index dropped. CREATE INDEX IDX_T142_GEOM ON TEXT_FORESHORE(GEOM) INDEXTYPE IS MDSYS.SPATIAL_INDEX PARAMETERS(\u0026#39;TABLESPACE=CAD_PRSN_IDX_SPAT SDO_RTR_PCTFREE=0\u0026#39;) NOPARALLEL; CREATE INDEX IDX_T142_GEOM ON TEXT_FORESHORE * ERROR at line 1: ORA-29855: error occurred in the execution of ODCIINDEXCREATE routine ORA-13203: failed to read USER_SDO_GEOM_METADATA view ORA-13203: failed to read USER_SDO_GEOM_METADATA view ORA-06512: at \u0026#34;MDSYS.SDO_INDEX_METHOD_10I\u0026#34;, line 10 Aha. Something different this time. Still not working though. It might be as simple as the CADDBA user not having the correct privileges. Create table and create sequence is required to create a spatial index - whether directly in the schema or as another user creating on in the schema in question. So:\n1 2 3 4 5 6 7 8 9 10 11 12 13 CONNECT / AS SYSDBA SELECT PRIVILEGE FROM DBA_SYS_PRIVS WHERE PRIVILEGE IN (\u0026#39;CREATE TABLE\u0026#39;, \u0026#39;CREATE SEQUENCE\u0026#39; ) AND GRANTEE = \u0026#39;CADDBA\u0026#39;; PRIVILEGE --------------- CREATE SEQUENCE CREATE TABLE 2 rows selected. So that\u0026rsquo;s not the problem this time. Looking into the USER_SDO_GEOM_METADATA view, for this user (every user with Spatial data should have this view) I see nothing for this table_name and column_name:\n1 2 3 4 5 6 7 CONNECT CADDBA/password SELECT * FROM USER_SDO_GEOM_METADATA WHERE TABLE_NAME = \u0026#39;TEXT_FORESHORE\u0026#39; AND COLUMN_NAME = \u0026#39;GEOM\u0026#39;; no rows selected Ok, a clue. I (vaguely) know that in order to create a spatial index, that view needs some data telling it all about the column in question. As this database had been created from a legacy database (which very very rarely gets updated) I was ok to extract the data from legacy and insert it directly here.\nDid I mention, each time the commands fail to create the index in question, they create the index in question? So after each failure, you have to drop it again. Sigh!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 DROP INDEX CADDBA.IDX_T142_GEOM ; Index dropped. INSERT INTO USER_SDO_GEOM_METADATA VALUES (\u0026#39;TEXT_FORESHORE\u0026#39;,\u0026#39;GEOM\u0026#39;, mdsys.SDO_DIM_ARRAY( mdsys.SDO_DIM_ELEMENT(\u0026#39;Easting\u0026#39;, 0, 700000, .0005), mdsys.SDO_DIM_ELEMENT(\u0026#39;Northing\u0026#39;, 0, 1300000, .0005) ), 81989); 1 row created. COMMIT; Commit complete. Now can I create the index?\n1 2 3 4 5 6 CREATE INDEX IDX_T142_GEOM ON TEXT_FORESHORE(GEOM) INDEXTYPE IS MDSYS.SPATIAL_INDEX PARAMETERS(\u0026#39;TABLESPACE=CAD_PRSN_IDX_SPAT SDO_RTR_PCTFREE=0\u0026#39;) NOPARALLEL; Index created. And success at long last. Spatial, I hate you! Does the query work now?\n1 2 3 4 5 6 7 8 9 SELECT * FROM TEXT_FORESHORE A WHERE MDSYS.SDO_RELATE( A.GEOM, MDSYS.SDO_GEOMETRY(2003,81989, NULL, MDSYS.SDO_ELEM_INFO_ARRAY(1,1003,3), MDSYS.SDO_ORDINATE_ARRAY(362000,600000,363000,601000)), \u0026#39;MASK=ANYINTERACT QUERYTYPE=WINDOW\u0026#39;) = \u0026#39;TRUE\u0026#39;; no rows selected After all that work, no rows selected is exactly the correct answer. The table is empty, so I would have been very surprised to see anything other than that response.\nThe Solution The solution to my particular problem was to:\nDrop the so called missing index. Make sure correct data is in USER_SDO_GEOM_METADATA for the table and column in question. Each user with Spatial data will have one of these views, so you need to be in the appropriate user. Create the index again. Test the failing query, and it should work. Cheers.\n","description":"","id":77,"section":"posts","tags":null,"title":"Spatial Indexes and Oracle Errors. How to fix.","uri":"http://localhost:1313/RantsAndRaves/posts/2013/05/spatial-indexes-and-oracle-errors-how-to-fix/"},{"content":"I have a small utility, written in C, that I need to use from time to time. I wrote it back in 2009 when 32 bit systems were all the rage. It reads a certain type of data file and outputs lots of, ahem, interesting information about the contents. When I compiled it on my 64 bit laptop, it produced complete and utter rubbish! How did I fix it?\nThe data file that the utility reads is a Firebird database file which has a number of long fields in various structs. On a 32 bit Linux system, a long is the same size as an int - both are 32 bit.\nThanks to Vlad Khorsun, one of the Firebird Development team (of volunteers) I realised that under a 64 bit system, ints remained as 32 bit while longs got a wee bit bigger at 64 bits - hence my structs were complete nonsense on a 64 bit system.\nThere are two solutions, well, at least two:\nRewrite the code to take account of the world of 32 and 64 bit development systems. This would involve discovering, somehow, if the utility was being built on a 32 or 64 bit system, and changing the definition of a long variable or struct member accordingly. Compile the system and make it believe it\u0026rsquo;s running on a 32 bit system. No code changes required! Being lazy, and because there\u0026rsquo;s only a slim possibility of this code being released into the wild, I chose the latter option. It turned out to be quite simple.\nFirst, I needed to install the gcc multilib support packages on my system to support compiling 32 bit C code:\n1 sudo apt-get install gcc-multilib As I will also need to perform 32 bit C++ code compilations, I also need to install the g++ multilib support packages:\n1 sudo apt-get install g++-multilib Now, when I compile the code, I use the -m32 option to gcc, as follows:\n1 gcc -m32 -o fbdump32 fbdump.c Simple, and it works. However, the correct solution would be to correctly define my structs with proper C/C++ standard data types that guarantee that the width will always be 32 bits and/or determine whether I\u0026rsquo;m compiling on 32 or 64 bit systems. For example:\nData types that guarantee specific widths\n[u]int8_t are 8 bit.\n[u]int16_t are 16 bit.\n[u]int32_t are 32 bit.\n[u]int64_t are 64 bit.\nMaximums and minimums for these data types are defined as:\n[U]INT8_MIN and [U]INT8_MAX\n[U]INT16_MIN and [U]INT16_MAX\n[U]INT32_MIN and [U]INT32_MAX\n[U]INT64_MIN and [U]INT64_MAX\nWhat bitsize am I compiling on\n1 2 3 4 5 6 // Am I compiling on a 64 bit system by any chance? #if __WORDSIZE==64 ... // Do 64 bit stuff here. #else ... // Do 32 bit stuff here. #endif Cheers. Norm.\n","description":"","id":78,"section":"posts","tags":null,"title":"Need to Compile a 32 bit Application on a 64 bit Linux System?","uri":"http://localhost:1313/RantsAndRaves/posts/2013/05/need-to-compile-a-32-bit-application-on-a-64-bit-linux-system/"},{"content":"I have been looking at this far too long, and I\u0026rsquo;m stumped. I resolved a similar problem yesterday on another server. That was down to the ORACLE_HOME setting in listener.ora having a \u0026lsquo;1\u0026rsquo; in it rather than a \u0026lsquo;2\u0026rsquo;. Took ages to spot that.\nAnyway, here the stuff you\u0026rsquo;ll need to know to sort this for me, or suggest stuff. It\u0026rsquo;s a question on Oracle L seeing as there is a lot of evidence to post.\nAs ever, server names etc have been changed to protect the innocent!\nUpdate We have a solution! Scroll to the bottom for details.\nOracle and OS Versions Oracle Database: Standard Edition, 11.2.0.3 64 bit.\nServer: SLES 10 sp 4\nUname -r: 2.6.16.60-0.97.1-smp\nhostname: orcl11gserver\nThe Problem In a word, setting ORACLE_SID and connecting to a user/password works fine. Connecting to user/password@alias gives the following error:\n1 2 3 4 5 6 ERROR: ORA-01034: ORACLE not available ORA-27101: shared memory realm does not exist Linux-x86_64 Error: 2: No such file or directory Process ID: 0 Session ID: 0 Serial number: 0 Database Info I can connect to the database, both as sysdba and as a non-sysdba user provided I don\u0026rsquo;t use the listener:\n1 2 3 $ sqlplus / as sysdba ... Connected. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 SQL\u0026gt; show parameter listener NAME TYPE VALUE ------------------------------------ ----------- ------------------------------ listener_networks string local_listener string remote_listener string SQL\u0026gt; show parameter db_name NAME TYPE VALUE ------------------------------------ ----------- ------------------------------ db_name string orcl11g SQL\u0026gt; show parameter service NAME TYPE VALUE ------------------------------------ ----------- ------------------------------ service_names string orcl11g.world SQL\u0026gt; select * from global_name; GLOBAL_NAME -------------------------------------------------------------------------------- orcl11g.WORLD Listener.ora 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 lsnr_orcl11g = (DESCRIPTION_LIST = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(HOST = orcl11gserver)(PORT = 1521)) ) ) ) SID_LIST_lsnr_orcl11g = (SID_LIST = (SID_DESC = (GLOBAL_DBNAME = orcl11g) (ORACLE_HOME = /opt/oracle/product/11.2.0.3/db_1) (SID_NAME = orcl11g) ) ) DYNAMIC_REGISTRATION_lsnr_orcl11g = off SUBSCRIBE_FOR_NODE_DOWN_EVENT_lsnr_orcl11g=OFF Tnsnames.ora 1 2 3 4 5 6 7 8 9 10 orcl11g,orcl11g.world = (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(Host = orcl11gserver)(Port = 1521)) ) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = orcl11g) ) ) Sqlnet.ora 1 NAMES.DIRECTORY_PATH= (LDAP, TNSNAMES, EZCONNECT, HOSTNAME) Oratab 1 orcl11g:/opt/oracle/product/11.2.0.3/db_1/:N Tnsping 1 $ tnsping orcl11g: 1 2 3 4 5 6 Used parameter files: /opt/oracle/product/11.2.0.3/db_1/network/admin/sqlnet.ora Used TNSNAMES adapter to resolve the alias Attempting to contact (DESCRIPTION = (ADDRESS_LIST = (ADDRESS = (PROTOCOL = TCP)(Host = orcl11gserver)(Port = 1521))) (CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = orcl11g))) OK (0 msec) Listener Status 1 $ lsnrctl status lsnr_orcl11g 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ... TNSLSNR for Linux: Version 11.2.0.3.0 - Production System parameter file is /opt/oracle/product/11.2.0.3/db_1/network/admin/listener.ora Log messages written to /opt/oracle/diag/tnslsnr/orcl11gserver/lsnr_orcl11g/alert/log.xml Listening on: (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=orcl11gserver.testds.ntnl)(PORT=1521))) Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=orcl11gserver)(PORT=1521))) STATUS of the LISTENER ------------------------ Alias lsnr_orcl11g Version TNSLSNR for Linux: Version 11.2.0.3.0 - Production Start Date 10-MAY-2013 16:51:52 Uptime 0 days 0 hr. 0 min. 0 sec Trace Level off Security ON: Local OS Authentication SNMP OFF Listener Parameter File /opt/oracle/product/11.2.0.3/db_1/network/admin/listener.ora Listener Log File /opt/oracle/diag/tnslsnr/orcl11gserver/lsnr_orcl11g/alert/log.xml Listening Endpoints Summary... (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=orcl11gserver.testds.ntnl)(PORT=1521))) Services Summary... Service \u0026#34;orcl11g\u0026#34; has 1 instance(s). Instance \u0026#34;orcl11g\u0026#34;, status UNKNOWN, has 1 handler(s) for this service... The command completed successfully Where is Listener Running From? 1 ps -ef|grep -i ls[n]r_cds 1 oracle 21180 1 0 16:51 ? 00:00:00 /opt/oracle/product/11.2.0.3/db_1/bin/tnslsnr lsnr_orcl11g -inherit Listener Log The listener log shows the connection attempt being made, and established ok with a result code of zero.\n1 2 3 4 5 6 \u0026lt;msg time=\u0026#39;2013-05-10T17:25:56.866+01:00\u0026#39; org_id=\u0026#39;oracle\u0026#39; comp_id=\u0026#39;tnslsnr\u0026#39; type=\u0026#39;UNKNOWN\u0026#39; level=\u0026#39;16\u0026#39; host_id=\u0026#39;orcl11gserver\u0026#39; host_addr=\u0026#39;10.57.18.116\u0026#39;\u0026gt; \u0026lt;txt\u0026gt;10-MAY-2013 17:25:56 * (CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=orcl11g)(CID=(PROGRAM=sqlplus)(HOST=orcl11gserver)(USER=oracle))) * (ADDRESS=(PROTOCOL=tcp)(HOST=10.57.18.116)(PORT=12633)) * establish * orcl11g * 0 \u0026lt;/txt\u0026gt; \u0026lt;/msg\u0026gt; Client Trace Don\u0026rsquo;t worry, I\u0026rsquo;m not about to paste an entire ADMIN level trace here. But looking in one, I saw this extract:\n1 2 3 4 5 6 7 8 nsbasic_brc:type=12, plen=11 nsbasic_brc:what=17, tot =11 nsbasic_brc:packet dump nsbasic_brc:00 0B 00 00 0C 00 00 00 |........| nsbasic_brc:01 00 01 |... | nsbasic_brc:exit: oln=0, dln=1, tot=11, rc=0 nioqrc: found a break marker... nioqrc: Recieve: returning error: 3111 This is sort of interesting, as it seems to indicate I got a break from somewhere or something! I saw this on my other similar problem as well, so it\u0026rsquo;s the same in the two trace files, but I solved the other problem by correcting the Oracle Home in listener.ora. Not this time!\nThe Solution There are many people on oracle-l who took the time to look at the problem, so thanks to all. There are, however, two people to whom I am extremely grateful. They took mere minutes to discover what had been staring me in the face all day, and the winners are:\n@martinberx on Twitter. David Barbour on oracle-l. Both noticed that in /etc/oratab, the Oracle Home path had a trailing slash, while in the listener.ora, it did not. Sheesh!\nThanks to both.\nThe Fix The fix was relatively simple:\nWith the current (wrong) oratab settings in force, shut down the database and the listeners. (The problem affected a number of databases/listeners on this server, not just the one I used in the above example.) Edit oratab to remove the trailing slash. Restart the listeners and databases with the new improved oratab. Test - it all \u0026ldquo;just works\u0026rdquo;. :-)\n","description":"","id":79,"section":"posts","tags":null,"title":"This Listener Problem is Driving Me Mad!","uri":"http://localhost:1313/RantsAndRaves/posts/2013/05/this-listener-problem-is-driving-me-mad/"},{"content":"This post is from 2013 when Internet Explorer was [still] a thing. I rather suspect nobody uses IE any more! Are you forced to use Internet Explorer at work? Are you, like me, forced to use an old, insecure, broken version of IE at work, because it\u0026rsquo;s the Government Standard version? And are you, like me, unable to upload evidence files to My Oracle Support?\nYou need to go to Tools, then Internet Options. On the Security tab, click the Custom Level button. Now find and enable the Include Local Directory Path option. OK your way back out, restart IE, and Robert is your mother\u0026rsquo;s brother. This works fine and has been tested on IE7, IE8, IE9 and possibly (but untested) IE10. Other browsers are not affected, but I\u0026rsquo;m told that Chrome also has problems.\nFirefox, on the other hand, just works.\nHope this helps, unlike me, you may be able to find and click the above options. Our (Government) security policies have that option disabled and I\u0026rsquo;m not allowed to change it. I\u0026rsquo;m not allowed to use Firefox either. Go figure.\nSecurity - sometimes it\u0026rsquo;s there to stop you doing your job.\nCheers.\n","description":"","id":80,"section":"posts","tags":null,"title":"Internet Explorer Won't Upload Files to MOS?","uri":"http://localhost:1313/RantsAndRaves/posts/2013/05/internet-explorer-wont-upload-files-to-mos/"},{"content":"Tired of trying to drop empty datafiles from a tablespace, which you know is empty? Keep getting errors telling you the data file isn\u0026rsquo;t empty? Getting frustrated with the whole thing? Me too. This link has the reason and solution.\n","description":"","id":81,"section":"posts","tags":null,"title":"ORA-03262 While Dropping Empty Datafile","uri":"http://localhost:1313/RantsAndRaves/posts/2013/05/ora-03262-while-dropping-empty-datafile/"},{"content":"Oh hum. An 11.2.0.3 Enterprise Edition production database has statspack taking a regular snapshot under the control of a dbms_scheduler job. For no apparent reason, the snapshot started failing with ORA-01400 Cannot insert NULL into PERFSTAT.STATS$SYSTEM_EVENT.EVENT. This was an interesting one to fix.\nThe following is the investigative process, in brief.\nTest the snapshot process with a manual one - same error.\nGoogle and My Oracle Support aka MOS, were no help whatsoever. I was on my own! Twitter was useful and Noons (@wizofoz2k) suggested a statspack mismatch could cause this error.\nKnowing that I had reinstalled statspack on this database a few weeks ago led me to drop and reinstall statspack and to recreate the jobs required to take regular snapshots and to purge old data. No joy, same problem.\nHunt down the code in V$SQL to see what\u0026rsquo;s going on here. A quick script helped out:\n1 2 3 4 SELECT sql_fulltext FROM v$sql WHERE DBMS_LOB.INSTR(sql_fulltext, \u0026#39;SYSTEM_EVENT\u0026#39;) \u0026lt;\u0026gt; 0 AND DBMS_LOB.INSTR(sql_fulltext, \u0026#39;INSERT\u0026#39;) \u0026lt;\u0026gt; 0; That showed an insert statement, as expected, reading the EVENT column from V$SYSTEM_EVENT â which, given half a brain, makes sense! I didn\u0026rsquo;t have half a brain at the time â as will become obvious!\nAnother quick script showed that there were 5 rows in V$SYSTEM_EVENT that were NULL:\n1 2 3 4 5 6 7 SELECT count(*) FROM sql_fulltext WHERE event IS NULL; COUNT(*) -------- 5 WTH?\nLooking at the EVENT column, showed a huge load of crud and nothing much like a proper Oracle event. Some of the data were:\n1 2 3 4 5 6 rwp err: No dash in error string r removing error %d is [ ][ ] rrupt, error stack is [ ][ ] down and process is starting up indicate must count [ ][ ] ... WTH? (The sequel!)\nThe next stop - I did say I didn\u0026rsquo;t have half a brain didn\u0026rsquo;t I - was the alert log, where I should have been looking in the first place! Bingo!:\n1 2 WARNING: Oracle executable binary mismatch detected. Binary of new process does not match binary which started instance. And there we have it. Noons was correct in as much as the version of statspack in use - 11.2.0.3 EE - didn\u0026rsquo;t match the running database binary which was 11.2.0.3 SE. Somehow, someone (no, not me - but thanks for asking!) had managed to start the database running on SE rather than EE.\nI\u0026rsquo;m thinking that this could have been done when an SE environment was enabled in a session, and someone simply did an export ORACLE_SID=whatever not realising that EE was required. This posting might help in that case! :-)\nAfter shutting down the database, making sure that the correct environment was set, a restart of the database got rid of the messages in the alert log, and a snapshot was successfully executed.\nSo, an interesting challenge that could have been resolved earlier if I\u0026rsquo;d gone straight to the alert log rather than dicking about thinking I knew that it must have been the reinstall I did previously! That\u0026rsquo;ll teach me to think then!\nAnd by the way, I know (oops) from previous experience, that the snapshot code in V$SQL will have table names and commands in upper case, which is why I used upper case tests for INSERT and SYSTEM_EVENT in the script above. If I wasn\u0026rsquo;t so sure, I\u0026rsquo;d have done this instead:\n1 2 3 4 SELECT sql_fulltext FROM v$sql WHERE DBMS_LOB.INSTR(upper(sql_fulltext), \u0026#39;SYSTEM_EVENT\u0026#39;) \u0026lt;\u0026gt; 0 AND DBMS_LOB.INSTR(upper(sql_fulltext), \u0026#39;INSERT\u0026#39;) \u0026lt;\u0026gt; 0; Cheers.\n","description":"","id":82,"section":"posts","tags":null,"title":"Statspack Snapshot Fails ORA-01400 Cannot Insert NULL ...","uri":"http://localhost:1313/RantsAndRaves/posts/2013/05/statspack-snapshot-fails-ora-01400-cannot-insert-null/"},{"content":"Antonio Enrico Martini was cremated on 19th April 2013. He had been a chef for many years and one of his friends read this Address to a chef when Enrico retired in 1987. The version below, was amended from the 1987 version. Both are based on Address to a Pudding by Rabbie Burns.\nAddress To A Chef Fair fa\u0026rsquo; your honest sonsie face,\nGreat Enrico o\u0026rsquo; th\u0026rsquo; Italian Race,\nYe cam\u0026rsquo; amang us lang years since, an\u0026rsquo; did enthrall,\nWi\u0026rsquo; savoury dish o\u0026rsquo; string an\u0026rsquo; mince, cried Spaggi Bol!\nMony\u0026rsquo;s the nicht we\u0026rsquo;ve supped your fare\nChosen frae menu wi lovin\u0026rsquo; care;\nAft times tempted tae hae some mair, o\u0026rsquo; Tagliatelli.\nThe end result, ye can be share â an ower-full belly!\nAye, greed\u0026rsquo;s a tempter, there\u0026rsquo;s nae doot,\nWhen faced wi\u0026rsquo; grub that\u0026rsquo;s unco\u0026rsquo; good,\nTae get stuck in tae favoured food, like Canelloni,\nAn\u0026rsquo; then consume, in reckless mood, Zabaglione!\nThat menu\u0026rsquo;s fu\u0026rsquo; o\u0026rsquo; names exotto,\nLike Lasagne and Risotto,\nTry them a\u0026rsquo;! - That\u0026rsquo;s been my motto â have nae fear,\nThis chef, say I, (no voce sotto), has nae peer.\nNow every king must hae his queen,\nWha better then than Bonnie Jean!\nHer work, it seems, is nivver deen, an\u0026rsquo; man she\u0026rsquo;s fleet,\nThe quickest thing ye\u0026rsquo;ve ivver seen on jist twa feet!\nWell, that wis then â his course is run,\nThe time has come; it waits for none,\nSae fare-thee-well tae San Remo\u0026rsquo;s son, oor heart\u0026rsquo;s wi\u0026rsquo; Jeannie,\nRest aye in peace the only one full-proof Martini.\nAs Amended 19th April 2013\nRest in peace Enrico. Your Lasagne will always be the best.\n","description":"","id":83,"section":"posts","tags":null,"title":"My Step-Father's Cremation","uri":"http://localhost:1313/RantsAndRaves/posts/2013/04/my-step-fathers-cremation/"},{"content":"Recently, I saw a mention of an interview question for SQL developers. It was something along the lines of:\nThere is a table with a sex column. It has been discovered that the values are swapped around and need to be corrected. How would you swap all \u0026lsquo;M\u0026rsquo; values to \u0026lsquo;F\u0026rsquo; and all \u0026lsquo;F\u0026rsquo; values to \u0026lsquo;M\u0026rsquo;, while leaving the other values untouched, in one single SQL statement and without requiring the use of any temporary variables. So, how would you do it? Here\u0026rsquo;s my example.\nFirst, the before look:\n1 2 3 4 5 6 7 8 9 10 11 12 select sex, forename from bedrock; S FORENAME - -------- M Wilma Dino M Betty F Fred F Barney U Pebbles 6 rows selected. I think we can safely say that the sex column is somewhat back to front. My solution was to use the CASE statement in an SQL UPDATE command:\n1 2 3 4 5 6 update bedrock set sex = case sex when \u0026#39;F\u0026#39; then \u0026#39;M\u0026#39; when \u0026#39;M\u0026#39; then \u0026#39;F\u0026#39; end; Finally, the after look:\n1 2 3 4 5 6 7 8 9 10 11 12 select sex, forename from bedrock; S FORENAME - -------- F Wilma Dino F Betty M Fred M Barney U Pebbles 6 rows selected. It appears to be safe to commit!\nUsing the CASE statement is useful. In the old days, we would need something like the following:\n1 2 3 Update bedrock set sex = \u0026#39;T\u0026#39; where sex = \u0026#39;M\u0026#39;; Update bedrock set sex = \u0026#39;M\u0026#39; where sex = \u0026#39;F\u0026#39;; Update bedrock set sex = \u0026#39;F\u0026#39; where sex = \u0026#39;T\u0026#39;; For huge tables, that could have taken a while, and it\u0026rsquo;s highly unlikely that a sex column would be indexed, so three full table scans would have been the order of the day.\nUsing CASE we get away with a single scan, plus, the statement short circuits when it hits the first matching WHEN clauses. So, if the first sex value was \u0026lsquo;F\u0026rsquo; it would be changed to \u0026lsquo;M\u0026rsquo; however it would then stop checking and would not change the newly set \u0026lsquo;M\u0026rsquo; back to an \u0026lsquo;F\u0026rsquo;.\n","description":"","id":84,"section":"posts","tags":null,"title":"Swap 2 Values, in SQL, Without Using a Temporary Variable","uri":"http://localhost:1313/RantsAndRaves/posts/2013/04/swap-2-values-in-sql-without-using-a-temporary-variable/"},{"content":"http://jarneil.wordpress.com/2013/04/23/recovering-from-rm-rf-on-a-datafile/ has all you need to know to help avert a total disaster. Tanel Poder, who knows these things, advises that the file should be quiesced or made read only before attempting the recovery.\n","description":"","id":85,"section":"posts","tags":null,"title":"Ever deleted a Data File From a Running Database?","uri":"http://localhost:1313/RantsAndRaves/posts/2013/04/ever-deleted-a-data-file-from-a-running-database/"},{"content":"Firefox starts off ok, but soon starts running slower and slower, until it eventually starts to time out on connecting to some pages. The error messages is \u0026ldquo;the server took too long to respond\u0026rdquo; however, Firefox might be telling porkies.\nIf you attempt to access the same URL in Opera, Chrome or, if you must, Internet Explorer, you may find that it is responding quite happily and speedily, while another try in Firefox takes ages to connect or fail again.\nYou need to sort out a couple of options:\nEdit-\u0026gt;Preferences (on Linux) or Tools-\u0026gt;Options (I think, on Windows) Click Advanced Click General tab Uncheck Use hardware acceleration when available Click Network tab Click Settings button beside Configure how Firefox connects to the internet Check No proxy but be aware that this option might be restricted if you use Firefox at work. Click OK button Click Close button You might need to restart Firefox after this, I didn\u0026rsquo;t on Linux, but try a link without restarting first and see what happens. On my system, it works perfectly quickly again.\n","description":"","id":86,"section":"posts","tags":null,"title":"Firefox Running Extremely Slowly?","uri":"http://localhost:1313/RantsAndRaves/posts/2013/04/firefox-running-extremely-slowly/"},{"content":"Ever see this error? I have, just today. An interesting one to debug. I got there in the end though.\nThe database is running on a two node VERITAS cluster. To protect the innocent, I shall refer to these as node_04 and node_05, for that is similar to their real names! The database is not RAC, it runs on one node or the other, but never both. There is one instance and one database.\nThe DNS and/or whatever they use to ensure that traffic goes to the correct node (I have a good grasp of the technicalities, haven\u0026rsquo;t I?) uses a host name of sgxxxx where xxxx is the database name. Tnsnames.ora and listener.ora use this special hostname in the configuration for the database alias resolution, and the listener.\nThe database is running on node_04 right now, but can be forced to run on node_05 due to a failure or a manual fail over. The listener follows the database and runs on the same node - wherever that happens to be. It is not possible for the listener to run on node_04 with the database on node_05, and vice versa.\nI was attempting to get a status from lsnrctl for the listener:\n1 Node_04\u0026gt; lsnrctl status lsnr_xxxx 1 2 3 4 5 6 LSNRCTL for Linux: Version 11.2.0.2.0 - Production on 19-MAR-2014 10:11:03 Copyright (c) 1991, 2010, Oracle. All rights reserved. Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=sgxxxx)(PORT=1603))) TNS-01189: The listener could not authenticate the user It failed, as shown above. Interesting because it has been working \u0026ldquo;forever\u0026rdquo;.\nRunning a tnsping on the database had no problems. It returned with a zero millisecond response time. Pretty quick. So we know the listener is up and running, and responding, we just can\u0026rsquo;t get a status.\n1 Node_04\u0026gt; tnsping xxxx 1 2 3 4 5 6 TNS Ping Utility for Linux: Version 11.2.0.2.0 - Production on 19-MAR-2014 10:12:23 Copyright (c) 1997, 2010, Oracle. All rights reserved. Attempting to contact (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = sgxxxx)(PORT = 1603)(CONNECT_DATA = (SERVER = DEDICATED) (SERVICE_NAME = xxxx))) OK (0 msec) Next step was to check the listener.ora file to see if there was a password set for the listener - there was not. Hmmm.\nInsert a delay here while various obvious stuff was checked, and found to be ok. Everything works unless we use the listener to access the database. At that point, the TNS error occurs again.\nEventually, I wondered what the sgxxx \u0026ldquo;special\u0026rdquo; hostname resolved to:\n1 2 3 4 5 6 7 Node_04\u0026gt; ssh oracle@sgxxxx password: ... Node_05\u0026gt; exit Node_04\u0026gt; Did you notice? I did, straight away. The special host name resolves to the other node in the VERITAS cluster. It should be resolving to the same node as the database - node_04.\nThe solution is to get the network guys and gals to fix it. It\u0026rsquo;s a cluster of two nodes and should work accordingly. Sgxxxx needs to resolve to the cluster node where the database is running. It wasn\u0026rsquo;t.\nNasty, but fun to debug. :-)\n","description":"","id":87,"section":"posts","tags":null,"title":"TNS-01189: The listener could not authenticate the user","uri":"http://localhost:1313/RantsAndRaves/posts/2013/03/tns-01189-the-listener-could-not-authenticate-the-user/"},{"content":"Ever needed to obtain the serial number (or other details) for a remote server? Couldn\u0026rsquo;t be bothered to walk/run/drive/fly all the way there just to read a sticky label on the back or bottom of said server? Read on then.\nThe command you want to run, as root, is dmidecode. For example, to get the make and model and serial number of a server, do this:\n1 dmidecode -t system The result will be similar to:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # dmidecode 2.11 SMBIOS 2.5 present. Handle 0x0002, DMI type 1, 27 bytes System Information Manufacturer: Dell Inc. Product Name: Vostro 1720 Version: Null Serial Number: 996C4L1 UUID: Not Settable Wake-up Type: Power Switch SKU Number: Null Family: Vostro Handle 0x000F, DMI type 12, 5 bytes System Configuration Options Option 1: Jumper settings can be described here. Handle 0x0018, DMI type 32, 20 bytes System Boot Information Status: No errors detected Other options for the -t parameter are:\nbios - tells you all about your bios. system - tells you about the system hardware. baseboard - all about the mother board. chassis - all you need to know about the \u0026ldquo;box\u0026rdquo; the system is made up of. processor - fairly obvious. memory - again, fairly obvious. cache - information about your CPU cache. connector - what sockets are present on the computer. USB, firewire, ethernet etc. slot - appears to be the bus information, and voltages present, supplied etc. There\u0026rsquo;s brief help available:\n1 dmidecode --help 1 2 3 4 5 6 7 8 9 10 11 Usage: dmidecode \\[OPTIONS\\] Options are: -d, --dev-mem FILE Read memory from device FILE (default: /dev/mem) -h, --help Display this help text and exit -q, --quiet Less verbose output -s, --string KEYWORD Only display the value of the given DMI string -t, --type TYPE Only display the entries of given type -u, --dump Do not decode the entries --dump-bin FILE Dump the DMI data to a binary file --from-dump FILE Read the DMI data from a binary file -V, --version Display the version and exit However, to find out the different types you can supply, you need to supply an erroneous type:\n1 dmidecode -t left_leg 1 2 3 4 5 6 7 8 9 10 11 Invalid type keyword: left_leg Valid type keywords are: bios system baseboard chassis processor memory cache connector slot I\u0026rsquo;ve just used the command to obtain information about a server located 150 odd miles away from my comfy chair, running in an unattended site. That saved me a bit of time!\nHave fun.\n","description":"","id":88,"section":"posts","tags":null,"title":"Linux Command to Retrieve Hardware Serial Numbers etc","uri":"http://localhost:1313/RantsAndRaves/posts/2013/03/linux-command-to-retrieve-hardware-serial-numbers-etc/"},{"content":"A quickie! How do you set the correct Oracle environment in scripts? Do you hard code? You\u0026rsquo;d better not.\nI\u0026rsquo;ve lost count of the times I\u0026rsquo;ve ended up with, for example, a 10g database running with bits of the 9i software hanging around. It leads to monumental problems that can be hard to track down.\nMoral: Do not hard code Oracle environment details.\nThis is what I usually do:\n1 2 3 4 5 6 7 8 #!/usr/bin/env bash export ORAENV_ASK=NO export ORACLE_SID=my_sid . oraenv export ORAENV_ASK=YES # Rest of bash script goes here.... Reagrdless of how many times the database version gets an update, you will still always have the correct Oracle software on the path and in the environment when the script runs.\nToday, I learned a new way to do the above, with much less typing. You can see an example here on Gokhan Atil\u0026rsquo;s Data Blog. (Update 23/02/2023: The linked page is no more, it has ceased to be!)\n1 2 3 4 5 6 7 #!/usr/bin/env bash . oraenv \u0026lt;\u0026lt;EOF my_sid EOF # Rest of bash script goes here.... All those years of Oracle and I never figured out that I could user a \u0026ldquo;here\u0026rdquo; document. Thanks Gokhan.\n","description":"","id":89,"section":"posts","tags":null,"title":"Setting Oracle Environment in Scripts","uri":"http://localhost:1313/RantsAndRaves/posts/2013/03/setting-oracle-environment-in-scripts/"},{"content":"Yesterday, Saturday 2nd March 2013, I went to check on my two bee hives. They were fine and healthy with plenty feed stock when I checked them recently.\nYesterday, nothing! Every single bee was dead. There was plenty food, so it seems that the recent damp, foggy, nasty, cold weather must have either killed or severely weakened them.\nI have no bees. :-(\nThis is all that remains, two dead hives. I need to clean and sterilise before I even attempt to house any new bees here. You can never be too careful.\nI have checked the hives and bodies over, but I saw no signs of disease.\n","description":"","id":90,"section":"posts","tags":null,"title":"Bee Tragedy","uri":"http://localhost:1313/RantsAndRaves/posts/2013/03/bee-tragedy-2/"},{"content":"I took this photo in Scotland, along the Crinan Canal, back in March 2009. Anyone know what it is? Click the image for a full screen version. (Warning - it\u0026rsquo;s 4000px by 3000px and 5.6 Mb file-size. The original file-size is even larger - it\u0026rsquo;s almost 12Mb!)\nUpdate: it\u0026rsquo;s a Southern Hawker. Thanks Howard!\nScottish Dragonfly. Crinan Canal, March 2009.: ","description":"","id":91,"section":"posts","tags":null,"title":"Here be Dragons!","uri":"http://localhost:1313/RantsAndRaves/posts/2013/02/here-be-dragons/"},{"content":"In the old days of exp/imp doing a Transportable Tablespace export/import was relatively simple - unless you had Spatial data, in which case, it wasn\u0026rsquo;t. Then we got hold of expdp/impdp and it became \u0026ldquo;different\u0026rdquo;. It now seems that in order to do a Transportable Tablespace import with impdp, you don\u0026rsquo;t tell it to do one! Confused?\nIn the old days, you told both exp and imp which tablespaces you were transporting using the TRANSPORT_TABLESPACE and TABLESPACES parameters to exp, and the same with imp. Life was good and symmetrical back then!\nWith expdp, we have a similar parameter whereby we simply list the tablespaces we wish to transport in the TRANSPORT_TABLESPACES (note, plural) parameter.\nYou would think that a similar arrangement would exist with impdp wouldn\u0026rsquo;t you? Well it does. Sort of!\nThere is indeed a TRANSPORT_TABLESPACES (note, plural) parameter on impdp but, if you use it, you must also specify a database link name in the NETWORK_LINK parameter and that link must exist in the importing database and it must point back to the exporting database. There is no intermediate dump file, the metadata is unloaded from the source database over the database link. Smart?\nNot quite. Imagine that you have a production database to export and then import into a QA database, for example. However, the two are on separate networks, and there\u0026rsquo;s an air gap between the two. You cannot set up a database link from the QA database to the Production one to get at the metadata for the Transportable Tablespace import.\nIf you try, without a database link in place and working, impdp will simply barf.\nThe solution is hidden away in a note attached to the TRANSPORT_TABLESPACES in the Utilities manual, impdp section, in the locked filing cabinet, in the office in the basement, behind the leopard guarding the stairs, down the dark corridor etc. (Douglas Adams.)\nWhat you have to do is not use the TRANSPORT_TABLESPACES parameter as you would imagine, instead, you list the transported data files using the TRANSPORT_DATAFILES and avoid the TRANSPORT_TABLESPACES like the plague! Goodbye symmetry, it was nice of you to drop in!\nSo far, so bad. However, my original problem with deferred segment creation still exists even with impdp instead of imp.\n1 $ impdp directory=... dumpfile=... logfile=... transport_datafiles=file1.dbf ... 1 2 3 4 5 ... Processing object type TRANSPORTABLE_EXPORT/TABLE ORA-39083: Object type TABLE:\u0026#34;NORMAN\u0026#34;.\u0026#34;TEST_TABLE\u0026#34; failed to create with error: ORA-01647: tablespace \u0026#39;TEST_TABLESPACE\u0026#39; is read-only, cannot allocate space in it ... Of course it\u0026rsquo;s read-only, it doesn\u0026rsquo;t exist in the database because impdp is supposed to create it, load up the metadata and connect that with the data files etc. Then I have to login and make the tablespace read write when I\u0026rsquo;m done.\nSigh!\n","description":"","id":92,"section":"posts","tags":null,"title":"Transportable Tablespace Migrations with Expdp/Impdp","uri":"http://localhost:1313/RantsAndRaves/posts/2013/02/transportable-tablespace-migrations-with-expdpimpdp/"},{"content":"Rsync is great for making sure that a destination directory is synchronised with a source directory. However, do you add a slash to the source and/or destination directory names, or do you not?\nThe answer is, it depends. Without a slash on the source directory means copy both the source directory, and the contents (recursively if specified) to the destination directory while adding a trailing slash means only copy the contents of the source directory, recursively if specified, to the destination. Easy?\nIf we take the following as the source directory:\n1 $ tree testing 1 2 3 4 5 6 7 testing |-- another |Â +-- wilma +-- betty +-- fred +-- nested +-- barney The destination is an empty directory named test_backup.\nNo Slashes The first test has no slashes on any of the directories.\n1 2 3 $ rm -r test_backup/* $ rsync --archive --recursive testing test_backup $ tree test_backup 1 2 3 4 5 6 7 8 test_backup +--testing +-- another | +-- wilma +-- betty +-- fred +-- nested +-- barney You can see that the whole hierarchy of the testing directory has been recreated within the destination directory.\nSlash on Destination Only The second test, after clearing out the destination directory, adds a slash to the end of the destination directory.\n1 2 3 $ rm -r test_backup/* $ rsync --archive --recursive testing test_backup/ $ tree test_backup 1 2 3 4 5 6 7 8 test_backup +--testing +-- another | +-- wilma +-- betty +-- fred +-- nested +-- barney So, there\u0026rsquo;s no difference there. A slash on the destination directory appears to have no effect.\nSlash on Source Only 1 2 3 $ rm -r test_backup/* $ rsync --archive --recursive testing/ test_backup $ tree test_backup 1 2 3 4 5 6 7 test_backup +-- another | +-- wilma +-- betty +-- fred +-- nested +-- barney This is different. The contents of the source directory have been duplicated into the destination directory.\nSlashes on Both 1 2 3 $ rm -r test_backup/* $ rsync --archive --recursive testing/ test_backup/ $ tree test_backup 1 2 3 4 5 6 7 test_backup +-- another | +-- wilma +-- betty +-- fred +-- nested +-- barney And this one again shows that having a slash on the destination directory has no effect.\n","description":"","id":93,"section":"posts","tags":null,"title":"Rsync - To Slash or Not To Slash?","uri":"http://localhost:1313/RantsAndRaves/posts/2013/02/rsync-to-slash-or-not-to-slash/"},{"content":"I have a network attached 1 Tb hard drive. It\u0026rsquo;s a white book, Western Digital \u0026ldquo;My Book World\u0026rdquo; device. I use it for backups. I needed to set it up to allow the backup scripts ssh access without a password. Here\u0026rsquo;s how I did it.\nThe web interface was used in the normal way to create a user named my_backups.\nA share was set up automatically for this new user. The share name is the same as the user name - my_backups.\nusing advanced mode-\u0026gt;network, I ensured that NFS was enabled and that only devices on my internal network were able to access the shares. This is not necesary for the backups though, but it allows me to NFS mount the share as required.\nI logged onto the WD device using ssh, as root. I needed to edit the /etc/passwd file to move the home directory for my new user from /shares to /shares/my_backups otherwise, setttng up ssh affects (potentially) all users. And besides, I want my users to have their own home, not a shared one.\nThe default permissions on the my_backups share is to allow group writes. Ssh will not work if this is the case, so that has to go, as root:\n1 chmod g-w /shares/my_backups The ownership of the my_backups share is also dubious. So another quick change is called for, as root:\n1 chown my_backups /shares/my_backups Now I can login using ssh as the my_backups user, so logout as root, and back in again as my_backups.\nNow set up all the stuff (technical term!) required by ssh:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 cd pwd /shares/my_backups mkdir .ssh backups chmod 600 .ssh ls -al ... drw------- 2 my_backu jewab 6 Feb 11 15:56 .ssh drwx------ 2 my_backu jewab 6 Feb 11 16:27 backups cd .ssh touch authorized_keys chmod 700 authorized_keys ls -l -rwx------ 1 my_backu jewab 0 Feb 11 15:58 authorized_keys exit Now, I copy my ssh public key to the new authorized_keys file, from my laptop, which is the device I\u0026rsquo;m backing up up to the shared drive:\n1 2 3 $ cd ~/.ssh $ cat id_dsa.pub | ssh root@wd \u0026#39;cat \u0026gt;\u0026gt; ~my_backups/.ssh/authorized_keys\u0026#39; root@wd\u0026#39;s password: ****** You will notice that I\u0026rsquo;m connecting as root to do this. For some reason, on the device, attempting to do this as my_backups gives me an error sh: cannot create .ssh/authorized_keys: Permission denied which is strange, as the file exists and is correctly owned by the my_backups user. However, there are a few foibles in the OS that is running on the device, so logging in as root works!\nNow I can login as my_backups without a password, from my laptop. And more to the point, my backup script can do it too.\n1 2 3 4 ssh my_backups@wd pwd /shares/my_backups Talking of backup scripts, it\u0026rsquo;s nothing more than an rsync command. The rsync option --archive cannot be used as it won\u0026rsquo;t let the --times part of it run and lots of errors are scrolled up the screen.\nTo get around this, I\u0026rsquo;m using --perms --delete --update --verbose --links --progress --recursive instead. The destination on the rsync command is specified as my_backups@wd:backups which does the job nicely!\nAhem, of course, the first time I did this, I neglected to create the backups directory didn\u0026rsquo;t I? So what happened on the first actual run? Rsync happily --deleted the .ssh directory holding my keys, so further testing didn\u0026rsquo;t work and requested the password that I had worked so hard to get rid of!\n","description":"","id":94,"section":"posts","tags":null,"title":"Setting up SSH on Western Digital My Book World Devices","uri":"http://localhost:1313/RantsAndRaves/posts/2013/02/setting-up-ssh-on-western-digital-my-book-world-devices/"},{"content":"Accessing a web server or an email server, directly from within a database, used to be quite simple. However, it all stops working at 11g. Why is that and what can be done to fix it?\nIntroduction Prior to Oracle 11g, any user in the database wishing to use the various network packages - UTL_HTTP, UTL_SMTP, UTL_TCP, UTL_MAIL etc, and their predecessors, only requires to be granted EXECUTE privileges on the appropriate package(s).\nFrom 11g onwards this is no longer the case. Execute privilege is still required, however, further fine grained access control has been added to the database to restrict the networks reachable, port ranges allowed, and even down to the actual start and end dates and times that the access will be allowed.\nThis fine grained control is achieved via ACLs. (Access Control Lists.) and the XML Database (XDB) product which must be installed on the database.\nInformation Requirements If an application wishes to use network resources, then the following information must be obtained, in advance, of the application being set up in the database. It is advised that it be documented in the database and/or application documentation\nEmail For an application connecting directly to a mail server, not via sendmail, or equivalent:\nHostname of the email server, or IP address of same. Port number(s) in use for the email server. Database username (ie schema names) of all schemas that will be executing the code that sends emails. If required, a range of dates and times when the service should be accessible. HTTP For an application connecting directly to a web server or URL:\nHostname of the web server, or IP address of same. Port number(s) in use for the email server. Database username (ie schema names) of all schemas that will be executing the code that connects to the web or email server. If required, a range of dates and times when the service must be accessible. Setting up ACLs The SYSDBA, or a user with DBA role granted, must execute the following code to create a new ACL:\n1 2 3 4 5 6 7 8 9 10 11 12 13 BEGIN DBMS_NETWORK_ACL_ADMIN.create_acl ( Acl =\u0026gt; \u0026#39;email_http_access.xml\u0026#39;, Description =\u0026gt; \u0026#39;Allows access to UTL_HTTP, UTL_SMTP etc\u0026#39;, Principal =\u0026gt; \u0026#39;USERNAME\u0026#39;, Is_grant =\u0026gt; TRUE, Privilege =\u0026gt; \u0026#39;connect\u0026#39;, Start_date =\u0026gt; SYSTIMESTAMP, End_date =\u0026gt; NULL); Commit; End; / The parameters, and points to note are:\nACL - is the name of an xml file. ACLs are kept in the XML Database product, so XDB must be installed. The data is kept in the table XDB.XDB$ACL and also in a folder on the database server. Description - is a meaningful, please, description of what the ACL is created to allow. Principal - the main user account in the database which requires access to the network utilities. This may be a role or a user. The parameter is case sensitive. Beware. Is_grant - true grants the privilege, false denies the privilege. Privilege - use \u0026lsquo;connect\u0026rsquo; for UTL_TCP, UTL_SMTP, UT:_MAIL and UTL_HTTP. Use \u0026lsquo;resolve\u0026rsquo; for UTL_INADDR. Beware, this parameter is also case sensitive. Start_date and End_date are null by default. Set these to a particular TIMESTAMP to prevent the ACL from being active until or after the specific date given. The commit is mandatory. Adding Users to ACLs Additional users and or roles are added to the ACL using the add_privilege procedure. The parameters are as above with the omission of the description parameter and the addition of the position parameter.\n1 2 3 4 5 6 7 8 9 10 11 12 13 BEGIN DBMS_NETWORK_ACL_ADMIN.add_privilege ( Acl =\u0026gt; \u0026#39;email_http_access.xml\u0026#39;, Principal =\u0026gt; \u0026#39;OTHER_USERNAME\u0026#39;, Is_grant =\u0026gt; TRUE, Privilege =\u0026gt; \u0026#39;connect\u0026#39;, Position =\u0026gt; NULL, Start_date =\u0026gt; NULL, End_date =\u0026gt; NULL); Commit; End; / The parameters and points to note are:\nPosition - defines the position in the list of ACL privileges, for this principal. If an ACL higher up the list denies access and one lower down grants it again, the latter takes precedence. The commit is mandatory. Assigning ACLs to a Network Resource Once created, the ACL must be assigned to a network using the assign_acl procedure. This is where the principal(s) which have been granted privileges in an ACL, are given access to a network resource for the duration of the ACL.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 BEGIN DBMS_NETWORK_ACL_ADMIN.assign_acl ( Acl =\u0026gt; \u0026#39;email_http_access.xml\u0026#39;, Host =\u0026gt; \u0026#39;IP_or_hostname\u0026#39;, Lower_port =\u0026gt; 80, Upper_port =\u0026gt; 80); DBMS_NETWORK_ACL_ADMIN.assign_acl ( Acl =\u0026gt; \u0026#39;email_http_access.xml\u0026#39;, Host =\u0026gt; \u0026#39;IP_or_hostname\u0026#39;, Lower_port =\u0026gt; 443, Upper_port =\u0026gt; 443); Commit; End; / The parameters and points to note are:\nACL - is the name of an existing ACL xml file. Host - defines the network resource allowed. Only those network resources mentioned here will be allowed to be accessed. Wild cards can be used, but are advised against. See below. Lower_port and upper_port define the port range permitted. For default HTTP servers this will be ports 80 and 443 (for HTTPS access). For email servers, the default SMTP port is 25. Only the specific host(s) and port(s) are accessible via the ACL. If you allow access to a web server on port 80 only, then attempting to use https on port 443 will be rejected. The commit is mandatory. Network Resources A network can be specified as follows:\nA hostname - \u0026lsquo;oracle.com\u0026rsquo; or \u0026lsquo;capgemini.com\u0026rsquo;. In this case, only that hostname will be accessible from the database. If the IP address changes, it will continue working once DNS has propagated. An IP address - \u0026lsquo;84.37.86.172\u0026rsquo;. Again, only this network address will be accessible. An IP range - \u0026lsquo;84.37.86.\u0026rsquo;. All devices on the 84.37.86 network are accessible. The \u0026lsquo;\u0026rsquo; indicates \u0026ldquo;everything\u0026rdquo; and can be used in one or more of the \u0026lsquo;dotted quads\u0026rsquo;. Everything - \u0026lsquo;*\u0026rsquo; - best avoided on the grounds that someone could use your database to set up and execute attacks on any device anywhere in the world. Use the principal of least privilege when setting up these ACLs. Ports By omitting the upper and lower ports, everything on the host is accessible. If you specify a range of port numbers then all ports in that range, inclusive, are accessible. If you need to set up a discontinuous port range, you must call the assign_acl procedure once for each range, as per the example above - ports 80 and 443 only are accessible. A Worked Example The following example shows the user norman attempting to access an HTTP web site directly to pull down a script, all from within SQL*Plus.\nInitial Test The URL shown below is fictitious. The web site does not actually exist. The process shown below, however, does work and I\u0026rsquo;m grateful to Tanel Poder for the details, which you can find here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 SQL\u0026gt; connect norman/secret Connected. SQL\u0026gt; set lines 1000 SQL\u0026gt; set trimspool on SQL\u0026gt; set trimout on SQL\u0026gt; set pagesize 0 SQL\u0026gt; set long 99999999 longchunksize 99999999 SQL\u0026gt; set feedback off SQL\u0026gt; set head off SQL\u0026gt; SQL\u0026gt; spool useful_script.sql SQL\u0026gt; SQL\u0026gt; select httpuritype(\u0026#39;http://fictitious.oracle.com/useful_script.sql\u0026#39;).getCLOB() 2\u0026gt; from dual; ERROR: ORA-29273: HTTP request failed. ORA-06512: at \u0026#34;SYS.UTL_HTTP\u0026#34;, line 1819 ORA-24247: network access denied by access control list (ACL) ORA-06512: at \u0026#34;SYS.HTTPURITYPE\u0026#34;, line 34 As you can see, it failed due to ACL reasons.\nSetup The following script is executed by the SYSDBA:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 BEGIN DBMS_NETWORK_ACL_ADMIN.create_acl ( Acl =\u0026gt; \u0026#39;email_http_access.xml\u0026#39;, Description =\u0026gt; \u0026#39;Allows access to UTL_HTTP, UTL_SMTP etc\u0026#39;, Principal =\u0026gt; \u0026#39;NORMAN\u0026#39;, Is_grant =\u0026gt; TRUE, Privilege =\u0026gt; \u0026#39;connect\u0026#39;, Start_date =\u0026gt; SYSTIMESTAMP, End_date =\u0026gt; NULL); Commit; End; / BEGIN DBMS_NETWORK_ACL_ADMIN.assign_acl ( Acl =\u0026gt; \u0026#39;email_http_access.xml\u0026#39;, Host =\u0026gt; \u0026#39;fictitious.oracle.com\u0026#39;, Lower_port =\u0026gt; 80,\t-- Default web server port Upper_port =\u0026gt; 80);\t-- Ditto Commit; End; / Successful Test 1 2 3 4 5 6 7 8 9 10 11 12 13 14 SQL\u0026gt; connect norman/secret Connected. SQL\u0026gt; set lines 1000 SQL\u0026gt; -- etc etc. See above. SQL\u0026gt; spool useful_script.sql SQL\u0026gt; SQL\u0026gt; select httpuritype(\u0026#39;http://fictitious.oracle.com/useful_script.sql\u0026#39;).getCLOB() 2\u0026gt; from dual; -- Lots of output scrolls up the screen here ... SQL\u0026gt; spool off Further Details Further details are available from Tim Hall\u0026rsquo;s web site where he takes the Oracle docs and makes sense of them. Most of the inforation given above is blatently borrowed from Tim\u0026rsquo;s excellent explanation.\nI had to use the above today to allow a database procedure access to an email server. The application is being ported from 10g to 11g and for some unknown reason, emails started failing! Tim\u0026rsquo;s blog post was extremely helpful in explaining and helping resolve the problem.\nThis blog post is simply a reminder to myself as to what had to be done to fix it. I\u0026rsquo;ll need it again soon I expect! For the whole story, get over to Tim\u0026rsquo;s web site. Don\u0026rsquo;t stick around here, trust me, it\u0026rsquo;s not worth it!\nThanks Tim.\nUpdate 1st March 2013: There\u0026rsquo;s an interesting blog post here where a database trigger is used to create the ACLs after a particular user is granted execute on UTL_MAIL. You might find it interesting.\n","description":"","id":95,"section":"posts","tags":null,"title":"Cannot Send Emails, or Read Web Servers From Oracle 11g","uri":"http://localhost:1313/RantsAndRaves/posts/2013/02/cannot-send-emails-or-read-web-servers-from-oracle-11g/"},{"content":"Oracle has raised an alert in the alert.log and created a trace file as well, for a failed DBMS_SCHEDULER job with a strange name which doesn\u0026rsquo;t appear in DBA_SCHEDULER_JOBS or DBA_SCHEDULER_PROGRAMS - what\u0026rsquo;s going on?\nAn extract from the alert log and/or the trace file mentioned in the alert log shows something like:\n1 2 3 *** SERVICE NAME:(SYS.USERS) ... *** MODULE NAME:(DBMS_SCHEDULER) ... *** ACTION NAME:(ORA$AT_SA_SPC_SY_nnn) ... Where \u0026rsquo;nnn\u0026rsquo; in the action name is a number.\nNo matter how hard you scan the DBA_SCHEDULER_% views, you will not find anything with this name. What is actually failing?\nOracle 11.1.0.6 onwards stopped listing these internal jobs in DBA_SCHEDULER_JOBS, as they did in 10g, and instead lists them in DBA_AUTOTASK_% views. However, not by actual name, so don\u0026rsquo;t go looking for a TASK_NAME that matches the above action name. You will fail.\nThere are three different autotask types:\nSpace advisor Optimiser stats collection SQl tuning advisor The tasks that run for these autotask \u0026lsquo;clients\u0026rsquo; are named as follows:\nORA$AT_SA_SPC_SY_nnn for Space advisor tasks ORA$AT_OS_OPT_SY_nnn for Optimiser stats collection tasks ORA$AT_SQ_SQL_SW_nnn for Space advisor tasks See MOS notes 756734.1, 755838.1, 466920.1 and Bug 12343947 for details. The first of these has the most relevant and useful information.\nUPDATE: My original failing autotask has been diagnosed by Oracle Support as bug 13840704 for which a patch exists here for 11.2.0.2 and 11.2.0.3.\nOracle document id 13840704.8 has details, but it involves LOBs based on a user defined type. In this case, Spatial data in an MDSYS.SDO_GEOMETRY column.\nThe view DBA_AUTOTASK_CLIENT won\u0026rsquo;t show you anything about a specific task, with the above names, but will show you details of what the overall \u0026lsquo;client\u0026rsquo; is, There are three:\n1 2 3 4 5 6 7 8 select client_name, status from dba_autotask_client; CLIENT_NAME STATUS ------------------------------- -------- auto optimizer stats collection ENABLED auto space advisor ENABLED sql tuning advisor DISABLED I can see from the task name in the alert log and trace file, that my failing task is a space advisor one, so, by looking into the DBA_AUTOTASK_JOB_HISTORY view, I can see what\u0026rsquo;s been happening:\n1 2 3 4 5 6 7 8 9 select distinct client_name, window_name, job_status, job_info from dba_autotask_job_history where job_status \u0026lt;\u0026gt; \u0026#39;SUCCEEDED\u0026#39; order by 1,2; CLIENT_NAME WINDOW_NAME JOB_STATUS JOB_INFO ------------------ --------------- ---------- ------------------------------------------- auto space advisor SATURDAY WINDOW FAILED ORA-6502: PL/SQL: numeric or value error... auto space advisor SUNDAY WINDOW FAILED ORA-6502: PL/SQL: numeric or value error... So, in my own example, the auto space advisor appears to have failed on Saturday and Sunday. Given that this is an internal task, and nothing I can do will let me know about the invalid number problem, I need to log an SR with Oracle on the matter. However, as I don\u0026rsquo;t want my fellow DBAs to be paged in the wee small hours for a known problem, I have disabled the space advisor task as follows:\n1 2 3 4 5 6 7 8 9 BEGIN dbms_auto_task_admin.disable( client_name =\u0026gt; \u0026#39;auto space advisor\u0026#39;, operation =\u0026gt; NULL, window_name =\u0026gt; NULL); END; / PL/SQL procedure successfully completed Checking DBA_AUTOTASK_CLIENT again, shows that it is indeed disabled:\n1 2 3 4 5 6 7 select client_name, status from dba_autotask_client where client_name = \u0026#39;auto space advisor\u0026#39;; CLIENT_NAME STATUS ------------------------------- -------- auto space advisor DISABLED Enabling it again after Oracle Support have helped resolve the problem is as simple as calling dbms_auto_task_admin.enable with exactly the same parameters as for the disable call:\n1 2 3 4 5 6 7 8 9 BEGIN dbms_auto_task_admin.enable( client_name =\u0026gt; \u0026#39;auto space advisor\u0026#39;, operation =\u0026gt; NULL, window_name =\u0026gt; NULL); END; / PL/SQL procedure successfully completed When enabling and/or disabling auto tasks, you must use the CLIENT_NAME as found in DBA_AUTOTASK_CLIENT view.\nThe full list of DBA_AUTOTASK_% views is:\nDBA_AUTOTASK_CLIENT DBA_AUTOTASK_CLIENT_HISTORY DBA_AUTOTASK_CLIENT_JOB DBA_AUTOTASK_JOB_HISTORY DBA_AUTOTASK_OPERATION DBA_AUTOTASK_SCHEDULE DBA_AUTOTASK_TASK DBA_AUTOTASK_WINDOW_CLIENTS DBA_AUTOTASK_WINDOW_HISTORY Hope this helps!\n","description":"","id":96,"section":"posts","tags":null,"title":"ORA$AT_SA_SPC_SY Jobs failing?","uri":"http://localhost:1313/RantsAndRaves/posts/2013/01/oraat_sa_spc_sy-jobs-failing/"},{"content":"I use Java only when I have to, and only ever the JRE (Java Runtime Environment) - there is no way I\u0026rsquo;ll use Java for development work. I\u0026rsquo;d rather eat my own ear wax to be honest!\nLinux Mint 13 comes with OpenJDK installed, and a system I use which runs the fop FO Processor, barfs with a Java.Lang.NullPointer Exception if OpenJDK is used.\nThe following is brief instructions on how to install Oracle\u0026rsquo;s java on Linux Mint.\nDownload Java JRE Go to here and click on the \u0026ldquo;Free Java Download\u0026rdquo; button.\nClick on Linux or Linux-x64 depending on whether you are running a 32 or a 64 bit Linux system. When prompted, select a suitable place to save the file, and click OK.\nRemeber, you don\u0026rsquo;t want the various Linux*.rpm packages - they are for Red Hat/Fedora/Scientific Linux/Centos/OpenSuse/Oracle Linux distributions.\nWhen the download is complete, you should have a saved copy of something like jre-7u10-linux-x64.tar.gz\nInstall Java In a shell session (don\u0026rsquo;t be afraid!), change to the directory where you saved the downloaded file. In my case, that was Downloads/Java in my home directory. Then uncompress the file.\n1 2 $ cd ~/Downloads/Java $ tar -xvzf jre-7u10-linux-x64.tar.gz Lots of filenames will whizz past on the screen. Wait for the process to finish.\nYou now need to be root, so, use sudo sh or su - or whatever to get you into a root session. I use su as my root user has been given a secure password.\nIn a root session, you need to move all the files you just extracted to the /usr/lib/jvm\n1 2 3 4 5 6 7 8 $ su - password: $ cd ~norman/Downloads/Java $ ls jre1.7.0_10 jre-7u10-linux-x64.tar.gz $ mv jre1.7.0_10 /usr/lib/jvm/ That\u0026rsquo;s all done now, Java is in the correct place. All that remains is to configure the system to use it in preference to anything else that is present.\nStill working as root, run the update-alternatives command, as follows, to tell the system about the new Java version, and to set it as default.\n1 2 3 $ update-alternatives --install /usr/bin/java java /usr/lib/jvm/jre1.7.0_10/bin/java 1065 update-alternatives: using /usr/lib/jvm/jre1.7.0_10/bin/java to provide /usr/bin/java (java) in auto mode. And also the following:\n1 2 3 $ update-alternatives --install /usr/bin/javaws javaws /usr/lib/jvm/jre1.7.0_10/bin/javaws 1065 update-alternatives: using /usr/lib/jvm/jre1.7.0_10/bin/javaws to provide /usr/bin/javaws (javaws) in auto mode. That\u0026rsquo;s it, Oracle\u0026rsquo;s Java is now installed and will be used as the default.\nIf, for some strange reason, I had installed a full development kit, rather than just the runtime, I would have required the following two commands to be run as well - to configure the default java compiler and jar tool.\n1 2 $ update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/jre1.7.0_10/bin/javac 1065 $ update-alternatives --install /usr/bin/jar jar /usr/lib/jvm/jre1.7.0_10/bin/jar 1065 Changing the Default Java Version The above makes my development tool happy as fop no longer barfs with an exception and my FO source code is happily converted into pdf documents.\nWhat happens when some other program doesn\u0026rsquo;t like to play with Oracle\u0026rsquo;s Java and needs the original OpenJDK version instead?\nSimple, change the default again:\n1 2 3 4 5 6 7 8 9 update-alternatives --config java There are 3 choices for the alternative java (providing /usr/bin/java). Selection Path Priority Status ------------------------------------------------------------ * 0 /usr/lib/jvm/jre1.7.0_10/bin/java 1065 auto mode 1 /usr/lib/jvm/java-6-openjdk-amd64/jre/bin/java 1061 manual mode 2 /usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java 1051 manual mode 3 /usr/lib/jvm/jre1.7.0_10/bin/java 1065 manual mode Press enter to keep the current choice[*], or type selection number:\nThe update-alternatives displays all known installed versions of Java and lets you choose the one you want. The first entry is the current one, and you simply type in the number of the one you want instead.\nHave fun. (If there\u0026rsquo;s such a thing as fun where Java is involved!)\n","description":"","id":97,"section":"posts","tags":null,"title":"Installing Oracle Java 7 on Linux Mint 13","uri":"http://localhost:1313/RantsAndRaves/posts/2013/01/installing-oracle-java-7-on-linux-mint-13/"},{"content":"WARNING - If you are easily offended, I would advise that you do not read any further. You have been advised, proceed at your own risk and don\u0026rsquo;t bother to complain, because if you do, I will most likely delete your comments. I have the power!\nAs I write this, the temperature outside is minus 60C - minus 76F in old money, or American degrees - but then again, I am about 36,000 feet up and still have three hours to go until Manchester.\nI am heading home, after two weeks in Tenerife over Christmas 2012. It was my Mother in Law\u0026rsquo;s idea to go away for Christmas, and she\u0026rsquo;s been a right royal PITA for a fortnight! Suffice to say I will not be holidaying with her ever again.\nOther than the wittering one, the usual crop of \u0026ldquo;chavs\u0026rdquo; and other people\u0026rsquo;s effing kids are driving me to drink, yet again.\nWhy is it that so many people think it is acceptable to force about three hundred other people to listen to their whining brat for four and a half hours flying time? Not to mention the three hours of \u0026ldquo;hanging about in bloody airport\u0026rdquo; time as well.\nThere is a reason we have kennels you know, next time, leave the noisy little bastard there if you absolutely must have a holiday. (Alternatively, do the environment a huge favour and stop breeding!)\nThey say travel broadens the mind. I say \u0026ldquo;Bullshit!\u0026rdquo;\nWhat travel actually does is remind you of how nice it is to stay at home, with the doors locked, and something decent (with David Attenborough or Professor Brian Cox in it) on TV.\nTravel is also a great reminder of just how widespread the UK\u0026rsquo;s import of the American obesity problem actually is.\nIf I am allowed only 20Kg luggage allowance and have to pay an extortionate fee for each ounce over my allowance, how come some fat, over-eating, self inflicted - and most likely diabetic - person, who probably weighs three times what I do, gets charged the same price per seat as I do? What about their \u0026ldquo;excess baggage\u0026rdquo; charges?\nEspecially when they think it\u0026rsquo;s ok for their spare blubber to spill over into my seat? And \u0026ldquo;do you mind if I raise the seat arm between us so that I can get a bit more comfy?\u0026rdquo;\nActually, I do effing mind. I paid for a seat on this plane and so did you. You did not pay half my fare so you are not entitled to half my seat space as well as your own. Two words - the second one is \u0026ldquo;off\u0026rdquo; and the first rhymes with \u0026ldquo;truck\u0026rdquo;! Catch my drift?\nDon\u0026rsquo;t like my attitude? Guess what? I don\u0026rsquo;t like your blubber spilling over and touching me! You deal with your problem and you might find me a bit more agreeable in future. In fact, you will find me more agreeable, because normally I am a considerate, courteous person - but you have seriously pissed me off!\nAnd smokers, yes you! Your drug habit is your own affair, please keep it that way.\nPlease do not shut yourself in a telephone box and chain smoke your way through your last 200 because you can\u0026rsquo;t last all the way home. You stink! You may not know it, but you are tremendously obnoxious to non-smokers as well as other smokers. It\u0026rsquo;s absolutely disgusting. I may puke very soon! (Or spew, as we Jocks normally say!)\nThe in flight movie on this trip appears to be something called \u0026ldquo;Miles To Go\u0026rdquo;. It is crap.\nIt has been on since we left - almost two hours ago - and so far, no dialogue. In fact, there has been nothing but what appears to be a satellite view of a small aeroplane, similar to this one, heading roughly North towards Manchester.\nI\u0026rsquo;ve saw something very similar once before when flying to and from, Australia. It was a crap movie then and this latest version has not improved things at all.\n(That last bit was supposed to be amusing - it may not have worked!)\nI have luckily got extra leg room seats. Well, I say luckily, I paid extra for a seat that can accomodate someone over 4'6\u0026quot; without their knees being somewhere close to their forehead for the duration of the flight.\nSurely a plane should have sufficient leg room in every seat?\nMind you, I have to say that the standard cattle class seat on this Thomson Airways Boeing 737-800, has far more leg room than any Monarch flight I\u0026rsquo;ve ever been on. For an excellent rant on that subject, search this blog for \u0026ldquo;Monarch\u0026rdquo;.\nOh great! The obnoxious brat two seats away has now started playing with one of his bloody \u0026ldquo;musical\u0026rdquo; toys. Everything he does is now accompanied by plinky-plonky \u0026ldquo;music\u0026rdquo; and the seemingly obligitory screaming and high pitched squealing so prevalent amongst children these days.\nThe \u0026ldquo;right on\u0026rdquo; and so called \u0026ldquo;modern\u0026rdquo; parents think that whispering \u0026ldquo;ssh\u0026rdquo; at the screaming little bar steward is helping. Well they must be deaf, because the little darling is paying them no heed, and continues to wake the other sleeping passengers. People actually want these things? What the fuck for?\nHave sex for fun, and stuff the \u0026ldquo;multiplying\u0026rdquo; bit so loved by the Roman Catholic Church!\nTwo more effing hours to go. What with the human whale next to me and that damned kid not far (enough) away - like on another planet - I could be in for a long prison sentence any time soon! I might be out in 15 years or so, if I behave myself.\nAs for entertainment, there is none. The \u0026ldquo;in seat\u0026rdquo; entertainment centre ( a place to plug in your specially purchased headset) has nine channels of total silence, and one of what sounds to me to be a 440 Hz sinusoidal tone. Could be an A but wtf do I know about music? The only thing I play is CDs and the odd parp on my didjeridoo!\nThe tone sound is, unfortunately, not loud enough to drown (now there\u0026rsquo;s a thought!) out the din from the wonder child two seats away. I hate him with a vengeance! Not just because of his incredibly ill mannered behaviour, but because the little shit was sat in the same bloody seats on the way out and screamed all the fucking way there as well!\nAccording to the \u0026ldquo;movie\u0026rdquo;, we are no longer over the sea and have just made landfall over the Spanish coast, somewhere near, or North, of Vigo.\nDid I mention the in flight \u0026ldquo;meal\u0026rdquo;? No? I won\u0026rsquo;t bother then, other than over priced, and tasteless!\nThe human whale did manage two meals, one after the other, then wiped his hands on the seat in front. Disgusting or what? Not as disgusting as his metronomic farting - fart fart fart fart - every two minutes. Jesus!\nOh excellent! Now we have a \u0026ldquo;wonderful opportunity\u0026rdquo; to buy assorted \u0026ldquo;stuff\u0026rdquo; from the \u0026ldquo;duty free\u0026rdquo; trolley. I think I shall give it a wide berth.\nNoise cancelling headphones are great on flights. They really do cancel out most of the din from the airflow and engines. At least my Sony ones do.\nUnfortunately, they don\u0026rsquo;t appear to do anything about screaming kids. Now there\u0026rsquo;s a market, ready made for a set of headphones that do! I would be first in the queue for those at any price!\nI did mention being driven to drink a while back, near the start of this rant. The brandy is not helping much.\nI\u0026rsquo;ve drunk the whole 5Cl \u0026ldquo;bottle\u0026rdquo; (which is of course priced similar to a half bottle in normal life) and the screaming brat is no easier to bear.\nOver the North coast of Spain now, heading upÂ towards Biscay, and they have decided to add in some turbulence. That isn\u0026rsquo;t helping the brat, nor my typing!\nTravel sucks! And I hate it. However, my wife loves it and the threat of divorce finds me stuck on yet another stressful commercial flight. (If I could afford my own plane, say a Honda Jet, I would travel more often but driving myself!)\nAnother child has just wandered past. He too is sitting two seats away. The difference is, he is quiet, well behaved and a credit to his parents. I can almost see the appeal - but there\u0026rsquo;s no way I will ever, ever, have a child!\nHeading towards Brest in France now, some way to go, but for some reason, the turbulence has stopped and so has the screaming child from hell. How long can the peace and quiet possibly last I wonder?\nAs it happens, oops - shades of Jimmy Savile there - not long. The demon son of Satan\u0026rsquo;s cat has started squealing again. 30 seconds was the answer.\nAnd now the battery in my noise cancelling headphones is dwindling and the cancellation is fading in and then cutting out. How positively irritating! I wonder how much worse this flight can get.\nAn hour to go. Is it worth trying to sleep? You never know, sleep might calm me down. They have just dimmed the cabin lights. Let\u0026rsquo;s see if it works. Wish me luck.\nHappy New Year - if it comes! ;-) Here\u0026rsquo;s to a peaceful, child free 2013.\n","description":"","id":98,"section":"posts","tags":null,"title":"Travel Sucks!","uri":"http://localhost:1313/RantsAndRaves/posts/2012/12/travel-sucks/"},{"content":"This post is all about cloning an 11g database from one server to another using an RMAN active database clone. This is not being done for Standby Database purposes, only to duplicate an existing database onto another server.\nThe physical structure on both servers is the same, some path names have been changed.\nSource Structure Database SID: msmdppr /srv/msmdp/oradata/msmdppr/ /srv/msmdp/flashback_area/msmdppr/ Everything hangs off /srv/msmdp/oradata/msmdppr and there are data, index, temp, undo, redo, ctrl, dbs, diag directories there.\nThe redo and ctrl directories are also to be found under the flashback_area for this database. A copy of the redo files are held there and there are some of the control files as well. This is a separate LUN from the oradata structure.\nThese are all part of a multi-spindle SAM, so it\u0026rsquo;s not as bad as it looks having everything in the \u0026ldquo;same place\u0026rdquo;.\nDestination Structure Database SID: msmpr /srv/msm/oradata/msmpr/all as above /srv/msmdp/flashback_area/msmpr/all as above The destination structure will end up the same as the source, and only the names will change. Where the source has /msmdp/ or /msmdppr/ the destination will be /msm/ or /msmpr/\nDestination Preparation Build the desired directory structures for the new database. In this example, I did the following, as the oracle user:\n1 2 3 4 5 6 7 8 9 $ mkdir -p /srv/msm/oradata/msmpr/ $ mkdir -p /srv/msm/flashback_area/msmpr/ $ cd /srv/msm/oradata/msmpr/ $ mkdir -p data index temp undo redo ctrl dbs changetracking $ mkdir -p diag/rdbms/msmpr/msmpr/adump $ cd /srv/msm/flashback_area/msmpr/ $ mkdir redo ctrl Create a temporary password file for the new database. Set the password to be the same as the source database. It won\u0026rsquo;t work otherwise. Ask me how I know?\nSource Preparation Make sure when you start the source database that there are no warnings about any deprecated parameters in the spfile. If there are, check the alert log and remove them. If you don\u0026rsquo;t, they will get transferred to the destination database and the clone will fail!\nRecreate a pfile from the spfile. You must have an up to date pfile. You can use a minimum pfile on the destination server, but I find it is better to be explicit. I\u0026rsquo;ve had problems.\nCopy the new pfile over to the destination server. You can put it straight into $ORACLE_HOME/dbs if you wish, or put it somewhere else, and create a symbolic link to $ORACLE_HOME/dbs. The latter is what I did. All individual database startup and password files live in the /dbs/ directory and are linked from there.\nFurther Preparation The remainder of the work needs some parts to be done on the source server and others to be done on the destination server.\nOn the destination server, edit the pfile that was created on the source server and copied over. You need to change all occurrences of the source database name to the destination one, and make sure that all your paths etc are correctly defined for the new database\nNow add the following two parameters to the pfile:\ndb_file_name_convert=\u0026rsquo;/srv/msmdp/oradata/msmdppr/\u0026rsquo;, \u0026lsquo;/srv/msm/oradata/msmpr/\u0026rsquo;, \u0026lsquo;/srv/msm/flashback_area/msmpr/\u0026rsquo;, \u0026lsquo;/srv/msm/flashback_area/msmpr/\u0026rsquo; log_file_name_convert=\u0026rsquo;/srv/msmdp/oradata/msmdppr/\u0026rsquo;, \u0026lsquo;/srv/msm/oradata/msmpr/\u0026rsquo;, \u0026lsquo;/srv/msm/flashback_area/msmpr/\u0026rsquo;, \u0026lsquo;/srv/msm/flashback_area/msmpr/\u0026rsquo; They will convert the filenames to the destination format. Beware, I have found that these parameters only substiture at the start of a parameter, not where the parameter includes the text to be replaced.\nStartup the destination database in NOMOUNT mode.\nOn the destination server, create a listener for this database and start it up.\nOn the destination server, create a tnsnames.ora entry for the new database.\nOn the destination server, test that you can connect to the destination database with sqlplus sys/password@msmpr as sysdba. If it works, carry on, otherwise, fix it.\nOn the source server, create a tnsnames.ora entry for the destination database.\nOn the source server, test the connection. You need to be able to connect to the database with sqlplus sys/password@msmpr as sysdba\nOn the source server, if the database is running in NOARCHIVELOG mode, shut it down, then restart it in MOUNT mode. If the database is running in ARCHIVELOG mode, it can either be OPEN or MOUNTed.\nOn the source server, set the environment for the source database (msmdppr) and start RMAN. You may need to specify $ORACLE_HOME/bin/rman to get the correct \u0026ldquo;rman\u0026rdquo; if you are on a Linux server. There are other rmans!\nIn the RMAN session, run the following commands:\n1 2 3 4 5 6 7 connect target sys/password connect auxiliary sys/password@msmpr duplicate target database to msmpr from active database nofilenamecheck password file; then leave RMAN to do its thing.\nIn my case that meant an overnight job as there was around 6.5Tb to clone. Luckily RMAN, when configured for performance, will skip the completely virgin blocks in a data file which makes things a little quicker.\nThe RMAN commands are saying to clone the source database and copy accross the password file from the source database to the destination one. If you have any users with SYSOPER or SYSDBA roles granted, their passwords will be in the password file, so you\u0026rsquo;ll need it on the destination server as well.\nWhen it finishes, you will find the password file and a new spfile for the destination database in $ORACLE_HOME/dbs. if necessary, move them to where you want them, and create symbolic links back to $ORACLE_HOME/dbs.\nWhen Things Go Wrong Problems happen. I\u0026rsquo;ve had my share, especially with RMAN clones!\nIn the event that the clone exercise fails at some point, don\u0026rsquo;t panic. Log out of RMAN on the source server, and go back in.\nOn the destination server, shut down the database, you can ABORT if necessary. Delete the spfile for the new database, if it still exists, then restart the destination database in NOMOUNT mode again.\nFix the problem, and start again from the connect target ... command in RMAN.\nOne thing I did notice, in my xxxx_file_name_convert parameters, I set it to convert \u0026lsquo;/msmdp/\u0026rsquo;,\u0026rsquo;/msm/\u0026rsquo;,\u0026rsquo;/msmdppr/\u0026rsquo;,\u0026rsquo;/msmpr/\u0026rsquo; and it ignored the second pair of substitutions. It seems as if it can cope with the first pair being part of the file names in question, but not the second pair, they remained unconverted and my data file copies failed because the /msmdppr/ directory wasn\u0026rsquo;t to be found.\nin the end I gave up on it completely, and just forced it to convert the entire thing.\nGood luck, RMAN, when it\u0026rsquo;s working is great. When you struggle with it, you struggle!\n","description":"","id":99,"section":"posts","tags":null,"title":"RMAN Active Database Clone - Different Servers, Same Structure","uri":"http://localhost:1313/RantsAndRaves/posts/2012/12/rman-active-database-clone-different-servers-same-structure-2/"},{"content":"Having a Raspberry Pi is fun, but like the days of my ZX-81, Spectrum and QL (I see a Sinclair habit there!) you get all sorts of bits hanging off and making it all look untidy. Here\u0026rsquo;s how to fit a whole pile of \u0026ldquo;stuff\u0026rdquo; into the space of an A4 sheet of paper.\nI know it\u0026rsquo;s the size of an A4 sheet because that\u0026rsquo;s what I drew it on before cutting the wood. The image shows the final result. (Click it for a bigger picture.)\nFrom the top left, working clockwise, we have the following:\nA 60Gb USB hard drive. Sitting on the drive, is a 400 pin breadboard currently stuffed full of Adafruit Pi Cobbler breakout connector. A 4 way powered USB hub. This powers the Pi, the USB drive, the WiFi connector and the wireless keyboard and mouse dongle. My Pi. It lives in a PiBow case. The GPIO ribbon cable works the Cobbler breakout or the Gertboard. There is just enough room to change the SD card or to remove it for backups. The sound and the composite video outputs are both accessible. The white box on a cable poking out the back is an HDMI to VGA converter box. My Gertboard. I was lucky enough to get one of the first ones that had to be built from a large kit of parts. I had much fun putting that together, and even more fun when it all \u0026ldquo;just\u0026rdquo; worked! The pear shaped \u0026ldquo;thing\u0026rdquo; in the middle of the PiBoard, partially hidden under the GPIO ribbon cable, is a USB extension, supplied with my wireless router, to plug the WiFi dongle into. You can just make out the dongle standing upright on top of the extension. The board iktself is a half inch, sorry, 12 mm thick piece of MDF which I cut to size and sanded smooth while wearing a face mask to avoid the dust, which can be nasty stuff.\nEverthing is stuck down with double sided sticky foam that was left over from when the new kitchen was installed (I knew it would come in handy one day!) and, in the case of the Gertboard, it\u0026rsquo;s also screwed down.\nUnderneath, to avoid scratching whatever I place it on, the board has a wide strip of single sided sticky foam, from the same kitchen left overs, down each side to act as non-slip feet.\nIn use, I plug it it and switch on. It either runs headless or I can connect it to a VGA cable to an old 12\u0026quot; LED telly with a VGA input. It can be run on WiFi or cabled Ethernet depending on whatever else I have plugged in at the time!\nAll this makes moving the Raspberry Pi around the house quite simple. With a quick edit to config.txt, I can have it running on the 22\u0026quot; HDMI monitor, or another edit and it\u0026rsquo;s back to the small VGA telly. Most of the time, it just sits there, running headless.\nRaspberry Pi\u0026rsquo;s are putting the fun back in computing.\n","description":"","id":100,"section":"posts","tags":null,"title":"It's Not a PiBow, It's a Raspberry PiBoard!","uri":"http://localhost:1313/RantsAndRaves/posts/2012/12/its-not-a-pibow-its-a-piboard/"},{"content":"Looking for the location of oraInventory on a server? Want to know where it is? Read on.\nThere is a file, known to Oracle, which holds the location of the inventory. Of course, it isn\u0026rsquo;t in the same place on every server, but the ones I know of have it as follows:\nLinux: /etc/oraInst.loc HP-UX: /var/opt/oracle/oraInst.loc Windows: Registry at HKLM/software/oracle/inst_loc For any other Unix, you can find it (as the oracle or root user) with:\n1 find / -type f -name oraInst.loc -print Once you know where it lives, you can move it simply. The following example moves it from the current location, found in oraInst.loc to /opt/oracle/oraInventory:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ cat /etc/oraInst.loc inventory\\_loc=/u01/app/oracle/oraInventory inst\\_group=oinstall $ cd /opt/oracle $ cp -Rp /u01/app/oracle/oraInventory ./ $ vi /etc/oraInst.loc :1 s?/u01/app?/opt? :wq $ cat /etc/oraInst.loc inventory\\_loc=/opt/oracle/oraInventory inst\\_group=oinstall Job done! Although it might be wise to take a backup of the original location, just in case, and then delete it from the old location:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ cd /opt/oracle $ tar -cvzf u01.app.oracle.tgz /u01/app/oracle ... $tar -tzf u01.app.oracle.tgz ## Just checking ... ... $cd /u01/app/ $pwd ## Safety check! /u01/app $ls ## Another safety check! oracle $rm -Rf oracle ## Getting nervous yet? I am! And that it, all done.\n","description":"","id":101,"section":"posts","tags":null,"title":"Where Does OraInventory Live? Can I Move it?","uri":"http://localhost:1313/RantsAndRaves/posts/2012/11/where-does-orainventory-live-can-i-move-it/"},{"content":"As you may know, Oracle databases come with a number of options. Some of these cost extra and if inadvertantly installed, Oracle must be paid money - note, you don\u0026rsquo;t have to be using them, only have them installed, to require payment. So what do you do if you need to remove an option?\nIn the old days, you used to have to rebuild the oracle binaries to add or remove options. To do this you needed to know the names of a number of make targets - not for the faint hearted.\nFrom 11g onwards, the process is much simpler. Oracle now supply the chopt (change option) utility to make your DBS\u0026rsquo;s life simple.\nThe utility is supplied with Enterprise and Standard Editions. It allows you to enable or disable the following options:\nData Mining Database Vault Oracle Label Security OLAP Partitioning Real Application Testing You can run the utility with no parameters to see what it does and how you should call it in anger:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ chopt usage: chopt \u0026lt;enable|disable\u0026gt; \u0026lt;option\u0026gt; Options: dm = Oracle Data Mining RDBMS Files dv = Oracle Database Vault option lbac = Oracle Label Security olap = Oracle OLAP partitioning = Oracle Partitioning rat = Oracle Real Application Testing e.g. chopt enable rat So, for example, to disable partitioning because the junior DBA has mistakenly installed it (all these options are selected and installed by default in Enterprise Edition!) then all you do is set the correct Oracle Home using oraenv in the normal manner, then:\nchopt disable partitioning as the following example demonstrates:\n1 2 3 4 5 $ chopt disable partitioning Writing to /srv/oracle/product/11gR1/db/install/disable\\_partitioning.log... /usr/bin/make -f /srv/oracle/product/11gR1/db/rdbms/lib/ins\\_rdbms.mk part\\_off ORACLE\\_HOME=/srv/oracle/product/11gR1/db /usr/bin/make -f /srv/oracle/product/11gR1/db/rdbms/lib/ins\\_rdbms.mk ioracle ORACLE\\_HOME=/srv/oracle/product/11gR1/db You can check the log file named on the first line of output for details. Enabling an option is just as simple:\n1 2 3 4 5 $ chopt enable partitioning Writing to /srv/oracle/product/11gR1/db/install/enable\\_partitioning.log... /usr/bin/make -f /srv/oracle/product/11gR1/db/rdbms/lib/ins\\_rdbms.mk part\\_on ORACLE\\_HOME=/srv/oracle/product/11gR1/db /usr/bin/make -f /srv/oracle/product/11gR1/db/rdbms/lib/ins\\_rdbms.mk ioracle ORACLE\\_HOME=/srv/oracle/product/11gR1/db You can only enable or disable a single option at a time, unlike when you are running the old style make commands where you could specify to turn them all on or off in one go. Progress?\n","description":"","id":102,"section":"posts","tags":null,"title":"Using \"chopt\" to Enable and Disable Oracle Options","uri":"http://localhost:1313/RantsAndRaves/posts/2012/11/using-chopt-to-enable-and-disable-oracle-options/"},{"content":"Have you ever found that the virtual hard drive attached to one of your VirtualBox VMs is getting too small? Want to know how to make it beigger?\nIt\u0026rsquo;s simple. The steps are as follows:\nDisconnect the drive from your VM. The VM should be shut down and not snapshotted for this step. You can either use the VMs storage settings to disconnect the hard drive, or go to File-\u0026gt;Virtual Media Manager, choose the correct hard drive, and click the release button. In a command line session, type the following command - you do not have to be root: 1 VBoxManage modifyhd /path/to/vdi/file.vdi --resize nnnn The size given, nnnn in the above, is the new size in Mb and must be bigger than the present size. You will then see something like the following example where I resize a virtual hard disc to 12Gb (12288 Mb): 1 2 norman@hubble $ VBoxManage modifyhd /data/VirtualBox/HardDisks/Antix.vdi --resize 12288 0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100% Go back to the VM\u0026rsquo;s storage settings, and re-attach the newly resized hard drive. Start the VM. Run the disc partitioner tool of your distro to extend an existing partition into the new free space, or to create a new partition. You may add the extra space to an existing logical volume if you are using LVM devices. As mentioned above, you cannot reduce the size of a virtual disc. If you try, you will see the following:\n1 2 3 4 norman@hubble $ VBoxManage modifyhd /data/VirtualBox/HardDisks/Antix.vdi --resize 6144 0%... Progress state: VBOX\\_E\\_NOT\\_SUPPORTED VBoxManage: error: Resize hard disk operation for this format is not implemented yet! That\u0026rsquo;s all there is to it!\n","description":"","id":103,"section":"posts","tags":null,"title":"How To Resize a VirtualBox Hard Drive","uri":"http://localhost:1313/RantsAndRaves/posts/2012/11/how-to-resize-a-virtualbox-hard-drive/"},{"content":"In order to downgrade an 11.2.0.3 Enterprise Edition database to Standard Edition, I had to use a Transportable Tablespace export/import. Because the default setting for DEFERRED_SEGMENT_CREATION is TRUE, the tablespace import barfed with numerous \u0026ldquo;IMP-00017: following statements failed with ORACLE error 1647:\u0026rdquo; errors. Want to know why?\nThe ORA-01649 error is \u0026ldquo;Tablespace is read only, cannnot allocate space in it\u0026rdquo; which is interesting as I\u0026rsquo;m importing a Transportable Tablespace dump file and all the tablespaces are read only after being created, until I manually make then read write.\nIn the source database, the \u0026ldquo;broken\u0026rdquo; tables all have zero rows in them, and have no entry in DBA_SEGMENTS which means that the Deferred Segment Allocation feature has indeed done its stuff, and deferred allocating a segment until the first row of data is entered (but not necessarily committed!) into the table.\nThe DEFERRED_SEGMENT_CREATION is also defaulted to TRUE in Standard Edition databases, but the parameter has no effect in these, as the following shows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Connected to: Oracle Database 11g Release 11.2.0.3.0 - 64bit Production SQL\u0026gt; show parameter deferred_seg NAME TYPE VALUE -------------------------- -------- ------------- deferred_segment_creation boolean TRUE SQL\u0026gt; create table test(a number); Table created. SQL\u0026gt; select table_name, segment_created 2 from user_tables 3 where table_name = \u0026#39;TEST\u0026#39;; TABLE_NAME SEG -------------------- --- TEST YES So, even with an empty table, in Standard Edition, the deferred segment allocation does not take place. This is exactly why I\u0026rsquo;m having problems running a transportable tablespace import from an Enterprise Edition database to a Standard Edition one. It seems that Standard expects everything to have at least one allocated segment.\nIf the importing database is Enterprise Edition, this problem doesn\u0026rsquo;t occur.\nThe quick workaround is to run a table based export, using exp or expdp, of the affected tables, and import that at the receiving database using imp or impdp.\nThe longer term workaround is to make sure that DEFERRED_SEGMENT_CREATION is set to FALSE in the spfile.\nIn the meantime, I\u0026rsquo;m logging a bug with Oracle as the DBMS_TTS.TRANSPORT_SET_CHECK procedure should identify these tables and warn about them, or, the import should correctly import them anyway.\n","description":"","id":104,"section":"posts","tags":null,"title":"Oracle's Deferred Segment Allocation Breaks Transportable Tablespace Imports.","uri":"http://localhost:1313/RantsAndRaves/posts/2012/11/oracles-deferred-segment-allocation-breaks-transportable-tablespace-imports/"},{"content":"Ever wanted to backup your Raspberry Pi\u0026rsquo;s SD card, but didn\u0026rsquo;t know who to ask? Me too. Read on \u0026hellip;.\nThe first thing to remember is that you should really always have a backup of your SD card. In theory every time you make a change, but in practice, it will be less frequently than that!\nI have had two cards corrupt themselves when I managed to lock my Pi completely, and the only recourse was to pull the power. Unfortunately, in both cases, I was left with a card that the Pi could read the first partition from, but then couldn\u0026rsquo;t mount the second, so no operating system.\nLuckily I had a backup, but it was of a 4Gb SD card and I\u0026rsquo;m running on 8 Gb, so I hjad to restore and then extend the second partition to use the remaining 4 Gb of unallocated space. Normally I\u0026rsquo;d restore the 8 Gb image to an 8 Gb card.\nThese instructions are for Linux only. If you use one of the Windows utilities for writing to your SD card, there should be an option to read from it and write to a file, if so, this is what you need to use. Beware, there may need to be two reads if you have two partitions, with some of these utilities.\nMake a Backup Plug your card into your computer, no, not your raspberry Pi ;-), and wait a second or two for it to settle down.\nIf you have a card slot, you most likely have the card on /dev/mmcblk0 and if you have it in a USB adaptor of some kind, it\u0026rsquo;s most likely going to be /dev/sdx where \u0026lsquo;x\u0026rsquo; is the digit representing the highest device of this kind.\nTo find out for certain, do the following, either using sudo, or as root:\n1 2 3 4 5 6 7 8 9 10 11 12 13 fdisk -l ... ... Disk /dev/mmcblk0: 7948 MB, 7948206080 bytes 4 heads, 16 sectors/track, 242560 cylinders, total 15523840 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000dbfc6 Device Boot Start End Blocks Id System /dev/mmcblk0p1 8192 122879 57344 c W95 FAT32 (LBA) /dev/mmcblk0p2 122880 15523839 7700480 83 Linux The above shows the tail end of the output on my system. I can recognise the card by the size, 8 Gb (or 7948 as listed above) which is the only drive I have of this size, and its two partitions bearing in mind the fact that one of them is small and W95 FAT (LBA) and the other plain old Linux.\nIn the above, the suffixes \u0026lsquo;p1\u0026rsquo; and \u0026lsquo;p2\u0026rsquo; indicate the partitions on the card. The device itself is /dev/mmcblk0 and that\u0026rsquo;s what we need, the device name, not the partition names.\nTo take a backup, switch to your backup directory, where you intend to keep the copy of your SD card, and run the following command as the root user (or use sudo):\n1 2 3 4 dd if=/dev/mmcblk0 of=Rpi_8gb_backup.img bs=2M 3790+0 records in 3790+0 records out 7948206080 bytes (7.9 GB) copied, 1369.42 s, 5.8 MB/s That\u0026rsquo;s it. The whole 8Gb SD card has been (slowly) copied to my computer. How do I know it worked?\nCheck the Backup File The output above shows that there were no errors in copying the SD card to disk, but it\u0026rsquo;s always best to be sure that it will work when the time comes to carry out a restore.\nThe first step, as root - as ever - is to determine where the partitions begin. Because we have an image copy of the SD card, we can look at the file itself and see the partition table within.\nThe following commands will be carried out as root, or using the sudo command.\nWe first list the SD image file to see where the partitions begin and the sizes of the sectors. Fdisk reports sizes in sectors, but handily displays the sector size at the top.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 fdisk Rpi_8gb_backup.img Welcome to fdisk (util-linux 2.21.2). ... Command (m for help): p Disk /home/norman/Backups/Rpi_8gb_backup.img: 7948 MB, 7948206080 bytes 255 heads, 63 sectors/track, 966 cylinders, total 15523840 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000dbfc6 Device Boot Start End Blocks Id System /home/norman/Backups/Rpi_8gb_backup.img1 8192 122879 57344 c W95 FAT32 (LBA) /home/norman/Backups/Rpi_8gb_backup.img2 122880 15523839 7700480 83 Linux We can see from the line starting with \u0026lsquo;Units\u0026rsquo; or \u0026lsquo;Sector size\u0026rsquo; that a sector is 512 bytes. We can also see that our two partitions start at sectors 8,192 and 122,880.\nMultiplying these start sectors by 512 gives the start position in bytes. This works out at 4,194,304 and 62,914,560 bytes respectively.\nOn your Raspberry Pi, the first partition is mounted at /boot while the second is the root (/) mount point.\nTo check these, without needing access to the Pi, we simply proceed as follows on a Linux computer:\n1 2 3 4 5 6 mkdir /mnt/root /mnt/boot chmod a=rwx /mnt/root /mnt/boot mount -t vfat -o loop,offset=4194304 /home/norman/Backups/Rpi_8gb_backup.img /mnt/boot mount -t ext4 -o loop,offset=62914560 /home/norman/Backups/Rpi_8gb_backup.img /mnt/root The above creates a couple of mount points (directories) and then mounts the first partition within the image file as a vfat file system on /mnt/boot and then mounts the second partition as an ext4 file system on /mnt/root.\nThe first two commands to create the mount points and set the permissions on them are only required once, the first time you carry out this exercise.\nYou can now see the files by opening a file manager and looking at the /mnt/boot and /mnt/root directories - you should see your various files as if you were looking on your Raspberry Pi.\nRestore a Single File This method of mounting a disc image as a device is useful for times when you manage to delete a file, accidentally of course, on your Pi, but you know that you have a backup. If the Pi is on your network, you can simply mount the backup image as above, locate the file and check that it is the one you want, then use scp or sftp to copy the file to your Pi, as demonstrated below:\n1 2 3 4 5 mount -t ext4 -o loop,offset=62914560 /home/norman/Backups/Rpi_8gb_backup.img /mnt/root scp /mnt/root/etc/network/interfaces pi@raspberrypi: pi@raspberrypi\u0026#39;s password: interfaces 100% 183 0.2KB/s 00:00 The file is now located in the home directory of the pi user. All you need to do to move it to the correct place is:\n1 2 3 sudo mv interfaces /etc/network/ sudo chmod u=rw,go=r /etc/network/interfaces sudo chown root:root /etc/network/interfaces Obviously, if you are like me, you have already given your root user a password, so you could miss out the above and simply scp the file straight into the correct location. You would only need to set the permissions and not the ownership of the file afterward.\n1 2 3 4 5 scp /mnt/root/etc/network/interfaces root@raspberrypi: root@raspberrypi\u0026#39;s password: interfaces 100% 183 0.2KB/s 00:00 sudo chmod u=rw,go=r /etc/network/interfaces So there you have it, how to backup your SD card, how to mount it on the backup computer to check that it is ok, and as a bonus, how to extract a file (or files) for an individual recover. What else could we do?\nRestore a Full Backup This is as simple as initialising the SD card for the first time. You can restore a backup file to your SD card provided that the SD card is bigger or the same size as the image file.\nAs ever, the following commands are executed as root or by using the sudo command.\n1 dd if=Rpi_8gb_backup.img of=/dev/mmcblk0 bs=2M It takes a while, but it will restore in the end. All you have to do is wait for the copy to finish and then boot the Pi with the restored SD card in the slot.\nCopy Your Current System to a Larger Card We can also use the smaller backup files to initialize a larger SD card, if we were perhaps upgrading. This is what I had to do when I restored a 4 Gb backup to my 8 Gb card. Once that was done, I booted the Pi with the restored SD card in place, and when I logged in, executing sudo raspi-config and selecting the option to Expand root partition to fill SD card extended the current almost 4 Gb partition to the full almost 8 Gb size required (on the next reboot that is.)\nYou can see that it worked by executing the df command, which does not need to be executed as root!\n1 2 3 4 5 6 7 8 9 df -h Filesystem Size Used Avail Use% Mounted on rootfs 7.3G 1.6G 5.4G 23% / /dev/root 7.3G 1.6G 5.4G 23% / devtmpfs 109M 0 109M 0% /dev tmpfs 22M 208K 22M 1% /run tmpfs 5.0M 0 5.0M 0% /run/lock tmpfs 44M 0 44M 0% /run/shm /dev/mmcblk0p1 56M 25M 32M 44% /boot The first and last entries in the above show that the root file system, mounted on / is 7.3 Gb while the /boot file system is 56 Mb in size. So, my 4 Gb restore is now happily running on an 8 Gb card and I have all the space available to use.\n","description":"","id":105,"section":"posts","tags":null,"title":"Backup \u0026 Check a Raspberry Pi SD Card","uri":"http://localhost:1313/RantsAndRaves/posts/2012/10/backup-check-a-raspberry-pi-sd-card/"},{"content":"Update: 23/02/2023: The following link no longer exists.\nSome people are confused by the clustering factor of an index in an Oracle database. Be confused no more, just read this article by Tom Kyte on the subject.\n","description":"","id":106,"section":"posts","tags":null,"title":"Oracle Index Clustering Factor Explained","uri":"http://localhost:1313/RantsAndRaves/posts/2012/10/oracle-index-clustering-factor-explained/"},{"content":"Want to know how to redirect the sound from your Pi to either the HDMI or to the headphone socket? Read on \u0026hellip;\nUpdate - 11 February 2013 I\u0026rsquo;ve written a small command line utility - PiSound - to control the settings of the audio device on your RaspberryPi. You can download it from https://github.com/NormanDunbar/PiSound. Enjoy.\nDeciding on the Output Device The following command is all you need:\n1 sudo amixer cset numid=3 n Where the final \u0026rsquo;n\u0026rsquo; is as follows:\n0 = Auto - if HDMI is connected, use that, otherwise try the headphone socket.\n1 = Sound goes to the headphone socket.\n2 = Sound goes to HDMI socket.\nIf your current user is a member of the audio group, the sudo parts of the amixer commands is not required.\nYou can make sure that it works by running a command such as:\n1 aplay /usr/share/sounds/alsa/Front_Left.wav You should be able to hear the sound if you have set the correct output as above.\nHave fun.\nRaspbain 16th December 2012 - Update It appears that something (a technical term) has gone wrong in the 16/12/2012 Raspbian release and/or after sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade - which stops sound working.\nYou can tell if you are affected as follows:\n1 2 3 4 5 6 $ sudo amixer controls numid=4,iface=MIXER,name=\u0026#39;Master Playback Switch\u0026#39; numid=3,iface=MIXER,name=\u0026#39;Master Playback Volume\u0026#39; numid=2,iface=MIXER,name=\u0026#39;Capture Switch\u0026#39; numid=1,iface=MIXER,name=\u0026#39;Capture Volume\u0026#39; If you see the above, then you are affected and nothing you can do will allow you to redirect sound to the headphone socket. When I say nothing, I mean, nothing, except the following, as explained here, however, read the next section before you start removing stuff that you might need!\n1 2 3 sudo apt-get purge --yes pulseaudio ... sudo reboot When your Pi comes back up, login and try again:\n1 2 3 4 5 $ sudo amixer controls numid=3,iface=MIXER,name=\u0026#39;PCM Playback Route\u0026#39; numid=2,iface=MIXER,name=\u0026#39;PCM Playback Switch\u0026#39; numid=1,iface=MIXER,name=\u0026#39;PCM Playback Volume\u0026#39; Now you can use the sudo amixer cset numid=3 1 command as described above, to direct the audio output to your headphones.\nBut I need PulseAudio \u0026hellip; You might be in a situation where you need to keep PulseAudio installed. What to do? The answer is simple, in all the calls to amixer add in a card selector such as -c 0.\nNormally, the PCM card is card 0 (zero) and PulseAudio is card 1 (one). Somehow, PulseAudio sets itself as the default card. I haven\u0026rsquo;t bothered to discover how or where it does this yet, I deinstalled PulseAudio on my system.\n1 2 3 4 5 $ sudo amixer -c 0 controls numid=3,iface=MIXER,name=\u0026#39;PCM Playback Route\u0026#39; numid=2,iface=MIXER,name=\u0026#39;PCM Playback Switch\u0026#39; numid=1,iface=MIXER,name=\u0026#39;PCM Playback Volume\u0026#39; Hooray! If the above works for you, where leaving out the card select options -c 0 does not, then you must add -c 0 to all the amixer commands below.\nMuting Sound Numid=2 determines if sound is muted or not. To mute sound, regardless of its output device, do this:\n1 $ sudo amixer cset numid=2 0 and to unmute the sound:\n1 $ sudo amixer cset numid=2 1 Volume Control Numid=1 allows you to set the volume. The range is slightly strange in that it runs from -10239 to +400 with +400 being the maximum. On my system, I have a pair of X-mini powered and amplified speakers attached. A minimum value of -1000 gives a quiet sound, 0 (zero) gives reasonable sound and 400 is a bit too loud.\nYou adjust the volume as follows:\n1 $ sudo amixer cset numid=1 -- -1000 Please note the double hyphen. This is required in front of any parameter that has a leading hyphen. In this case, the volume setting I require is -1000, so the double hyphen says \u0026ldquo;the following is a value, even though it has a hypen, it is not another flag or option!\u0026rdquo;\nYou can use the double hyphen in front of positive numbers as well, without any adverse effects.\n1 $ sudo amixer cset numid=1 -- 234 Positive values between 0 and 400 appear unchanged while negative values between -1 and -10239 are rounded up to 0 to -10238.\nThe only way to get -10239 is to mute the sound.\nOf course, being human, it would be nice to set the volume to something easily figured out, like a percentage, wouldn\u0026rsquo;t it? This would be nice, for example:\n1 $ sudo amixer cset numid=1 60% No need to work out numbers in a weird range, no need for the double hyphens etc. Try it, it works! The range is obviously from 0% to 100%, anything outside of those boundaries will be limited to the appropriate percentage. Setting the volume to 0% effectively mutes the audio output.\nWhat are my Settings? You may, if you wish, view all the settings on your Pi with the following single command:\n1 2 3 4 5 6 7 8 9 10 11 12 $ sudo amixer contents numid=3,iface=MIXER,name=\u0026#39;PCM Playback Route\u0026#39; ; type=INTEGER,access=rw------,values=1,min=0,max=2,step=1 : values=1 numid=2,iface=MIXER,name=\u0026#39;PCM Playback Switch\u0026#39; ; type=BOOLEAN,access=rw------,values=1 : values=on numid=1,iface=MIXER,name=\u0026#39;PCM Playback Volume\u0026#39; ; type=INTEGER,access=rw---R--,values=1,min=-10239,max=400,step=0 : values=-1000 | dBscale-min=-102.39dB,step=0.01dB,mute=1 If you wish to find the settings for one control only, use the same numid as you used to cset the control, but read the setting with the cget command instead:\n1 2 3 4 5 6 $ sudo amixer cget numid=3 numid=3,iface=MIXER,name=\u0026#39;PCM Playback Route\u0026#39; ; type=INTEGER,access=rw------,values=1,min=0,max=2,step=1 : values=1 scale-min=-102.39dB,step=0.01dB,mute=1 There doesn\u0026rsquo;t appear to be a way of fetching the current setting into a variable for use in, say, a bash script. Not unless you parse the data out of the returned string. The following python code will do this for you:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import os ... volume = None stdout = os.popen(\u0026#39;amixer cget numid=1\u0026#39;) try: volume = stdout.read() finally: stdout.close() # At this point, volume (a string) and contains all the output from # the amixer cget command. Extract the volume value, if no exceptions # occurred. It is None if there was an exception if volume: volume = volume.split(\u0026#39;:\u0026#39;)[1].split(\u0026#39;\\n\u0026#39;)[0].split(\u0026#39;=\u0026#39;)[1] # At this point, volume contains the volume setting as a string. ... There isn\u0026rsquo;t, as far as I can find, any way of getting the current volume as a percentage. If that\u0026rsquo;s what you want or need, I\u0026rsquo;m afraid you will have to work it out - as a slight clue, the following python code might help:\n1 2 3 4 5 6 7 8 9 ... # volume is a string holding the volume setting or is None. # We want it as an integer percentage, or -1 for errors.. if volume: percentage = int(((float(volume) + 10240) / 10640) * 100) else: percentage = -1 ... You must float the string value or some calculations end up as zero percent due to the division by 10640.\n","description":"","id":107,"section":"posts","tags":null,"title":"Raspberry Pi Sound","uri":"http://localhost:1313/RantsAndRaves/posts/2012/10/raspberry-pi-sound/"},{"content":"And while you are at it, no warranty problems when you do so!\nAn announcement on the Raspberry Pi Foundation web site, read it here, (Update: 23/02/2023: Sorry, dead link now.) explains that it is now possible to use Turbo mode on your Raspberry Pi without invalidating your warranty by over volting the device.\nPlease go and read it and then pop back here for instructions on adding Turbo mode to your own Raspberry Pi.\nBack again? Ready to begin? Let\u0026rsquo;s do it!\nThe following assumes that you are running Raspbian as your chosen distro, and that you are logged in as the pi user.\n1 2 3 4 5 sudo apt-get update sudo apt-get upgrade sudo raspi-config ### Select the \u0026#34;update\u0026#34; option which updates raspi-config and exits. 1 2 3 4 sudo raspi-config select the new \u0026#34;configure overclocking\u0026#34; option and choose one. select finish. select yes to reboot now. On reboot, your Raspberry Pi will be running [much] faster under load and, if it gets too hot, will slip back to normal non-turbo mode automagically.\nIt is possible that your raspberry Pi will not boot if the settings are too high for your system. If this is the case, hold down the shift key during the boot.\nI set mine to full TURBO mode and it booted, but the CPUFreq Monitor (see below) shows 700 MHz rather than 1 GHz, however, this is because the Pi is now running in automatic gearbox mode - when the CPU usage gets above 85%, then it switches to a higher gear.\nYou can see this if you have the CPUFreq Monitor installed (see below) and, in a shell session run something like sudo apt-get update and while that is running, hover over the CPUFreq Monitor icon to see the speed changing up and down as the load comes and goes.\nYou can see the CPU Frequency in use in a console as per the following example:\n1 2 cat /sys/devices/system/cpu/cpu0/cpufreq/scaling\\_cur\\_freq 1000000 The output from which will be the frequency running now, in KHz. Divide by 1,000 to get the MHz frequency. The example above was taken while an sudo apt-get update was in progress. After the hard work was done, it dropped again:\n1 2 cat /sys/devices/system/cpu/cpu0/cpufreq/scaling\\_cur\\_freq 700000 As mentioned above, if your Pi refuses to boot up with your chosen settings, simply reboot while holding down the shift key to skip the turbo mode setting, then sudo raspi-config to set it to a lower setting again.\nYou can, if you are running a GUI using XFCE, add the temperature sensor output and the current CPU frequency settings to your task bar, as follows:\nRight-click the task bar and select \u0026ldquo;Panel settings\u0026rdquo; Click on the \u0026ldquo;Panel Applets\u0026rdquo; tab Click the \u0026ldquo;Add\u0026rdquo; button. Select \u0026ldquo;Temperature Monitor\u0026rdquo; from the list, and click ADD. Use the \u0026ldquo;Up\u0026rdquo; and \u0026ldquo;Down\u0026rdquo; buttons to put it where you want it to be. Click the \u0026ldquo;Add\u0026rdquo; button. Select \u0026ldquo;CPUFreq Frontend\u0026rdquo; from the list and click ADD. Use the \u0026ldquo;Up\u0026rdquo; and \u0026ldquo;Down\u0026rdquo; buttons to put it where you want it to be. Click the \u0026ldquo;Close\u0026rdquo; button. You should see a couple of new items in the task bar. One is a two digit number (or hopefully two digits) in green text, and the other looks like a, well, I\u0026rsquo;m not sure what it looks like in my VNC session!\nAnyway, hover the mouse over each and you will get told what\u0026rsquo;s what. The CPUFreq Monitor tells you what frequency you are running at and the temperature gauge stays green as long as the Raspberry Pi is running within temperature bounds. If the text turns red, it\u0026rsquo;s getting too hot and your Pi should shut down to a slower overclock/overvolt to reduce the temperature.\nYou can configure these applets by right-clicking on them, and selecting the applet\u0026rsquo;s own settings menu option (at the top of the menu).\nAgain, there is a console tool to allow you to see the current temperature:\n1 2 cat /sys/class/thermal/thermal\\_zone0/temp 42236 The temperature reported in in thousandths of a degree Celsius. Divide by 1,000 to get degrees C. (Add 273 to degrees C to get degrees Kelvin!)\nSee the original article, links above, for full details of what else is included in this release. You can also read this article on setting up the other options in the new release.\n","description":"","id":108,"section":"posts","tags":null,"title":"Give Your Raspberry Pi Turbo Mode","uri":"http://localhost:1313/RantsAndRaves/posts/2012/09/give-your-raspberry-pi-turbo-mode/"},{"content":"Brilliant! Read Tony Ballantyne\u0026rsquo;s excellent article. And enjoy.\n","description":"","id":109,"section":"posts","tags":null,"title":"You Are Not Allowed Science Any More","uri":"http://localhost:1313/RantsAndRaves/posts/2012/09/you-are-not-allowed-science-any-more/"},{"content":"As a Raspberry Pi user, I figured I needed a USB hub to allow me to connect stuff (that\u0026rsquo;s a technical term by the way) to my Pi without overloading the two on board USB ports as these are limited to a maximum of 100 mA each. I purchased a cheap 7 port externally powered hub and it arrived by next day delivery.\nThe USB 2.0 specification says that anything advertising compliance should, amongst other things be able to supply 500 mA minimum on each and every port while externally powered and should not feed any power back to the computer via the USB interconnect cable - that\u0026rsquo;s the one between the computer and the hub.\nThis is a link to a USB hub that is identical to the one I purchased and there are plenty of others that look exactly like this on Ebay as well as other places. Please note, I did not purchase my hub from Amazon and I certainly didn\u0026rsquo;t pay nearly Â£20 for it either! It looks a whole lot better in the Amazon photo that it does in real life, I can assure you.\nThey are available new, on Ebay, from around Â£3.00 to about Â£9.00. Mine cost Â£7.75 and was badged with the name Dynamode (only on the packaging though!) and claimed to be fully USB 2.0 compliant. Ha! They all look remarkably like the image to be found on this page somewhere around here! The text on top is \u0026ldquo;[HUB]\u0026rdquo; if you can\u0026rsquo;t make it out.\nOk, if you read my review on Amazon at the above link, you\u0026rsquo;ll know what was wrong with it. If you can\u0026rsquo;t be bothered, read on\u0026hellip;\nWhen plugged into the Pi with the USB interconnect cable, and the hub was then powered on (but not yet the Pi), the power LED on my Pi lit up. That should never happen and shows that the hub is allowing power to be sourced from the hub rather than only sunk from the Pi. That fails section 7.2.1 of the USB 2.0 standard and means that the hub is not USB 2.0 complaint at all.\nIn addition, removing the interconnect cable and powering the Pi from one of the USB ports, didn\u0026rsquo;t supply enough power to allow the Pi to boot. Yet again, this fails the USB 2.0 standard which clearly says that all ports must be able to provide a minimum of 500 mA when the hub is externally powered (not on batteries though). It should perhaps be noted that the Pi requires 500 mA to boot and an additional 100 mA per \u0026ldquo;in use\u0026rdquo; USB port. My Pi had nothing plugged in to the USB so only requires 500 mA.\nThat\u0026rsquo;s two failures so far!\nEven worse, given that this was a 7 port hub, each of those ports should have been able to supply a minimum of 500 mA and yet, the power supply that came with the hub was rated at 1000 mA - which is only just enough for a two port hub and nowhere near enough for a 7 port hub.\nAnd it gets worse!\nWith this hub attached to the Pi using the interconnect cable, the Pi refused to boot. Removing the hub and booting the Pi was fine, but on connecting the hub, all networking stopped functioning! My SSH and VNC sessions dropped out and attempting to reconnect just failed. Removing the hub allowed networking to work again.\nSo, all in all, it\u0026rsquo;s not worth the money and it doesn\u0026rsquo;t do what it says on the packaging. These things are best avoided. I took it back to the shop for a refund and explained why I was returning it. All the details were marked down on their system and it\u0026rsquo;s just possible that they will not be selling these any more.\nThese things do work with laptops and such like, but are you willing to connect one to your laptop when it supplies power back down the interconnect cable? If your laptop (or computer) doesn\u0026rsquo;t have a diode present to prevent a back feed, who knows what it might damage? The Raspberry Pi doesn\u0026rsquo;t have such a diode, but why should it have one? The standard is completely clear on the matter - no power down the interconnect!\nThat Power Supply Again It appears that someone with better electronic skills than myself has taken the power supply on one of these hubs apart. Watch the video and listen carefully to the commentary on this You Tube video. Stick with it to the end and be amazed at how close you come to stuffing mains voltage down your USB devices!\nWorking, Tested USB Hubs The Pi Hut has any number of good quality working USB hubs. This search will take you to a list of them. Enjoy.\nMy Pi is running now with a 60Gb USB hard drive, a USB Floppy Drive (don\u0026rsquo;t ask!) and is powered by the hub itself. The power supply is 2A and the hub is a 4 port one which works out at 500 mA per port which is exactly correct according to the standards. There is no back feed of power via the interconnect - so, it\u0026rsquo;s perfect.\n","description":"","id":110,"section":"posts","tags":null,"title":"The Dangers of Cheap USB Hubs","uri":"http://localhost:1313/RantsAndRaves/posts/2012/08/the-dangers-of-cheap-usb-hubs/"},{"content":"Adrci is a new tool in Oracle 11g which makes life a little easier when gathering evidence to send off to Oracle Support, but it can make life easier when you simply wish to view the alert log, for example.\nAs ever, you need to be logged in to the database server and have the environment set in the normal oraenv manner. This is how we used to do it in the old, pre 11g, days:\n1 2 3 4 5 6 7 8 9 10 SQL\u0026gt; connect / as sysdba connected SQL\u0026gt; show parameter background_dump_dest NAME TYPE VALUE -------------------- ------ ----------------------------------------- background_dump_dest string /srv/xxx/oradata/yyyyy/diag/rdb ms/yyyyy/yyyyy/trace SQL\u0026gt; exit 1 $ view /srv/xxx/oradata/yyyyy/diag/rdbms/yyyyy/yyyyy/trace/alert_yyyyy.log In modern times, with 11g of course, we no longer have to do this, as we have adrci instead, as follows:\n1 2 3 4 5 6 7 $ adrci ADRCI: Release 11.2.0.3.0 - Production ..... ... ADR base = \u0026#34;/srv/xxx/oradata/yyyyy\u0026#34; adrci\u0026gt; show alert That\u0026rsquo;s all there is to it. The alert log is, on my server, displayed in the vi editor and when you quit with the :q command, you return to adrci.\nWhile you are there, you can check for any incidents recorded in the alert log:\n1 2 3 4 5 6 7 8 adrci\u0026gt; show incident ADR Home = /srv/xxx/oradata/yyyyy/diag/rdbms/yyyyy/yyyyy ********************************************************************** INCIDENT_ID PROBLEM_KEY CREATE_TIME ------------ ----------------- --------------------------------- 145 ORA 227 2012-08-06 16:22:40.026000 +01:00 40165 ORA 600 2012-08-10 13:14:23.033000 +01:00 Given that ORA-600, I can get more detail about it with the following command:\n1 2 3 4 5 6 7 8 9 10 11 12 show incident -mode detail -p \u0026#34;incident_id=40165\u0026#34; ADR Home = /srv/xxx/oradata/yyyyy/diag/rdbms/yyyyy/yyyyy ********************************************************************** ************************************** INCIDENT INFO RECORD 1 ************************************** INCIDENT_ID 40165 STATUS ready PROBLEM_ID 4 ... The -mode option can be one of:\nBasic - simply lists the appropriate line(s) that you would see in a show incident command output. Not much use to be honest. Brief - gives a bit more detail that basic. Doesn\u0026rsquo;t include details of any trace files created for this incident. Detail - gives lots of information including details of all trace files created for this incident. The -p option is simply a where clause on any of the columns in the incident table. You can obtain a list of columns as follows:\n1 2 3 4 5 6 7 8 9 10 11 adrci\u0026gt; describe incident Name Type NULL? ------------- ------------- ------ INCIDENT_ID number PROBLEM_ID number CREATE_TIME timestamp CLOSE_TIME timestamp STATUS number FLAGS number .... The -p options can be ANDed and/or ORed together:\n1 adrci\u0026gt; show incident -p \u0026#34;incident_id = 40165 or incident_id = 145\u0026#34; Given that incident 40165 is an ORA-600, it should be sent off to Oracle Support for diagnosis, so I need to gather all the evidence together. Remember how that was done in the old days? Well, this is how simple it is with adrci:\nFirst, create a new package:\n1 2 adrci\u0026gt; ips create package Create package 7 without any contents, correlation level typical That creates a blank package, with nothing in it. You now add all the incidents you need to add:\n1 2 adrci\u0026gt; ips add incident 40165 package 7 Added incident 40165 to package 7 That has added the incident to the package\u0026rsquo;s metadata only, there are still no actual package files created. You may add further incidents to the package as desired.\nWhen done adding incidents, generate a physical package for support:\n1 2 adrci\u0026gt; ips generate package 7 Generate package 7 in /home/oracle/IPSPKG_20120820104439_COM_7.zip, mode complete The zip file name created contains everything Oracle Support will need, all the trace files, the alert log, various metadata files and so on. This file can be uploaded to Oracle Support.\nThe file is created in the current working directory. If you want to specify where to put it, the following command will help:\n1 2 adrci\u0026gt; ips generate package 7 in \u0026#39;/srv/xxx/oradata/yyyyy\u0026#39; Generate package 7 in /srv/xxx/oradata/yyyyy/IPSPKG_20120820104842_COM_7.zip, mode complete That concludes this very brief overview of the adrci utility. For more details check out the Database Administrator\u0026rsquo;s Guide, Chapter 9 - Managing Diagnostic Data Have fun.\n","description":"","id":111,"section":"posts","tags":null,"title":"Using Oracle 11g Adrci for Incident Reporting","uri":"http://localhost:1313/RantsAndRaves/posts/2012/08/using-oracle-11g-adrci-for-incident-reporting/"},{"content":"Installing Oracle Multimedia, which is required for Spatial and/or Locator is quite simple.\nAll of the following must be carried out while logged in as a SYSDBA user.\n1 2 3 4 5 6 7 8 9 10 11 SQL\u0026gt; spool ordinst.log SQL\u0026gt; @?/ord/admin/ordinst SYSAUX SYSAUX ... ... SQL\u0026gt; spool off SQL\u0026gt; spool catim.log SQL\u0026gt; @?/ord/im/admin/catim ... ... SQL\u0026gt; spool off Lots of stuff will scroll up the screen but will also be copied to the spool files named. Check those for obvious errors, then check to see if it worked as follows:\n1 2 3 4 5 6 7 SQL\u0026gt; select comp_id, version, status 2 from dba_registry 3 where comp_id = \u0026#39;ORDIM\u0026#39;; COMP_ID VERSION STATUS ---------- ---------- --------- ORDIM 11.2.0.3.0 VALID You can also force check the validity as follows, should you ever need to:\n1 2 3 4 SQL\u0026gt; set serveroutput on SQL\u0026gt; execute sys.validate_ordim; PL/SQL procedure successfully completed. If there are any invalid objects in the ORDIM component, the above procedure will display a message about them and set the status column in the DBA_REGISTRY to INVALID. You should check the status after running the above.\n","description":"","id":112,"section":"posts","tags":null,"title":"Installing Oracle Multimedia on 11g","uri":"http://localhost:1313/RantsAndRaves/posts/2012/08/installing-oracle-multimedia-on-11g/"},{"content":"So, you renamed your database using the nid utility as outlined here but now you need (or want) to change all the file system names to suit. Read on.\nIn the following example, we have two mount points for the database. Files on these mounts are spread all over a pile of separate discs making up the LUN - so it\u0026rsquo;s not as bad as it looks!\nThe two mounts are wrongly named at the moment since we changed the database name using nid and we would like to tidy things up. The current names are:\n/srv/old_name/oradata/old_sid/ /srv/old_name/flashback_area/old_sid What we would like to see after the renaming is the following:\n/srv/new_name/oradata/new_sid/ /srv/new_name/flashback_area/new_sid First of all, you need a current pfile. If you don\u0026rsquo;t have one already (from the rename exercise) then create one as follows:\n1 create pfile=\u0026#39;/home/oracle/initnew_sid.ora\u0026#39; from spfile; We also need a control file trace taking, so we do this next:\n1 alter database backup controlfile to trace as \u0026#39;/home/oracle/new_sid_controlfile.sql\u0026#39;; It should be obvious that you wouldn\u0026rsquo;t want the pfile and control file trace to be located on the file system that is about to be moved. If they are, you will have to wait for the Sys Admins to complete the remounting exercise before you can do your own editing in preparation for restarting the database. The next thing to do is backup the database. Shut it down and take a cold backup in your preferred manner - RMAN or some other utility. Better safe than sorry after all.\nIn a shell session, as the oracle user, remove (or move) the initnew_sid and spfilenew_sid.ora files, if present, from $ORACLE_HOME/dbs:\n1 rm $ORACLE_HOME/dbs/initnew_sid.ora` `rm $ORACLE_HOME/dbs/spfileold_sid.ora Edit the newly generated parameter file as required and move it from /home/oracle/initnew_sid.ora to $ORACLE_HOME/dbs/initnew_sid.ora. Things to change will include some or all of the following:\naudit_file_dest control_files db_recovery_file_dest diagnostic_dest log_archive_dest etc Anything that has been moved (or will be moved) by renaming the mount points will require changing.\nNext, the control file trace that was created needs to be edited. There is much waffle to be removed and a choice of script to generate. Edit the file /home/oracle/new_sid_controlfile.sql and:\nFind the section of the trace file that is applicable to your wish to start the database with RESETLOGS or not. For NORESETLOGS it is the first section, near the top of the trace, otherwise it is the second section near the bottom.\nFor my own databases, I have chosen not to reset the logs, so I use the top section of the trace file.\nDelete all the comment lines above the section you want. You need to start the script from the STARTUP NOMOUNT command in the appropriate section.\nScroll down, but retain all the lines from (and including) STARTUP NOMOUNT until you find a comment line that looks something like -- End of tempfile additions. Delete that line, and everything else to the end of file.\nYou should now be in possession of a script that begins with STARTUP NOMOUNT and ends with a few lines adding your TEMPFILEs back into your temporary tablespace9s).\nRun a global search and replace to ensure that all filenames match with the new locations. If the database is in ARCHIVELOG mode, pay particular attention to the commented out commands to ALTER DATABASE REGISTER LOGFILE and make sure you enter the full path to an existing log file. You can ignore this part if your database is running in NOARCHIVELOG mode. Save the file. When the Sys Admins have given you back the file systems mounted on their new locations, it\u0026rsquo;s a simple matter to:\n1 2 connect / as sysdba @/home/oracle/new_sid_controlfile.sql When that has finished, everything should now be in order. You may create a new spfile in the usual manner:\n1 2 3 4 5 6 create spfile=\u0026#39;/home/oracle/spfilenew_sid.ora\u0026#39; from pfile; shutdown ... host cp /home/oracle/spfilenew_sid.ora $ORACLE_HOME/dbs/ startup ... Nothing to it!\n","description":"","id":113,"section":"posts","tags":null,"title":"Rename an Oracle 10g or 11g Database - Part 2","uri":"http://localhost:1313/RantsAndRaves/posts/2012/08/rename-an-oracle-10g-or-11g-database-part-2/"},{"content":"As you already know, an Oracle database\u0026rsquo;s PMON process will register your database with a listener without you having to do anything about it. However \u0026hellip;\nThis will only happen if the listener in question is running on port 1521. And it doesn\u0026rsquo;t have to be named LISTENER either \u0026ndash; as I mistakenly thought\u0026ndash; it only has to be port 1521.\nIf you have a listener running on port 1521, and you have databases configured to connect via different listeners on other ports (on the same server) then your databases will be grabbed by the 1521 listener!\nThe following shows the output from Lsnrctl status for a listener running on port 1525. You can see that it is not handling anything at all. (Names changed to protect the guilty):\n1 2 3 4 5 6 7 8 ... Listening Endpoints Summary... (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=test_server)(PORT=1525))) Services Summary... Service \u0026#34;test5\u0026#34; has 1 instance(s). Instance \u0026#34;test5\u0026#34;, status UNKNOWN, has 1 handler(s) for this service... The command completed successfully Meanwhile, the listener listening on port 1521 has indeed grabbed everything! And as the tnsnmaes.ora specifies port 1525 for the test5 database, we will not connect!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ... Listening Endpoints Summary... (DESCRIPTION=(ADDRESS=(PROTOCOL=tcp)(HOST=test_server)(PORT=1521))) Services Summary... Service \u0026#34;test1\u0026#34; has 1 instance(s). Instance \u0026#34;test1\u0026#34;, status READY, has 1 handler(s) for this service... Service \u0026#34;test1XDB\u0026#34; has 1 instance(s). Instance \u0026#34;test1\u0026#34;, status READY, has 1 handler(s) for this service... Service \u0026#34;test2\u0026#34; has 1 instance(s). Instance \u0026#34;test2\u0026#34;, status UNKNOWN, has 1 handler(s) for this service... Service \u0026#34;test3.world\u0026#34; has 1 instance(s). Instance \u0026#34;test3\u0026#34;, status READY, has 1 handler(s) for this service... Service \u0026#34;test4\u0026#34; has 1 instance(s). Instance \u0026#34;test4\u0026#34;, status READY, has 1 handler(s) for this service... Service \u0026#34;test5\u0026#34; has 1 instance(s). Instance \u0026#34;test5\u0026#34;, status READY, has 1 handler(s) for this service... The command completed successfully If you want to prevent this from happening, you can add the following one line to the listener.ora for the listener listening on port 1521:\n1 DYNAMIC_REGISTRATION_lsnr_name = off That way, the listener is prevented from accepting dynamic registration requests from the other databases\u0026rsquo; PMON process.\n","description":"","id":114,"section":"posts","tags":null,"title":"Beware of Listener Port 1521 and Dynamic Registration","uri":"http://localhost:1313/RantsAndRaves/posts/2012/08/beware-of-listener-port-1521-and-dynamic-registration/"},{"content":"It was my own fault, but in case it proves even slightly useful\u0026hellip;.\nI was upgrading from 11202 to 11203 Enterprise using the DB Upgrade Assistant utility. When I said \u0026lsquo;go do it\u0026rsquo; it went off, chugged for a bit, then barfed. DBUA informed me that the database wasn\u0026rsquo;t running. I checked, it was.\nCutting a long story short, I checked the indicated logfile and discovered that DBUA had connected to the database but then got a couple of errors telling it that \u0026lsquo;oracle was not available\u0026rsquo;. Hmm.\nTurns out that I\u0026rsquo;d added a couple of SQL statements to glogin.sql, with\n1 alter session set nls_date_format = \u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39;; being one of the offending statements. This works perfectly as long as you connect while the database(s) are open but, if a database is shut, and you login as sysdba to start it, glogin.sql is still executed, and because the database is down, the SQL statement(s) fail.\nDBUA picked up the failure and refused to carry on.\nRemoving the SQL statement(s) from glogin.sql fixed the problem.\nThe moral to this tale is simple, don\u0026rsquo;t put SQL in glogin.sql (or login.sql) if you ever shut down your databases!\nIn case you are wondering, glogin.sql lives in $ORACLE_HOME/sqlplus/admin and will be executed on each successful connection to a database with SQL*Plus. Login.sql will also be executed on every successful connection to a database (at least, from 10g onwards) after glogin.sql, but only if it is found on $SQLPATH.\n","description":"","id":115,"section":"posts","tags":null,"title":"How to Screw Up DB Upgrade Assistant","uri":"http://localhost:1313/RantsAndRaves/posts/2012/06/how-to-screw-up-db-upgrade-assistant/"},{"content":"Locator is \u0026ldquo;Spatial Lite\u0026rdquo; if you wish, and costs nothing. It can be installed in Standard or Enterprise Editions with no additional licensing costs. You cannot do everything in Locator that you can in Spatial - but what do you expect for free? ;-)\nTo install Locator you need to be aware that things changed at 11g, so what used to work on 10g no longer does on 11g and can lead to you having a huge mess of objects and types to hunt down and remove from your SYS account - ask me how I know!\nNote: If you have already installed Oracle Multimedia, the MDSYS user will already exists. In this case, simply skip the create user command in the following.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SQL\u0026gt; connect / as sysdba SQL\u0026gt; create user mdsys identified by secret 2 default tablespace sysaux; SQL\u0026gt; @?/md/admin/mdprivs.sql SQL\u0026gt; connect mdsys/secret SQL\u0026gt; @?/md/admin/catmdloc SQL\u0026gt; connect / as sysdba SQL\u0026gt; alter user mdsys 2 account lock 3 password expire; You can read the details about Locator in the file $ORACLE_HOME/md/doc/README_LOCATOR.txt - but don\u0026rsquo;t expect too much assistance, it doesn\u0026rsquo;t even mention how to install it!\nYou won\u0026rsquo;t be able to find Locator in DBA_REGISTRY nor will you be able to find it in V$OPTION either.\n1 2 3 4 5 6 7 8 9 10 11 SQL\u0026gt; col parameter format a20 SQL\u0026gt; col value format a5 SQL\u0026gt; select parameter, value 2 from v$option 3 where parameter like \u0026#39;Spatial%\u0026#39; 4 or parameter like \u0026#39;Locator%\u0026#39;; PARAMETER VALUE -------------------- ----- Spatial FALSE You can see that Locator doesn\u0026rsquo;t turn up in the output - that\u0026rsquo;s because it\u0026rsquo;s not an option, unlike Spatial.\nMy Oracle Support (MOS) has a document that explains how to detect if Locator is installed. The document id is 357943.1. Unfortunately, the process explained doesn\u0026rsquo;t work. Sigh.\nYou cannot even assume Locator is installed if Spatial is not installed and you have located a schema named MDSYS which owns objects, because installing Oracle Multimedia creates the MDSYS user if it is not there already.\nInstalling Spatial requires that you are running Enterprise Edition and that you didn\u0026rsquo;t deselect it when installing the software originally. If you did, you need to re-run the installer and when you see the Enterprise Options button, click it and select the Spatial component.\nIf you don\u0026rsquo;t do this then the file catmd.sql and it\u0026rsquo;s associated helpers, will not be copied to $ORACLE_HOME/md/admin.\nSpatial costs extra dosh and must be licensed separately from your Enterprise Edition software.\n1 2 3 4 5 6 7 8 9 10 SQL\u0026gt; connect / as sysdba SQL\u0026gt; create user mdsys identified by secret 2 default tablespace sysaux 3 account lock 4 password expire; SQL\u0026gt; @?/md/admin/mdprivs.sql SQL\u0026gt; @?/md/admin/catmd You can read the details about Spatial in the file $ORACLE_HOME/md/doc/README.txt.\nAs with Locator, you won\u0026rsquo;t find Spatial in DBA_REGISTRY so, as before, you must look in V$OPTION.\n1 2 3 4 5 6 7 8 9 10 SQL\u0026gt; col parameter format a20 SQL\u0026gt; col value format a5 SQL\u0026gt; select parameter, value 2 from v$option 3 where parameter like \u0026#39;Spatial%\u0026#39;; PARAMETER VALUE -------------------- ----- Spatial TRUE ","description":"","id":116,"section":"posts","tags":null,"title":"Installing Locator or Spatial on 11g","uri":"http://localhost:1313/RantsAndRaves/posts/2012/05/installing-locator-or-spatial-on-11g/"},{"content":"In the previous instalment of this exciting series, we completed the full (complete) recovery of the database, tablespaces and data files, as well as looking at recovering individual blocks.\nIn this article, we will perform incomplete recovery where we restore and recover the databases to a specific point in time that is previous to \u0026ldquo;now\u0026rdquo;.\nNote that in the following examples I\u0026rsquo;m using the until time option and specifying a date and time. You can restore to a particular SCN, or archive log number etc. See the manual for all the options.\nIt is possible that data will be lost with this kind of recovery. It is not possible to perform incomplete recovery on tablespaces or data files, you are limited to restoring and recovering the entire database. Although, this is not quite true, read on \u0026hellip;\nIncomplete Recovery of the Database As the whole database is being restored, the SYSTEM and UNDO tablespaces will be restored and recovered, so the database will need to be in a mounted state. You cannot carry out incomplete recovery on an open database.\nFirst of all, let\u0026rsquo;s add a new row to our test table which we use to show progress etc.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 SQL\u0026gt; alter session set nls_date_format=\u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39;; Session altered. SQL\u0026gt; insert into test values (sysdate); 1 row created. SQL\u0026gt; commit; Commit complete. SQL\u0026gt; select * from test order by 1; A ------------------- 06/02/2012 20:25:11 16/02/2012 17:26:20 17/02/2012 15:03:42 17/02/2012 15:54:49 07/03/2012 11:21:53 19/03/2012 20:58:18 15/05/2012 13:44:29 7 rows selected. Now, restore and recover the database back to a time that is about 15 minutes ago. The first stage is to get the database into a mount state.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 rman target / RMAN\u0026gt; shutdown; database closed database dismounted Oracle instance shut down RMAN\u0026gt; startup mount connected to target database (not started) Oracle instance started database mounted Total System Global Area 768331776 bytes Fixed Size 2230360 bytes Variable Size 213911464 bytes Database Buffers 549453824 bytes Redo Buffers 2736128 bytes Next, we restore the database to our chosen time.\n1 RMAN\u0026gt; restore database until time \u0026#34;to_date(\u0026#39;15/05/2012 13:30:00\u0026#39;,\u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39;)\u0026#34;; 1 2 3 4 5 6 7 8 9 10 11 12 Starting restore at 15/05/2012 14:08:47 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=19 device type=DISK channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00002 to /srv/nffs/oradata/ant12/data/sysaux01.dbf ... ... channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 Finished restore at 15/05/2012 14:13:04 I\u0026rsquo;ve trimmed the usual RMAN verbose output from the above to save you falling asleep reading it! ;-)\nNext, we need to recover the database to the same time.\n1 RMAN\u0026gt; recover database until time \u0026#34;to_date(\u0026#39;15/05/2012 13:30:00\u0026#39;,\u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39;)\u0026#34;; 1 2 3 4 5 6 7 8 9 10 Starting recover at 15/05/2012 14:16:35 using channel ORA_DISK_1 starting media recovery archived log for thread 1 with sequence 12 is already on disk as file /srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_12_7mwyxlk8_.arc ... ... media recovery complete, elapsed time: 00:00:36 Finished recover at 15/05/2012 14:17:24 Again, I\u0026rsquo;ve trimmed the output. RMAN will restore to disc any archived logs it requires to carry out the recovery if it determines that they are not available online.\nThe final stage is to reopen the database. You must use the resetlogs option because you\u0026rsquo;ve carried out an incomplete recovery.\n1 2 RMAN\u0026gt; alter database open resetlogs; database opened So, the restore and recovery went well, did it really work?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SQL\u0026gt; conn norman/norman Connected. SQL\u0026gt; alter session set nls_date_format=\u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39;; Session altered. SQL\u0026gt; select * from test; A ------------------- 07/03/2012 11:21:53 19/03/2012 20:58:18 06/02/2012 20:25:11 16/02/2012 17:26:20 17/02/2012 15:03:42 17/02/2012 15:54:49 6 rows selected. So, our most recent row in the table is no longer present, we now have 6 rows instead of the 7 that we had at the start of this recovery. The database has been restored back in time.\nYou will note that in both the restore database and the recover database commands, I had to specify the same time. There is a short cut that you can use to save typing.\n1 2 3 4 5 RMAN\u0026gt; run { 2\u0026gt; set until time = \u0026#34;to_date(\u0026#39;15/05/2012 13:30:00\u0026#39;,\u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39;)\u0026#34;; 3\u0026gt; restore database; 4\u0026gt; recover database; 5\u0026gt; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 executing command: SET until clause Starting restore at 15/05/2012 14:02:23 using channel ORA_DISK_1 ... ... Finished restore at 15/05/2012 14:02:24 Starting recover at 15/05/2012 14:02:24 using channel ORA_DISK_1 ... ... media recovery complete, elapsed time: 00:00:02 Finished recover at 15/05/2012 14:02:26 Now you can open the database in resetlogs mode as above.\nRestoring a Database Through a Resetlogs In the past it wasn\u0026rsquo;t possible to restore a database that had been opened with resetlogs specified. RMAN used to treat this as a new incarnation of the database, so you had to make sure you took a fresh backup as soon as the database was opened.\nRMAN in 11g is much more forgiving and it is now possible to restore through a resetlogs. You do this by restoring a backup from the previous incarnation and do a recover. If you watch the recovery phase carefully, you will see the archived logs files being applied from before and after the resetlogs.\nThe following assume that you haven\u0026rsquo;t taken a backup of the new incarnation yet. If you have, you must restore the controlfile from a named backup as opposed to letting RMAN work it out.\nThe first step is to restore the controlfile.\n1 2 3 4 5 RMAN\u0026gt; shutdown immediate; database closed database dismounted Oracle instance shut down 1 2 3 4 5 6 7 8 9 10 11 RMAN\u0026gt; startup nomount; connected to target database (not started) Oracle instance started Total System Global Area 768331776 bytes Fixed Size 2230360 bytes Variable Size 213911464 bytes Database Buffers 549453824 bytes Redo Buffers 2736128 bytes 1 2 3 4 5 6 7 8 RMAN\u0026gt; restore controlfile from autobackup; Starting restore at 15/05/2012 15:12:57 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=17 device type=DISK ... ... Finished restore at 15/05/2012 15:13:03 1 2 3 4 RMAN\u0026gt; alter database mount; database mounted released channel: ORA_DISK_1 Now that we are on an old controlfile and the database is mounted, we can restore and recover the database in the normal manner.\n1 RMAN\u0026gt; restore database; 1 2 3 4 5 6 7 8 Starting restore at 15/05/2012 15:15:06 Starting implicit crosscheck backup at 15/05/2012 15:15:06 allocated channel: ORA_DISK_1 ... ... channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 Finished restore at 15/05/2012 15:19:27 Now all we have to do is the recover. As mentioned above, watch the thread and sequence numbers closely.\n1 RMAN\u0026gt; recover database; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Starting recover at 15/05/2012 15:19:36 using channel ORA_DISK_1 starting media recovery ... ... archived log ... thread=1 sequence=8 archived log ... thread=1 sequence=9 ... ... archived log ... thread=1 sequence=20 archived log ... thread=1 sequence=1 media recovery complete, elapsed time: 00:00:52 Finished recover at 15/05/2012 15:20:42 You can hopefully see, in the above, that the sequence number dropped from 20 to 1 as we recovered through the resetlogs. Talking of which, we need another one now.\n1 2 RMAN\u0026gt; alter database open resetlogs; database opened Tablespace Incomplete Recovery It was never possible to do a tablespace point in time recovery without cloning the database first and using the clone to export the data. However, with 11g it is now possible to perform incomplete recovery on a tablespace (or tablespaces) as RMAN takes care of all the cloning etc.\nFirst of all, create a working area for the clone, as root.\n1 2 3 4 5 6 7 8 mkdir /media/oracle_backups/working_aux chown oracle:oinstall /media/oracle_backups/working_aux/ ls -l /media/oracle_backups/ total 24 drwxr-xr-x 2 oracle oinstall 4096 Feb 17 16:51 ant12 drwx------ 2 root root 16384 Feb 6 08:07 lost+found drwxr-xr-x 2 oracle oinstall 4096 May 15 15:34 working_aux Next, check our test table to see how far back we can recover the users tablespace.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SQL\u0026gt; conn norman/norman Connected. SQL\u0026gt; alter session set nls_date_format=\u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39;; Session altered. SQL\u0026gt; select * from test; A ------------------- 07/03/2012 11:21:53 19/03/2012 20:58:18 06/02/2012 20:25:11 16/02/2012 17:26:20 17/02/2012 15:03:42 17/02/2012 15:54:49 6 rows selected. Obviously your decision criteria for the actual time to restore back to will be a little more scientific than mine! But I\u0026rsquo;m going to select a date of just after 16/02/2012 17:26:20, lets call it 16/02/2012 17:30:00.\nThe first problem we may happen across is whether there are any objects in the tablespace to be restored which have constraints etc in other tablespaces. We check the SYS.TS_PITR_CHECK.\n1 2 3 4 5 6 7 SQL\u0026gt; conn / as sysdba Connected. SQL\u0026gt; select * from ts_pitr_check 2 where (ts1_name = \u0026#39;USERS\u0026#39; and ts2_name \u0026lt;\u0026gt; \u0026#39;USERS\u0026#39;) or 3 (ts2_name = \u0026#39;USERS\u0026#39; and ts1_name \u0026lt;\u0026gt; \u0026#39;USERS\u0026#39;); no rows selected In this case as we are about to restore the users tablespace, we have to see if there are relationships from users to other tablespaces and also if there are relationships from other tablespaces to users.\nThere are none, but if there had been, we would either need to temporarily drop those constraints or include the other tablespace(s) in the recovery.\nThe next problem is those objects that exist in the database now, that didn\u0026rsquo;t at the time of the recovery point in time. Again we can find a list of those by checking SYS.TS_PITR_OBJECTS_TO_BE_DROPPED and giving the date and time we intend to use.\n1 2 3 4 5 SQL\u0026gt; select tablespace_name, owner, name 2 from ts_pitr_objects_to_be_dropped 3 where tablespace_name = \u0026#39;USERS\u0026#39; 4 and creation_time \u0026gt; to_date(\u0026#39;16/02/2012 17:30:00\u0026#39;,\u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39;); no rows selected So far so good. Had there been any objects listed, we would need to run an export of those objects using data pump to preserve them across the recovery.\nWe are now ready to carry out a point in time restore of the users tablespace to the working area instead of to the normal database area. You do not need to take the tablespace offline, RMAN does that for you.\nNote, in the following command _recover_ is correct. Do not use _restore_ as it will not work.\n1 2 RMAN\u0026gt; recover tablespace users until time \u0026#34;to_date(\u0026#39;16/02/2012 17:30:00\u0026#39;,\u0026#39;dd/mm/yyyy hh24:mi:ss\u0026#39;)\u0026#34; 2\u0026gt; auxiliary destination \u0026#39;/media/oracle_backups/working_area\u0026#39;; 1 2 3 4 5 6 7 8 9 10 11 12 13 Starting recover at 15/05/2012 15:55:52 using channel ORA_DISK_1 Creating automatic instance, with SID=\u0026#39;qvbt\u0026#39; initialization parameters used for automatic instance: db_name=NORM db_unique_name=qvbt_tspitr_NORM ... ... Removing automatic instance Automatic instance removed Finished recover at 15/05/2012 15:58:42 You will note that there is a lot of output from RMAN as it builds a clone database and recovers the tablespace to the specified time.\nYou cannot recover a tablespace to a time previous to the last resetlogs time. If you try, you will get the following error message:\nRMAN-03002: failure of recover command at 05/15/2012 15:55:53\nRMAN-20207: UNTIL TIME or RECOVERY WINDOW is before RESETLOGS time\nAfter the tablespace has been cloned and recovered, you must back it up and then bring it online.\n1 2 3 4 RMAN\u0026gt; backup tablespace users; ... RMAN\u0026gt; sql \u0026#39;alter tablespace users online\u0026#39;; ... If you do not have an RMAN catalogue in use, you are not permitted to carry out more than one of these tablespace point in time recovery operations because the controlfile no longer knows about the previous incarnations of the recovered tablespace(s).\nThis is one of the reasons why you must backup the tablespace immediately after recovering it.\nIf, on the other hand, you do use a catalogue, then multiple recoveries of the same tablespace(s) are permitted.\n","description":"","id":117,"section":"posts","tags":null,"title":"Oracle RMAN for Beginners - Part 8","uri":"http://localhost:1313/RantsAndRaves/posts/2012/05/oracle-rman-for-beginners-part-8/"},{"content":"We have foxes in our garden in the mornings and evenings. We seem to have a dog and vixen and three cubs. This morning they were cavorting around in the sun and I managed to get a few pictures taken before they headed off to wherever they lie up during the day.\nThis is my favourite shot:\nNext up, we have one of the vixen and one cub:\nThen one of the cubs playing together. They are so quick, it\u0026rsquo;s hard to get a decent focus. This shot is definitely out of focus, but high on cuteness!\nAnd finally, for now anyway, one of the whole family together. Something in the vixen\u0026rsquo;s look tells me she might be a tad fed up of all these kids!\nBy the way, all shots taken on a Konica Minolta Dynax 5D fitted with a Minolta 75-300 Zoom and a 2* doubler. As the camera won\u0026rsquo;t quite autofocus at the top end of the zoom with the doubler attached, I was on manual focus.\nThese images are poor quality jpegs rather than the high quality raw format we normally shoot in.\nNo foxes were harmed in the taking of these photos.\n","description":"","id":118,"section":"posts","tags":null,"title":"I'm Quite Pleased With This Photo","uri":"http://localhost:1313/RantsAndRaves/posts/2012/05/im-quite-pleased-with-this-photo/"},{"content":"Itâs supposed to be installed by default, according to the documentation, but for some reason or another, I managed to build a brand new 11.2 database, on Linux, with no CTXSYS user present.\nInstalling Context Hereâs how to install Oracle Text and the English language defaults, into an 11.2 database.\n1 2 3 4 5 6 7 8 9 10 11 12 13 SQL\u0026gt; connect / as SYSDBA SQL\u0026gt; spool ctxsys_installation.log -- Parameters are password, default t/s, temp t/s, don\u0026#39;t lock the account. SQL\u0026gt; @?/ctx/admin/catctx.sql secret SYSAUX TEMP NOLOCK SQL\u0026gt; connect CTXSYS/secret SQL\u0026gt; @?/ctx/admin/defaults/dr0defin.sql \u0026#34;ENGLISH\u0026#34;; SQL\u0026gt; connect / as SYSDBA SQL\u0026gt; alter user ctxsys account lock password expire; SQL\u0026gt; spool off Removing Context Hereâs how to remove Oracle Text from an 11.2 database.\n1 2 3 4 5 6 SQL\u0026gt; connect / as SYSDBA SQL\u0026gt; spool ctxsys_removal.log SQL\u0026gt; @?/ctx/admin/catnoctx.sql SQL\u0026gt; spool off ","description":"","id":119,"section":"posts","tags":null,"title":"Oracle Text aka CONTEXT - Installing on 11g","uri":"http://localhost:1313/RantsAndRaves/posts/2012/05/oracle-text-aka-context-installing-on-11g/"},{"content":"When installing the Pro*C compiler stuff (technical term) from the 11.2.0.3 client install disc (it\u0026rsquo;s in zipfile 4 or 7 in case you need to know!) I hit a strange error almost immediately the installer started the GUI.\nI don\u0026rsquo;t have a screen dump, but the error message was the following:\nvbOEL.dunbar-it.co.uk:vbOEL.dunbar-it.co.uk\nIn other words, my fully qualified Linux x86-64bit server name, vbOEL.dunbar-it.co.uk, twice. Interesting.\nNo buttons were present to allow me to carry on, abort etc. Just an OK button which is not much help as it bales out of the installer when clicked.\nGoogling and DuckDuckGoing (will that ever be a verb?) found me nothing.\nAs it turned out, I was using the wrong CD as I should have been putting in the 11.2.0.2 version. Once I mounted the correct CD image, I started again. This time I got an error dialoge with a bit more information:\n[INS-06101] IP address of localhost could not be determined.\nAre you sure you wish to continue?\nAnd this time, I have buttons for YES and NO. I clicked YES and all was well.\nIt looks like 11.2.0.3 is a tad flaky in the error message/dialogue department.\nCheers, Norm.\n","description":"","id":120,"section":"posts","tags":null,"title":"Weird Error Dialogue when Installing Oracle 11.2.0.3 Client?","uri":"http://localhost:1313/RantsAndRaves/posts/2012/04/weird-error-dialoge-when-installing-oracle-11-2-0-3-client/"},{"content":"I\u0026rsquo;ve just created a new database on an HP-UX server. The database is Oracle 9i (yes, I know, I know!) and no matter what I do, I can\u0026rsquo;t connect via the listener without getting the dreaded ORA-12505: TNS:listener could not resolve SID given in connect descriptor error message.\nI\u0026rsquo;ve done this lots of times in the past, but for some reason, I can\u0026rsquo;t get it to work today. It\u0026rsquo;s driving me mad!\nThe reason I have to use a local_listener parameter on HP-UX is because the hostname and the uname -n commands don\u0026rsquo;t return the same result as the hostname is longer than 8 characters:\n1 2 3 4 5 SQL\u0026gt; !hostname myserver0011 SQL\u0026gt; !uname -n myserver So, we have to use a local_listener setting in the spfile, and register the database with that listener because we can\u0026rsquo;t connect with user/password@alias otherwise.\n1 2 3 4 SQL\u0026gt; connect my_user/secret@my9idb ERROR: ORA-12505: TNS:listener could not resolve SID given in connect descriptor Warning: You are no longer connected to ORACLE. Hmmm, it works fine without using the listener:\n1 2 SQL\u0026gt; conn / as sysdba Connected. Some sanity checks:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 SQL\u0026gt; show parameter db_name NAME TYPE VALUE ------------------------------------ ----------- ------------------------------ db_name string my9idb SQL\u0026gt; show parameter local_listener NAME TYPE VALUE ------------------------------------ ----------- ------------------------------ local_listener string (ADDRESS=(PROTOCOL=TCP)(HOST=m yserver0011)(PORT=1556)) SQL\u0026gt; !tnsping my9idb 1 2 3 4 5 6 7 8 9 10 TNS Ping Utility for HPUX: Version 9.2.0.6.0 - Production on 23-APR-2012 16:12:30 Copyright (c) 1997 Oracle Corporation. All rights reserved. Used parameter files: /opt/oracle/product/9.2.0.6/db/network/admin/sqlnet.ora Used TNSNAMES adapter to resolve the alias Attempting to contact (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=myserver0011)(PORT=1556)) (CONNECT_DATA=(SID=my9idb)(SERVER=DEDICATED))) OK (10 msec) 1 2 3 4 5 6 7 8 9 SQL\u0026gt; !ping myserver0011 PING myserver0011: 64 byte packets 64 bytes from 10.55.127.122: icmp_seq=0. time=0. ms 64 bytes from 10.55.127.122: icmp_seq=1. time=0. ms 64 bytes from 10.55.127.122: icmp_seq=2. time=0. ms ----myserver0011 PING Statistics---- 3 packets transmitted, 3 packets received, 0% packet loss round-trip (ms) min/avg/max = 0/0/0 1 SQL\u0026gt; !lsnrctl services lsnr_my9idb 1 2 3 4 5 6 7 8 LSNRCTL for HPUX: Version 9.2.0.6.0 - Production on 23-APR-2012 16:15:51 Copyright (c) 1991, 2002, Oracle Corporation. All rights reserved. Connecting to (DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=myserver0011)(PORT=1556))) The listener supports no services The command completed successfully The clue was in the second last line, the listener supports no services. But I missed it the first time I checked, and the second, third \u0026hellip;..\nIt eventually dawned on me:\n1 2 3 4 5 6 7 8 SQL\u0026gt; alter system set service_names=\u0026#39;my9idb\u0026#39; scope=both; System altered. SQL\u0026gt; alter system register; System altered. SQL\u0026gt; conn myuser/secret@my9idb Connected. Result!\nIt appears that the alter system register command isn\u0026rsquo;t required, as soon as I set the service_names parameter, the listener seems to notice. However, I\u0026rsquo;m taking no chances after today!\nAll usernames, passwords, IP addresses and server names in this article have been disguised to protect the innocent!\n","description":"","id":121,"section":"posts","tags":null,"title":"Oracle 9i, HP-UX and ORA-12505 Drives Me Mad!","uri":"http://localhost:1313/RantsAndRaves/posts/2012/04/oracle-9i-hp-ux-and-ora-12505-drives-me-mad/"},{"content":"This one has been around for a while, but it\u0026rsquo;s still amusing. Other opinions are available \u0026hellip;\n40â: Californians shiver uncontrollably. People in Scotland sunbathe.\n35â: Italian cars won\u0026rsquo;t start. People in Scotland drive with the windows down.\n20â: Floridians wear coats, gloves, and wool hats. People in Scotland throw on a T shirt.\n15â: Californians begin to evacuate the state. People in Scotland go swimming.\n0â: New York landlords finally turn up the heat. People in Scotland have the last bbq before it gets cold.\n10â below zero: People in Miami cease to exist. People in Scotland lick flagpoles.\n20â below zero: Californians fly away to Mexico. People in Scotland throw on a light jacket.\n80â below zero: Polar bears begin to evacuate the Arctic. Scottish Boy Scouts postpone \u0026ldquo;Winter Survival\u0026rdquo; classes until it gets cold enough.\n100â below zero: Santa Claus abandons the North Pole. People in Scotland pull down their ear flaps.\n173â below zero: Ethyl alcohol freezes. People in Scotland get frustrated when they can\u0026rsquo;t thaw their kegs.\n297â below zero: Microbial life start to disappear. Scottish cows complain of farmers with cold hands.\n460â below zero: ALL atomic motion stops. People in Scotland start saying \u0026ldquo;chilly th\u0026rsquo;day, you cauld an aw?\u0026rdquo; (Translation: \u0026ldquo;It\u0026rsquo;s chilly today, are you cold as well?\u0026rdquo;)\n500â below zero: Hell freezes over. Scottish people support England in World Cup!!!!\n","description":"","id":122,"section":"posts","tags":null,"title":"Scotsmen \u0026 the Weather.","uri":"http://localhost:1313/RantsAndRaves/posts/2012/04/scotsmen-the-weather/"},{"content":"It\u0026rsquo;s a post-installation configuration that hasn\u0026rsquo;t been done. The /etc/sudoers file has the following section in it:\nIn the default (unconfigured) configuration, sudo asks for the root password.\nThis allows use of an ordinary user account for administration of a freshly\ninstalled system. When configuring sudo, delete the two following lines:\n1 2 Defaults targetpw # ask for the password of the target user i.e. root ALL ALL=(ALL) ALL # WARNING! Only use this together with \u0026#39;Defaults targetpw\u0026#39;! The administrator needs to edit /etc/sudoers, using the visudo command, and comment out the final two lines above, or remove them altogether. Once done, individual user accounts can be added as follows:\n1 2 oracle ALL=(ALL) ALL norman ALL=(ALL) ALL Alternatively, groups can be given access as follows:\n1 2 %dba ALL=(ALL) ALL %oinstall ALL=(ALL) ALL Substituting the appropriate levels of commands and privileges of course!\n","description":"","id":123,"section":"posts","tags":null,"title":"Sudo on SLES Asking for Root Password?","uri":"http://localhost:1313/RantsAndRaves/posts/2012/03/sudo-on-sles-asking-for-root-password/"},{"content":"In part 6 of this mini-series, I left you with a backed up database, having using RMAN to take a hot backup. This episode looks at restoring and recovering from hot backups.\nThe joy of this is that most of the time you don\u0026rsquo;t need to have everyone off of the database twiddling their thumbs while you restore and recover, just anyone in those areas affected.\nRestoration and recovery is full in that the database will be completely recovered right up to date after the restore and recovery is finished.\nRecover the Entire Database To recover the entire database you do actually need to have the database mounted so the users will need to be offline. The steps involved are:\nMake sure that the database is mounted, not open:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 RMAN\u0026gt; shutdown database closed database dismounted Oracle instance shut down RMAN\u0026gt; startup mount connected to target database (not started) Oracle instance started database mounted Total System Global Area 768331776 bytes Fixed Size 2230360 bytes Variable Size 213911464 bytes Database Buffers 549453824 bytes Redo Buffers 2736128 bytes Restore the database from a suitable dump. RMAN will choose a suitable dump, or dumps, for you in order to reduce the amount of work required:\n1 RMAN\u0026gt; restore database; 1 2 3 4 5 6 7 8 9 10 11 Starting restore at 2012/03/19 21:19:35 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=17 device type=DISK channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00002 to /srv/nffs/oradata/ant12/data/sysaux01.dbf channel ORA_DISK_1: restoring datafile 00003 to /srv/nffs/oradata/ant12/data/undotbs01.dbf ... channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 Finished restore at 2012/03/19 21:24:02 Recover the database next. This will use any available backups of archived logs, the currently online and possibly never backed up archived logs plus the online redo logfiles to bring the database right up to date. Any archived logs that are no longer online will be restored first - which you can see in the following if you looks carefully:\n1 RMAN\u0026gt; recover database; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 Starting recover at 2012/03/19 21:28:58 using channel ORA_DISK_1 starting media recovery archived log for thread 1 with sequence 12 is already on disk as file /srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_12_7mwyxlk8_.arc archived log for thread 1 with sequence 13 is already on disk as file /srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_13_7mwzxtxg_.arc ... channel ORA_DISK_1: starting archived log restore to default destination channel ORA_DISK_1: restoring archived log archived log thread=1 sequence=8 channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_17/o1_mf_annnn_TAG20120217T160332_7mwylo3r_.bkp ... channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 ... archived log file name=/srv/nffs/flashback_area/ant12/NORM/archivelog/2012_03_19/o1_mf_1_9_7ph990ph_.arc thread=1 sequence=9 channel default: deleting archived log(s) archived log file name=/srv/nffs/flashback_area/ant12/NORM/archivelog/2012_03_19/o1_mf_1_9_7ph990ph_.arc RECID=77 STAMP=778368544 archived log file name=/srv/nffs/flashback_area/ant12/NORM/archivelog/2012_03_19/o1_mf_1_10_7ph990r7_.arc thread=1 sequence=10 channel default: deleting archived log(s) archived log file name=/srv/nffs/flashback_area/ant12/NORM/archivelog/2012_03_19/o1_mf_1_10_7ph990r7_.arc RECID=76 STAMP=778368544 channel ORA_DISK_1: starting archived log restore to default destination channel ORA_DISK_1: restoring archived log archived log thread=1 sequence=11 channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_17/o1_mf_annnn_TAG20120217T160922_7mwyxlvn_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_17/o1_mf_annnn_TAG20120217T160922_7mwyxlvn_.bkp tag=TAG20120217T160922 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 archived log file name=/srv/nffs/flashback_area/ant12/NORM/archivelog/2012_03_19/o1_mf_1_11_7ph996wn_.arc thread=1 sequence=11 channel default: deleting archived log(s) archived log file name=/srv/nffs/flashback_area/ant12/NORM/archivelog/2012_03_19/o1_mf_1_11_7ph996wn_.arc RECID=78 STAMP=778368551 archived log file name=/srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_12_7mwyxlk8_.arc thread=1 sequence=12 archived log file name=/srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_13_7mwzxtxg_.arc thread=1 sequence=13 archived log file name=/srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_14_7mwzxx1n_.arc thread=1 sequence=14 archived log file name=/srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_15_7mwzxxkb_.arc thread=1 sequence=15 media recovery complete, elapsed time: 00:00:25 Finished recover at 2012/03/19 21:29:36 And finally, open the database:\n1 2 RMAN\u0026gt; alter database open; database opened Recover Tablespaces Recovering individual tablespaces is done with the database open. Only the tablespaces to be recovered need to be offline.\nHowever, if the SYSTEM or UNDO tablespaces need to be recovered, the database will need to be mounted as you cannot restore and recover those with the database online - for pretty obvious reasons to be honest!\nYou cannot recover a tablespace that has been dropped. In that situation, you must perform a point in time recovery to just before the tablespace was dropped. The control file doesn\u0026rsquo;t keep details of the dropped tablespace.\nAttempting to recover a dropped tablespace will result in RMAN-20202 Tablespace not found in the recovery catalog errors.\nYou can, however, recover a tablespace where one or more of its datafiles have become corrupted or have been removed by nefarious means.\nThe following example shows the users tablespace being restored from a backup and recovered completely up to date.\nFirst of all, in an SQL*Plus session, add an up to date record to a test table in the tablespace to be restored and recovered:\n1 2 3 4 5 6 7 8 9 10 11 SQL\u0026gt; insert into test values (sysdate); 1 row created. SQL\u0026gt; select * from test order by a desc; A ------------------- 2012/03/19 20:58:18 2012/03/07 11:21:53 ... This change has not yet been archived so will be found in the online redo logs. The following is the RMAN recovery \u0026amp; restore process.\nThe first step is to take the affected tablespace(s) offline:\n1 2 RMAN\u0026gt; sql \u0026#39;alter tablespace users offline\u0026#39;; sql statement: alter tablespace users offline The next step will restore the datafiles in this tablespace from a suitable dump. RMAN will choose the dump in order to reduce the amount of work it has to do to complete the restoration:\n1 RMAN\u0026gt; restore tablespace users; 1 2 3 4 5 6 7 8 9 10 11 Starting restore at 2012/03/19 21:01:25 using channel ORA_DISK_1 channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00006 to /srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: reading from backup piece /media/oracle_backups/ant12/2nn3ict8_1_1 channel ORA_DISK_1: piece handle=/media/oracle_backups/ant12/2nn3ict8_1_1 tag=TAG20120217T165152 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 Finished restore at 2012/03/19 21:01:26 With the dump of the datafile(s) restored, I next need to recover the tablespace to the current state:\n1 RMAN\u0026gt; recover tablespace users; 1 2 3 4 5 6 7 Starting recover at 2012/03/19 21:01:32 using channel ORA_DISK_1 starting media recovery media recovery complete, elapsed time: 00:00:01 Finished recover at 2012/03/19 21:01:33 Finally, bring the tablespace back online:\n1 2 RMAN\u0026gt; sql \u0026#39;alter tablespace users online\u0026#39;; sql statement: alter tablespace users online And to be sure that I have indeed recovered the tablespace completely up to date, I re-ran the above query in my SQL*Plus session again:\n1 2 3 4 5 6 7 SQL\u0026gt; select * from test order by a desc; A ------------------- 2012/03/19 20:58:18 2012/03/07 11:21:53 ... If the SYSTEM or UNDO tablespace need restoring and recovery then the database has to be mounted, as follows:\n1 2 3 4 5 RMAN\u0026gt; shutdown database closed database dismounted Oracle instance shut down 1 2 3 4 5 6 7 8 9 10 11 12 RMAN\u0026gt; startup mount connected to target database (not started) Oracle instance started database mounted Total System Global Area 768331776 bytes Fixed Size 2230360 bytes Variable Size 213911464 bytes Database Buffers 549453824 bytes Redo Buffers 2736128 bytes 1 RMAN\u0026gt; restore tablespace system; 1 2 3 4 5 6 7 8 9 10 11 12 Starting restore at 2012/03/19 21:14:34 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=17 device type=DISK channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00001 to /srv/nffs/oradata/ant12/data/system01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_17/o1_mf_nnndf_TAG20120217T161210_7mwz2tyv_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_17/o1_mf_nnndf_TAG20120217T161210_7mwz2tyv_.bkp tag=TAG20120217T161210 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:25 Finished restore at 2012/03/19 21:15:00 1 RMAN\u0026gt; recover tablespace system; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Starting recover at 2012/03/19 21:15:07 using channel ORA_DISK_1 starting media recovery archived log for thread 1 with sequence 13 is already on disk as file /srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_13_7mwzxtxg_.arc archived log for thread 1 with sequence 14 is already on disk as file /srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_14_7mwzxx1n_.arc archived log for thread 1 with sequence 15 is already on disk as file /srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_15_7mwzxxkb_.arc archived log for thread 1 with sequence 16 is already on disk as file /srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_16_7mwzxzvm_.arc archived log for thread 1 with sequence 17 is already on disk as file /srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_17_7mwzy01m_.arc archived log for thread 1 with sequence 18 is already on disk as file /srv/nffs/flashback_area/ant12/NORM/archivelog/2012_03_19/o1_mf_1_18_7ph695lw_.arc archived log file name=/srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_13_7mwzxtxg_.arc thread=1 sequence=13 archived log file name=/srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_14_7mwzxx1n_.arc thread=1 sequence=14 archived log file name=/srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_15_7mwzxxkb_.arc thread=1 sequence=15 media recovery complete, elapsed time: 00:00:02 Finished recover at 2012/03/19 21:15:10 1 2 RMAN\u0026gt; alter database open; database opened Recover Datafiles Recovering datafiles instead of a complete tablespace could save you a lot of downtime for the affected users. As ever, the affected datafile(s) need to be offline in order to be recovered. The remainder of the process is as simple as restoring and recovering a tablespace so the following demonstration of a datafile recovery needs little comment, however, as before, if SYSTEM or UNDO are affected, the database needs to be mounted, not open.\n1 2 RMAN\u0026gt; sql \u0026#39;alter database datafile 5 offline\u0026#39;; sql statement: alter database datafile 5 offline 1 RMAN\u0026gt; restore datafile 5; 1 2 3 4 5 6 7 8 9 10 11 Starting restore at 2012/03/19 21:40:50 using channel ORA_DISK_1 channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00005 to /srv/nffs/oradata/ant12/data/tools01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_17/o1_mf_nnndf_TAG20120217T161511_7mwz8hvx_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_17/o1_mf_nnndf_TAG20120217T161511_7mwz8hvx_.bkp tag=TAG20120217T161511 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 Finished restore at 2012/03/19 21:40:52 1 RMAN\u0026gt; recover datafile 5; 1 2 3 4 5 6 7 8 9 Starting recover at 2012/03/19 21:40:57 using channel ORA_DISK_1 starting media recovery archived log for thread 1 with sequence 13 is already on disk as file /srv/nffs/flashback_area/ant12/ANT12/archivelog/2012_02_17/o1_mf_1_13_7mwzxtxg_.arc ... media recovery complete, elapsed time: 00:00:00 Finished recover at 2012/03/19 21:40:58 1 2 RMAN\u0026gt; sql \u0026#39;alter database datafile 5 online\u0026#39;; sql statement: alter database datafile 5 online Of course, the above assumes you know the datafile number, the report schema command helps here. You can always run the restore and recovery using the full filename in single quotes, but I find the data file number method to be easier and less prone to my abysmal typing skills!\nIf you have multiple files to restore and recover, separate them with a comma in the normal fashion:\n1 2 3 4 5 6 7 RMAN\u0026gt; sql \u0026#39;alter database datafile 5, 6 offline\u0026#39;; ... RMAN\u0026gt; restore datafile 5, 6; ... RMAN\u0026gt; recover datafile 5, 6; ... RMAN\u0026gt; sql \u0026#39;alter database datafile 5, 6 online\u0026#39;; Recover Individual Blocks If you can save time by restoring and recovering just a datafile or two rather than a complete tablespace, then how about restoring and recovering a block or two, that\u0026rsquo;s will save even more time surely?\nRMAN can help you out here as well, and you don\u0026rsquo;t need the datafile(s) to be offline at the time.\n1 RMAN\u0026gt; recover datafile 5 block 24; 1 2 3 4 5 6 7 Starting recover at 2012/03/19 21:49:47 using channel ORA_DISK_1 starting media recovery media recovery complete, elapsed time: 00:00:00 Finished recover at 2012/03/19 21:49:47 In this case, even the SYSTEM and UNDO tablespaces can be recovered while online. You should note that there is no need to restore the data block first, just run the recover command.\nSee the RMAN manual for many other formats of this useful command.\n","description":"","id":124,"section":"posts","tags":null,"title":"Oracle RMAN for Beginners â Part 7","uri":"http://localhost:1313/RantsAndRaves/posts/2012/03/oracle-rman-for-beginners-part-7/"},{"content":"Due to a document not being supplied to me recently, I built a few 11g databases with the wrong SID. I needed to change them all. I found a few web pages, Oracle and others, on using the nid utility to do just that, however, they were incomplete. The full process is described here.\nThe following has been tested fully on an 11.2.0.2 Oracle database. The nid utility used also exists in Oracle 10.2.0.5 and possibly other versions that I do not have access to at the moment.\nBackup the database or ensure a backup exists in RMAN. This shouldn\u0026rsquo;t be necessary but you are a DBA right? And chances we do not take!\ncreate pfile='/home/oracle/initnew_sid.ora' from spfile; this ensures that the pfile you are about to edit contains all the current settings. It is saved in a non-standard location for safety.\nShutdown the database cleanly. Use shutdown or shutdown immediate.\nStartup mount the database.\nCheck the open_cursors parameter. If you have a lot of data files you might need to increase this setting temporarily:\nalter system set open\\_cursors=1500 scope=memory;\nIn a shell session run the following nid command:\nnid target=/ dbname=new\\_sid setname=y logfile=new\\_name.log\nThe logfile created will show details of what just happened and should be checked for errors. As far as I am aware, but don\u0026rsquo;t trust me on this, errors will not change the database at all.\nThe instance will also have been shut down at the end of the nid command.\nEdit /etc/oratab to change the old sid to the new one.\nEdit tnsnames.ora to do likewise. Also applies to OID, LDAP, whatever you use for alias resolution.\nStop the appropriate listener, edit listener.ora and restart the listener. If the listener in question serves other databases, just edit the listener.ora file and run lsnrctl reload listener_name.\nCopy the newly created /home/oracle/initnew_sid.ora to $ORACLE_HOME/dbs/initnew_sid.ora then edit the new file and change the db_name parameter.\nYou may, if desired, delete the old files $ORACLE_HOME/dbs/initold_sid.ora and $ORACLE_HOME/dbs/spfileold_sid.ora as they are no longer needed. (Unless you plan on renaming the database back again of course!)\nExport ORACLE_SID as the new sid. ORACLE_HOME will remain the same as before.\nRun the orapwd command to create a new password file if required.\nStartup the database.\ncreate spfile='oracle_home/dbs/spfilenew_sid.ora' from pfile; Obviously substituting the correct values for new_sid and oracle_home.\nShutdown and restart the database to use the new spfile.\nCheck it all just worked:\nselect name from v$database;\nshow parameter db\\_name\nThat\u0026rsquo;s the simple bit and in theory is all you need to do. However, be aware that any data files, control files, recovery file destinations etc still have their old format names. You may end up with a database called fred located in /srv/barney/oradata for example. If you wish to go down the route of renaming everything to suit the database name, see part 2 of this discussion, here.\nAs the control files are not overwritten by this process, and because the DBID doesn\u0026rsquo;t change, all your RMAN backup information is safe and even after the database has been renamed, you can use an old backup to restore and recover the database. I\u0026rsquo;ve tested this on a tablespace recovery to be sure.\nOk, when I say your backups are safe, they are, but if you try to restore the controlfile from an old pre-rename dump, it will restore with no errors. However, when you attempt to mount the database you will be informed that the name in the controlfile doesn\u0026rsquo;t match the database. Edit the restored controlfile(s) to change the name and continue.\nI\u0026rsquo;m pretty sure that because the DBID doesn\u0026rsquo;t change, an RMAN catalogue will not lose any backup details either. On my test system, I don\u0026rsquo;t yet have a catalogue to play with, so best you test on an expendable database first. Just saying!\nOnline and backed up archived logs as well as the online redo logs are used quite happily to apply any required REDO.\nIt just works!\n","description":"","id":125,"section":"posts","tags":null,"title":"Rename an Oracle 10g or 11g Database","uri":"http://localhost:1313/RantsAndRaves/posts/2012/03/rename-an-oracle-10g-or-11g-database/"},{"content":"In the past few instalments, I\u0026rsquo;ve looked mainly at databases running in NOARCHIVELOG mode. It\u0026rsquo;s probably not the best way to run a production system but it is valid to do so.\nFor the safest systems and the ability to recover from numerous disasters without losing any data, the database should be run in ARCHIVELOG mode.\nThe remainder of the mini-series concentrates on the things you can do with RMAN when the database is running in ARCHIVELOG mode.\nHot Backups There\u0026rsquo;s nothing to taking a hot backup that I haven\u0026rsquo;t already shown you. Taking a backup of the full database in hot (or online) mode is exactly the same as taking a cold (offline) backup.\nOne difference is that you must backup the archived logs in addition to the database.\nAnother difference is that you can take a hot backup while the database is open, although you can also take one while the database is mounted.\nIn the following examples, I have set my archivelog deletion policy as follows:\n1 2 3 4 5 RMAN\u0026gt; configure archivelog deletion policy backed up 2 times to disk; new RMAN configuration parameters: CONFIGURE ARCHIVELOG DELETION POLICY TO BACKED UP 2 TIMES TO DISK; new RMAN configuration parameters are successfully stored Backup the Database and Archived Logs To backup the database, including controlfile and spfile if autobackup is configured - and the archived logs, all I need to do is as follows:\n1 2 3 4 RMAN\u0026gt; run { 2\u0026gt; backup full database; 3\u0026gt; backup archivelog all delete input; 4\u0026gt; } An extract of the results of running the above is this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Starting backup at 2012/02/17 15:07:46 allocated channel: ORA\\_DISK\\_1 channel ORA\\_DISK\\_1: SID=45 device type=DISK channel ORA\\_DISK\\_1: starting full datafile backup set channel ORA\\_DISK\\_1: specifying datafile(s) in backup set input datafile file number=00010 name=/srv/nffs/oradata/ant12/data/NLWLDELFTFEWSModDat01\\_01.dbf ... input datafile file number=00006 name=/srv/nffs/oradata/ant12/data/users01.dbf channel ORA\\_DISK\\_1: starting piece 1 at 2012/02/17 15:07:47 channel ORA\\_DISK\\_1: finished piece 1 at 2012/02/17 15:08:32 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/backupset/2012\\_02\\_17/o1\\_mf\\_nnndf\\_TAG20120217T150747\\_7mwvb3sy\\_.bkp tag=TAG20120217T150747 comment=NONE channel ORA\\_DISK\\_1: backup set complete, elapsed time: 00:00:45 Finished backup at 2012/02/17 15:08:32 Starting backup at 2012/02/17 15:08:34 current log archived using channel ORA\\_DISK\\_1 channel ORA\\_DISK\\_1: starting archived log backup set channel ORA\\_DISK\\_1: specifying archived log(s) in backup set input archived log thread=1 sequence=1 RECID=57 STAMP=775494253 input archived log thread=1 sequence=2 RECID=58 STAMP=775494254 input archived log thread=1 sequence=3 RECID=59 STAMP=775494255 input archived log thread=1 sequence=4 RECID=60 STAMP=775494258 input archived log thread=1 sequence=5 RECID=61 STAMP=775494274 input archived log thread=1 sequence=6 RECID=62 STAMP=775494514 channel ORA\\_DISK\\_1: starting piece 1 at 2012/02/17 15:08:35 channel ORA\\_DISK\\_1: finished piece 1 at 2012/02/17 15:08:38 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/backupset/2012\\_02\\_17/o1\\_mf\\_annnn\\_TAG20120217T150834\\_7mwvcm3s\\_.bkp tag=TAG20120217T150834 comment=NONE channel ORA\\_DISK\\_1: backup set complete, elapsed time: 00:00:03 The archived logs are backed up to the FRA, as usual, and then deleted from their normal position in the FRA.This helps keep the FRA tidy of archived logs which are no longer necessary as enough backups have been taken.\nSetting the archived log deletion policy sensibly means that you should never be without the logs you need to recover a restored database, either in full or up to a specific point in time, scn or log sequence number.\nYou may note, if you looked closely, that one of the first things RMAN did as part of the archived log backup, was to archive the current online redo log. This means that you will have all the archived logs necessary since the start of the backup (of the database) available in the event of a recovery being required.\nAs this is the first time my archived logs have been backed up, the attempt to delete them has not been successful, as the following extract shows:\n1 2 3 4 5 6 7 channel ORA\\_DISK\\_1: deleting archived log(s) RMAN-08138: WARNING: archived log not deleted - must create more backups archived log file name=/srv/nffs/flashback\\_area/ant12/ANT12/archivelog/2012\\_02\\_17/o1\\_mf\\_1\\_1\\_7mwv3ddf\\_.arc thread=1 sequence=1 ... RMAN-08138: WARNING: archived log not deleted - must create more backups archived log file name=/srv/nffs/flashback\\_area/ant12/ANT12/archivelog/2012\\_02\\_17/o1\\_mf\\_1\\_6\\_7mwvclf7\\_.arc thread=1 sequence=6 Finished backup at 2012/02/17 15:08:38 These warnings do not affect the backup. The archived logs in the FRA have been successfully backed up (to the FRA) but because the deletion policy states that they can only be deleted, from the FRA, when they have already been backed up twice to disc, then they must remain on disc in the FRA.\nFinally, the controlfile and spfile are automatically backed up:\n1 2 3 Starting Control File and SPFILE Autobackup at 2012/02/17 15:08:38 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/autobackup/2012\\_02\\_17/o1\\_mf\\_s\\_775494518\\_7mwvcpqn\\_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/17 15:08:41 It may be simpler to run the following command instead:\n1 RMAN\u0026gt; backup full database plus archivelog delete input; Looking at the log from the above command shows that the following steps take place:\nThe current redo log is archived. The archived logs are (all) backed up. The deletion policy for archived logs is applied. The database is backed up. The (new) current redo log is archived. The newly archived log is backed up on its own. The controlfile and spfile are backed up - if configured to do so. An extract from the log, showing the above steps, is as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 Starting backup at 2012/02/17 15:37:19 current log archived using channel ORA\\_DISK\\_1 channel ORA\\_DISK\\_1: starting archived log backup set channel ORA\\_DISK\\_1: specifying archived log(s) in backup set input archived log thread=1 sequence=1 RECID=57 STAMP=775494253 ... input archived log thread=1 sequence=7 RECID=63 STAMP=775496239 channel ORA\\_DISK\\_1: starting piece 1 at 2012/02/17 15:37:20 channel ORA\\_DISK\\_1: finished piece 1 at 2012/02/17 15:37:23 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/backupset/2012\\_02\\_17/o1\\_mf\\_annnn\\_TAG20120217T153720\\_7mwx1j93\\_.bkp tag=TAG20120217T153720 comment=NONE channel ORA\\_DISK\\_1: backup set complete, elapsed time: 00:00:03 channel ORA\\_DISK\\_1: deleting archived log(s) archived log file name=/srv/nffs/flashback\\_area/ant12/ANT12/archivelog/2012\\_02\\_17/o1\\_mf\\_1\\_1\\_7mwv3ddf\\_.arc RECID=57 STAMP=775494253 ... archived log file name=/srv/nffs/flashback\\_area/ant12/ANT12/archivelog/2012\\_02\\_17/o1\\_mf\\_1\\_6\\_7mwvclf7\\_.arc RECID=62 STAMP=775494514 RMAN-08138: WARNING: archived log not deleted - must create more backups archived log file name=/srv/nffs/flashback\\_area/ant12/ANT12/archivelog/2012\\_02\\_17/o1\\_mf\\_1\\_7\\_7mwx1hj5\\_.arc thread=1 sequence=7 Finished backup at 2012/02/17 15:37:23 Starting backup at 2012/02/17 15:37:23 using channel ORA\\_DISK\\_1 channel ORA\\_DISK\\_1: starting full datafile backup set channel ORA\\_DISK\\_1: specifying datafile(s) in backup set input datafile file number=00010 name=/srv/nffs/oradata/ant12/data/NLWLDELFTFEWSModDat01\\_01.dbf ... input datafile file number=00006 name=/srv/nffs/oradata/ant12/data/users01.dbf channel ORA\\_DISK\\_1: starting piece 1 at 2012/02/17 15:37:24 channel ORA\\_DISK\\_1: finished piece 1 at 2012/02/17 15:38:09 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/backupset/2012\\_02\\_17/o1\\_mf\\_nnndf\\_TAG20120217T153723\\_7mwx1n4f\\_.bkp tag=TAG20120217T153723 comment=NONE channel ORA\\_DISK\\_1: backup set complete, elapsed time: 00:00:45 Finished backup at 2012/02/17 15:38:09 Starting backup at 2012/02/17 15:38:09 current log archived using channel ORA\\_DISK\\_1 channel ORA\\_DISK\\_1: starting archived log backup set channel ORA\\_DISK\\_1: specifying archived log(s) in backup set input archived log thread=1 sequence=8 RECID=64 STAMP=775496289 channel ORA\\_DISK\\_1: starting piece 1 at 2012/02/17 15:38:09 channel ORA\\_DISK\\_1: finished piece 1 at 2012/02/17 15:38:10 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/backupset/2012\\_02\\_17/o1\\_mf\\_annnn\\_TAG20120217T153809\\_7mwx31qd\\_.bkp tag=TAG20120217T153809 comment=NONE channel ORA\\_DISK\\_1: backup set complete, elapsed time: 00:00:01 channel ORA\\_DISK\\_1: deleting archived log(s) RMAN-08138: WARNING: archived log not deleted - must create more backups archived log file name=/srv/nffs/flashback\\_area/ant12/ANT12/archivelog/2012\\_02\\_17/o1\\_mf\\_1\\_8\\_7mwx31h2\\_.arc thread=1 sequence=8 Finished backup at 2012/02/17 15:38:10 Starting Control File and SPFILE Autobackup at 2012/02/17 15:38:10 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/autobackup/2012\\_02\\_17/o1\\_mf\\_s\\_775496290\\_7mwx33bn\\_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/17 15:38:13 Backup Selected Tablespaces One of the joys of running a database in ARCHIVELOG mode is the fact that it is possible to backup and recover individual tablespaces.\n1 RMAN\u0026gt; backup tablespace users; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Starting backup at 2012/02/17 15:59:07 using channel ORA\\_DISK\\_1 channel ORA\\_DISK\\_1: starting full datafile backup set channel ORA\\_DISK\\_1: specifying datafile(s) in backup set input datafile file number=00006 name=/srv/nffs/oradata/ant12/data/users01.dbf channel ORA\\_DISK\\_1: starting piece 1 at 2012/02/17 15:59:07 channel ORA\\_DISK\\_1: finished piece 1 at 2012/02/17 15:59:08 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/backupset/2012\\_02\\_17/o1\\_mf\\_nnndf\\_TAG20120217T155907\\_7mwybcbd\\_.bkp tag=TAG20120217T155907 comment=NONE channel ORA\\_DISK\\_1: backup set complete, elapsed time: 00:00:01 Finished backup at 2012/02/17 15:59:08 Starting Control File and SPFILE Autobackup at 2012/02/17 15:59:08 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/autobackup/2012\\_02\\_17/o1\\_mf\\_s\\_775497548\\_7mwybdnt\\_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/17 15:59:09 Including the archived logs at the same time, is possible too, by running the backup tablespace tools plus archivelog delete input; command. It isn\u0026rsquo;t mandatory to delete the archived logs as part of the command, but it helps keep the FRA tidy.\nBackup Selected Data Files With ARCHIVELOG mode enabled, you can even backup at the data file level.\nDatafailes may be specified by id or my full name. The following backs up the SYSTEM tablespace\u0026rsquo;s single datafile.\n1 RMAN\u0026gt; report schema; 1 2 3 4 5 6 7 8 9 Report of database schema for database with db\\_unique\\_name ANT12 List of Permanent Datafiles =========================== File Size(MB) Tablespace RB segs Datafile Name ---- -------- -------------------- ------- ------------------------ 1 600 SYSTEM \\*\\*\\* /srv/nffs/oradata/ant12/data/system01.dbf ... 12 500 XDB \\*\\*\\* /srv/nffs/oradata/ant12/data/xdb01.dbf 1 RMAN\u0026gt; backup datafile 1; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Starting backup at 2012/02/17 16:12:10 using channel ORA\\_DISK\\_1 channel ORA\\_DISK\\_1: starting full datafile backup set channel ORA\\_DISK\\_1: specifying datafile(s) in backup set input datafile file number=00001 name=/srv/nffs/oradata/ant12/data/system01.dbf channel ORA\\_DISK\\_1: starting piece 1 at 2012/02/17 16:12:10 channel ORA\\_DISK\\_1: finished piece 1 at 2012/02/17 16:12:26 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/backupset/2012\\_02\\_17/o1\\_mf\\_nnndf\\_TAG20120217T161210\\_7mwz2tyv\\_.bkp tag=TAG20120217T161210 comment=NONE channel ORA\\_DISK\\_1: backup set complete, elapsed time: 00:00:16 Finished backup at 2012/02/17 16:12:26 Starting Control File and SPFILE Autobackup at 2012/02/17 16:12:26 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/autobackup/2012\\_02\\_17/o1\\_mf\\_s\\_775498346\\_7mwz3bdl\\_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/17 16:12:27 You can also backup multiple data files together by specifying a comma separated list of id numbers or names:\n1 2 3 RMAN\u0026gt; backup datafile 2\u0026gt; \u0026#39;/srv/nffs/oradata/ant12/data/tools01.dbf\u0026#39;, 3\u0026gt; \u0026#39;/srv/nffs/oradata/ant12/data/users01.dbf\u0026#39;; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Starting backup at 2012/02/17 16:15:11 using channel ORA\\_DISK\\_1 channel ORA\\_DISK\\_1: starting full datafile backup set channel ORA\\_DISK\\_1: specifying datafile(s) in backup set input datafile file number=00005 name=/srv/nffs/oradata/ant12/data/tools01.dbf input datafile file number=00006 name=/srv/nffs/oradata/ant12/data/users01.dbf channel ORA\\_DISK\\_1: starting piece 1 at 2012/02/17 16:15:11 channel ORA\\_DISK\\_1: finished piece 1 at 2012/02/17 16:15:12 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/backupset/2012\\_02\\_17/o1\\_mf\\_nnndf\\_TAG20120217T161511\\_7mwz8hvx\\_.bkp tag=TAG20120217T161511 comment=NONE channel ORA\\_DISK\\_1: backup set complete, elapsed time: 00:00:01 Finished backup at 2012/02/17 16:15:12 Starting Control File and SPFILE Autobackup at 2012/02/17 16:15:12 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/autobackup/2012\\_02\\_17/o1\\_mf\\_s\\_775498513\\_7mwz8k80\\_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/17 16:15:16 The above is equivalent to the command backup datafile 5,6; in the case of my database.\nBackup Archived Logs You can take a backup of the archived logs at any time you wish simply by running the backup archivelog all; command as demonstrated at the beginning of this instalment.\nIt is possible to backup only a selection of archived logs, for example, a single log file:\n1 RMAN\u0026gt; backup archivelog sequence 15; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Starting backup at 2012/02/17 16:27:15 using channel ORA\\_DISK\\_1 channel ORA\\_DISK\\_1: starting archived log backup set channel ORA\\_DISK\\_1: specifying archived log(s) in backup set input archived log thread=1 sequence=15 RECID=71 STAMP=775499197 channel ORA\\_DISK\\_1: starting piece 1 at 2012/02/17 16:27:15 channel ORA\\_DISK\\_1: finished piece 1 at 2012/02/17 16:27:16 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/backupset/2012\\_02\\_17/o1\\_mf\\_annnn\\_TAG20120217T162715\\_7mwzz3gk\\_.bkp tag=TAG20120217T162715 comment=NONE channel ORA\\_DISK\\_1: backup set complete, elapsed time: 00:00:01 Finished backup at 2012/02/17 16:27:16 Starting Control File and SPFILE Autobackup at 2012/02/17 16:27:16 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/autobackup/2012\\_02\\_17/o1\\_mf\\_s\\_775499236\\_7mwzz4w2\\_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/17 16:27:17 Or, if required, a number of them:\n1 RMAN\u0026gt; backup archivelog sequence between 16 and 19; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Starting backup at 2012/02/17 16:28:15 using channel ORA\\_DISK\\_1 channel ORA\\_DISK\\_1: starting archived log backup set channel ORA\\_DISK\\_1: specifying archived log(s) in backup set input archived log thread=1 sequence=16 RECID=72 STAMP=775499199 input archived log thread=1 sequence=17 RECID=73 STAMP=775499200 channel ORA\\_DISK\\_1: starting piece 1 at 2012/02/17 16:28:16 channel ORA\\_DISK\\_1: finished piece 1 at 2012/02/17 16:28:17 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/backupset/2012\\_02\\_17/o1\\_mf\\_annnn\\_TAG20120217T162815\\_7mx01041\\_.bkp tag=TAG20120217T162815 comment=NONE channel ORA\\_DISK\\_1: backup set complete, elapsed time: 00:00:01 Finished backup at 2012/02/17 16:28:17 Starting Control File and SPFILE Autobackup at 2012/02/17 16:28:17 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/autobackup/2012\\_02\\_17/o1\\_mf\\_s\\_775499297\\_7mx011ll\\_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/17 16:28:18 You may also specify the logs to be backed up using times. For example, the command backup archivelog from time 'sysdate-7' until time 'sysdate-1'; will backup those archived logs, still on disc in the FRA, that were created within the time period specified.\nAnd finally, for archived logs, if they have been stored in a section of the file system which is outside the FRA, you can still back those up.\nIf the archived logs are in the location /srv/nffs/oradata/arch then you back those up (to the FRA) by running the command backup archivelog like '/srv/nffs/oradata/arch%';\nSpecifying a New Destination for Backups In all of the preceding examples, the backups have been created in the FRA. What do you do if yo want to create a backup somewhere else?\nThe format parameter is how:\n1 RMAN\u0026gt; backup tablespace users format=\u0026#39;/media/oracle\\_backups/ant12/%U\u0026#39;; Starting backup at 2012/02/17 16:51:52 using channel ORA\\_DISK\\_1 channel ORA\\_DISK\\_1: starting full datafile backup set channel ORA\\_DISK\\_1: specifying datafile(s) in backup set input datafile file number=00006 name=/srv/nffs/oradata/ant12/data/users01.dbf channel ORA\\_DISK\\_1: starting piece 1 at 2012/02/17 16:51:52 channel ORA\\_DISK\\_1: finished piece 1 at 2012/02/17 16:51:53 piece handle=/media/oracle\\_backups/ant12/2nn3ict8\\_1\\_1 tag=TAG20120217T165152 comment=NONE channel ORA\\_DISK\\_1: backup set complete, elapsed time: 00:00:01 Finished backup at 2012/02/17 16:51:53 Starting Control File and SPFILE Autobackup at 2012/02/17 16:51:54 piece handle=/srv/nffs/flashback\\_area/ant12/ANT12/autobackup/2012\\_02\\_17/o1\\_mf\\_s\\_775500714\\_7mx1fbdd\\_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/17 16:51:55 It can be seen from the piece handle that the backup has indeed been sent to the non-FRA disc area.\nThe format parameter can be used for all types of backups, not just for tablespaces as in this example.\n","description":"","id":126,"section":"posts","tags":null,"title":"Oracle RMAN for Beginners â Part 6","uri":"http://localhost:1313/RantsAndRaves/posts/2012/02/oracle-rman-for-beginners-part-6/"},{"content":"Previously in this mini-series, I managed to dump and recover a database from a cold backup. Before I move on to similar practices with hot backups, a few terms and observations may well be in order.\nTerms and Conditions A restore is the action of copying files back from a backup, prior to recovery.\nRecovery is carried out after restoring the files and allows the database to be recovered in full or to a specific point in time, using backups of archived logs, and/or the current archived logs - which may not have been backed up yet.\nA full recovery restores and recovers the database to an up to date point. No data will be lost or missing after the recovery.\nA partial recovery restores and recovers the database to a point in time in the past. Data may be lost after the recovery has completed.\nA consistent backup is a cold backup. I have been using these types of backup since the beginning of this series of articles.\nA consistent backup is one which, when recovered, leaves the database ready to use with no further recovery required.\nAn inconsistent backup is a hot backup. We will be looking at these backups next as they are the ones most likely to be in popular use.\nAn inconsistent backup has to be restored and then rolled forward (recovered) to some specific point in time before the database can be opened for use.\nAn available backup is a backup, consistent or otherwise, which is available for use in recovering a database.\nAn unavailable backup is a backup, consistent or otherwise, which is deemed to be unavailable for use in recovering a database. In normal conditions, this is a forced action by the DBA who marks the backup as unavailable in order to prevent RMAN using it in a restoration\nAn expired backup is a backup which is registered in the catalogue and/or controlfile, but which is not found on disc or tape.\nAn obsolete backup is a backup for which the files remain on disc or tape, but the configured retention period has caused the backup to become no longer required.\nA full backup is a backup of the whole database.\nAn incremental backup is a backup which is not a full backup, but only contains blocks which have changed since the previous level 0 or level 1 incremental backup depending on whether the backup is cumulative or differential.\nRetention and Deletion Policy Oracle advise four (main) things when using RMAN to backup and recover your databases:\nUse a flashback recovery area Use a backup retention policy Use an archive log deletion policy Use a recovery catalogue. The use of the FRA has been covered already so no more need be said about that.\nI will be discussing a recover catalogue later in this mini-series. For now, I\u0026rsquo;m sticking with the use only of the control file to record database backup details.\nRetention Policy You may configure a retention policy for your backups to ensure that they remain available for as long as you deem necessary. You may define the retention as a recovery window - the number of days a backups should remain available for - or as a number of copies.\nRMAN\u0026rsquo;s default is a retention policy of one copy. This means that as each new database backup is created, the previous one becomes obsolete.\nThe method you use will depend on how your organisation likes to work. If the rules are that all backups should be kept for a fortnight then the command configure retention policy to recovery window of 14 days will cover that rule.\nIf, on the other hand, the rules demand that four copies of all backups should be retained then the command required will be configure retention policy to redundancy 4.\nWhichever way that you configure your retention policy, backups that fall outside of your configured policy will automatically be flagged as obsolete by RMAN.\nNote: RMAN may well hold on to a backup beyond its retention period if any of the files within the backup are still required to be able to restore the database.\nWith backup optimisation configured on, files that have not changed since the previous backup will not be backed up. This speeds up the backup process but does mean that more backups may be required to allow everything to be restored.\nArchive log Deletion Policy The archive log deletion policy is a similar system to specify which archived logs are eligible for deleting from the FRA to recover space.\nRMAN will delete those eligible logs as and when it needs to recover space in the FRA for further backups, or to avoid exceeding the setting of the DB_RECOVERY_FILE_DEST_SIZE for the database.\nIf you have mirrored copies of your archived logs stored outside the FRA, those will not be deleted automatically by RMAN, it is your responsibility to delete them.\nThe deletion policy also controls whether or not archived logs are deleted from the FRA when the backup archivelog all delete input command is executed. Any archived logs that have not yet been backed up enough times, or have been backed up but for too few days, will not be deleted. You can force the deletion though.\nExamples of setting up the deletion policy are configure archivelog deletion policy to applied on all standby or configure archivelog deletion policy to backed up 5 times to sbt etc.\nThe default setting is none which means that archived logs are never deleted automatically from the FRA.\n","description":"","id":127,"section":"posts","tags":null,"title":"Oracle RMAN for Beginners â Part 5","uri":"http://localhost:1313/RantsAndRaves/posts/2012/02/oracle-rman-for-beginners-part-5/"},{"content":"Frits Hoogland\u0026rsquo;s post on how to clone your Oracle Home rather than installing from scratch. Nice!\nOne thing to beware of, if you have already applied a PSU, then you must ensure that you also include the .patch_contents hidden directory, if you don\u0026rsquo;t, you will not be able to apply any further PSUs to the cloned home(s) created from the tarball.\n","description":"","id":128,"section":"posts","tags":null,"title":"Clone Oracle Home Easily","uri":"http://localhost:1313/RantsAndRaves/posts/2012/02/clone-oracle-home-easily/"},{"content":"So far I have managed to dump and recover a database running in ARCHIVELOG mode. That is the most sensible mode for a production database and will be the case for the rest of this small RMAN guide. However, what if your databases are not running in ARCHIVELOG mode? What can RMAN do for you?\nRunning a Database in NOARCHIVELOG Mode The database is shutdown at the moment, so we will MOUNT it using RMAN and create a brand new full cold backup in the FRA.\n1 2 3 4 5 6 7 8 RMAN\u0026gt; connect target / connected to target database (not started) RMAN\u0026gt; startup mount Oracle instance started database mounted ... 1 RMAN\u0026gt; backup full database; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Starting backup at 2012/02/06 20:47:41 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=18 device type=DISK channel ORA_DISK_1: starting full datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set input datafile file number=00010 name=/srv/nffs/oradata/ant12/data/NLWLDELFTFEWSModDat01_01.dbf ... input datafile file number=00006 name=/srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: starting piece 1 at 2012/02/06 20:47:43 channel ORA_DISK_1: finished piece 1 at 2012/02/06 20:48:28 piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_06/o1_mf_nnndf_TAG20120206T204742_7m0h3hmw_.bkp tag=TAG20120206T204742 comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:45 Finished backup at 2012/02/06 20:48:28 Starting Control File and SPFILE Autobackup at 2012/02/06 20:48:28 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2012_02_06/o1_mf_s_774563524_7m0h4xng_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/06 20:48:31 1 2 3 RMAN\u0026gt; startup; database is already started database opened Again, we will trash the database by renaming a data file, after doing some work in a SQL*Plus session. The session to attempt a recovery using RMAN went like this:\n1 RMAN\u0026gt; startup 1 2 3 4 5 6 7 8 9 connected to target database (not started) Oracle instance started database mounted RMAN-00571: =========================================================== RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS =============== RMAN-00571: =========================================================== RMAN-03002: failure of startup command at 02/06/2012 21:00:45 ORA-01157: cannot identify/lock data file 6 - see DBWR trace file ORA-01110: data file 6: \u0026#39;/srv/nffs/oradata/ant12/data/users01.dbf\u0026#39; From the filename, I know it\u0026rsquo;s the users tablespace that\u0026rsquo;s affected, so I will attempt to restore and recover just that:\n1 RMAN\u0026gt; restore tablespace users; 1 2 3 4 5 6 7 8 9 10 11 12 Starting restore at 2012/02/06 21:01:21 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=18 device type=DISK channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00006 to /srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_06/o1_mf_nnndf_TAG20120206T204742_7m0h3hmw_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_06/o1_mf_nnndf_TAG20120206T204742_7m0h3hmw_.bkp tag=TAG20120206T204742 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:01 Finished restore at 2012/02/06 21:01:23 It looks like RMAN allows a single tablespace to be restored, will it recover?\n1 RMAN\u0026gt; recover tablespace users; 1 2 3 4 5 6 7 Starting recover at 2012/02/06 21:01:44 using channel ORA_DISK_1 starting media recovery ... RMAN-08187: WARNING: media recovery until SCN 855319 complete Finished recover at 2012/02/06 21:01:46 That warning doesn\u0026rsquo;t look too healthy. I next attempt to open the database:\n1 RMAN\u0026gt; startup; 1 2 3 4 5 6 7 database is already started RMAN-00571: =========================================================== RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS =============== RMAN-00571: =========================================================== RMAN-03002: failure of startup command at 02/06/2012 21:02:09 ORA-01113: file 6 needs media recovery ORA-01110: data file 6: \u0026#39;/srv/nffs/oradata/ant12/data/users01.dbf\u0026#39; So, that\u0026rsquo;s it, with a database running in NOARCHIVELOG mode, you lose data if the changes that you are trying to recover have aged out of the online redo logs. Had they still been there I would have been able to recover the tablespace and open the database, as it is, I now need to do a full restore to get a consistent database back up and running. The database will be consistent, but my changes since the last full backup will be lost. Mine and everyone elses of course.\n1 RMAN\u0026gt; restore database; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Starting restore at 2012/02/06 21:08:11 using channel ORA_DISK_1 skipping datafile 6; already restored to file /srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00001 to /srv/nffs/oradata/ant12/data/system01.dbf ... channel ORA_DISK_1: restoring datafile 00012 to /srv/nffs/oradata/ant12/data/xdb01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_06/o1_mf_nnndf_TAG20120206T204742_7m0h3hmw_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_06/o1_mf_nnndf_TAG20120206T204742_7m0h3hmw_.bkp tag=TAG20120206T204742 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:03:56 Finished restore at 2012/02/06 21:12:08 1 2 RMAN\u0026gt; sql \u0026#34;alter database open resetlogs\u0026#34;; sql statement: alter database open resetlogs That\u0026rsquo;s it then, the database has been restored and opened.\nOne other thing, RMAN is smart enough to notice that my unsuccessful attempt to restore just the users tablespace meant that it didn\u0026rsquo;t have to restore it again. That saved a little time.\nThe moral to this little exercise is simple. Don\u0026rsquo;t run your databases in NOARCHIVELOG mode because you will lose data. In addition, you must always have downtime to backup the database. If you attempt to backup the database while it is open, you will see the following RMAN error message:\n1 RMAN\u0026gt; backup full database; 1 2 3 4 5 6 7 8 9 10 Starting backup at 2012/02/07 07:19:35 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=18 device type=DISK channel ORA_DISK_1: starting full datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set RMAN-00571: =========================================================== RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS =============== RMAN-00571: =========================================================== RMAN-03009: failure of backup command on ORA_DISK_1 channel at 02/07/2012 07:19:36 ORA-19602: cannot backup or copy active file in NOARCHIVELOG mode Recovery From a Total Loss Even in NOARCHIVEMODE and without using a catalogue, it is possible to recover a database when the controlfiles are lost or unusable. However, this is only possible, according to the manual, from a controlfile autobackup. You also must have the database unique identifier (DBID) to hand as well as the configured format of the controlfile autobackup files.\nIn the following example, I made sure that I had a backup of the database, the DBID (which RMAN helpfully displays when you connect to the target at the RMAN\u0026gt; prompt) and my controlfile autobackup format is the default, so I didn\u0026rsquo;t have to worry about that.\nI shut down the database and deleted all the files. That represents a total failure scenario. The database is running in NOARCHIVELOG by the way. Let\u0026rsquo;s recover it.\n1 2 3 4 5 RMAN\u0026gt; startup nomount connected to target database (not started) Oracle instance started ... The database cannot be mounted as we have no controlfiles.\n1 2 RMAN\u0026gt; set DBID=2799264292 executing command: SET DBID This allows RMAN to try and find the controlfile autobackup.\n1 RMAN\u0026gt; restore controlfile from autobackup; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Starting restore at 2012/02/07 08:21:39 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=19 device type=DISK recovery area destination: /srv/nffs/flashback_area/ant12 database name (or database unique name) used for search: ANT12 channel ORA_DISK_1: AUTOBACKUP /srv/nffs/flashback_area/ant12/ANT12/autobackup/2012_02_07/o1_mf_s_774602851_7m1noocb_.bkp found in the recovery area channel ORA_DISK_1: looking for AUTOBACKUP on day: 20120207 channel ORA_DISK_1: restoring control file from AUTOBACKUP /srv/nffs/flashback_area/ant12/ANT12/autobackup/2012_02_07/o1_mf_s_774602851_7m1noocb_.bkp channel ORA_DISK_1: control file restore from AUTOBACKUP complete output file name=/srv/nffs/oradata/ant12/ctrl/control01.ctl output file name=/srv/nffs/flashback_area/ant12/ctrl/control02.ctl output file name=/srv/nffs/oradata/ant12/ctrl/control03.ctl Finished restore at 2012/02/07 08:21:40 So far so good. I have my controlfiles back. As I was using the default autobackup format, I didn\u0026rsquo;t have to set the format, however, if I had changed the format at some point, I need to tell RMAN what it is. In that case, the above restore would look like the following:\n1 2 3 4 5 6 7 RMAN\u0026gt; set DBID=2799264292 executing command: SET DBID RMAN\u0026gt; run { 2\u0026gt; set controlfile autobackup format for device type disk to \u0026#39;your format here\u0026#39;; 3\u0026gt; restore controlfile from autobackup; 4\u0026gt; } The end result would be, hopefully, the restoration of the latest controlfile backup.\nThe controlfiles are back, but I\u0026rsquo;m are still without the data files.\nNote: any time that you have to restore the controlfile as part of a restore means that you must open the database with the resetlogs option.\n1 2 RMAN alter database mount; database mounted 1 RMAN\u0026gt; restore database; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Starting restore at 2012/02/07 08:40:20 Starting implicit crosscheck backup at 2012/02/07 08:40:20 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=19 device type=DISK Crosschecked 1 objects Finished implicit crosscheck backup at 2012/02/07 08:40:21 Starting implicit crosscheck copy at 2012/02/07 08:40:21 using channel ORA_DISK_1 Crosschecked 17 objects Finished implicit crosscheck copy at 2012/02/07 08:40:22 searching for all files in the recovery area cataloging files... cataloging done List of Cataloged Files ======================= File Name: /srv/nffs/flashback_area/ant12/ANT12/autobackup/2012_02_07/o1_mf_s_774602851_7m1noocb_.bkp using channel ORA_DISK_1 channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00001 to /srv/nffs/oradata/ant12/data/system01.dbf ... channel ORA_DISK_1: restoring datafile 00012 to /srv/nffs/oradata/ant12/data/xdb01.dbf channel ORA_DISK_1: reading from backup piece /srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_07/o1_mf_nnndf_TAG20120207T072806_7m1nn6rq_.bkp channel ORA_DISK_1: piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_07/o1_mf_nnndf_TAG20120207T072806_7m1nn6rq_.bkp tag=TAG20120207T072806 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:03:38 Finished restore at 2012/02/07 08:44:03 1 2 RMAN\u0026gt; alter database open resetlogs; database opened Now that\u0026rsquo;s what I call magic! ;-)\n","description":"","id":129,"section":"posts","tags":null,"title":"Oracle RMAN for Beginners â Part 4","uri":"http://localhost:1313/RantsAndRaves/posts/2012/02/oracle-rman-for-beginners-part-4/"},{"content":"At the end of Oracle RMAN for Beginners â Part 2 I had created a few backups of the ant12 database. In this part, I explore how to trash a database and recover from that trashing using the cold backups taken.\nBear in mind that any work done after the cold backup was taken will be lost. With a cold backup, you cannot roll forward to re-apply archived logs etc because when the backup was taken, the database was in a consistent state.\nA cold backup, taken of a database running in archivelog mode may still require archived logs to be present to restore the backup. Document 337450.1 on My Oracle Support has the details. This affects databases running RMAN version 9.2.0.1 through 10.2.0.5.\nThis could explain why I\u0026rsquo;m not seeing it as I\u0026rsquo;m restoring an 11.2 RMAN backup.\nThanks to David Farkough (@Farkough) for this information.\nCold Recovery â Backupset Type In order to restore a database I first need a broken database. To see what files I need to destroy, I can use the RMAN command report schema as follows:\n1 RMAN\u0026gt; report schema; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Report of database schema for database with db_unique_name ANT12 List of Permanent Datafiles =========================== File Size(MB) Tablespace RB segs Datafile Name ---- -------- -------------------- ------- ------------------------ 1 600 SYSTEM *** /srv/nffs/oradata/ant12/data/system01.dbf 2 500 SYSAUX *** /srv/nffs/oradata/ant12/data/sysaux01.dbf 3 512 UNDOTBS1 *** /srv/nffs/oradata/ant12/data/undotbs01.dbf 4 700 PERFSTAT *** /srv/nffs/oradata/ant12/data/perfstat01_01.dbf 5 10 TOOLS *** /srv/nffs/oradata/ant12/data/tools01.dbf 6 10 USERS *** /srv/nffs/oradata/ant12/data/users01.dbf 7 300 AUDIT01 *** /srv/nffs/oradata/ant12/data/audit01_01.dbf 8 1024 NLWLDELFTFEWSDAT01 *** /srv/nffs/oradata/ant12/data/NLWLDELFTFEWSMCDat01_01.dbf 9 350 NLWLDELFTFEWSIDX01 *** /srv/nffs/oradata/ant12/data/NLWLDELFTFEWSMCIdx01_01.dbf 10 3224 NLWLDELFTFEWSLOB01 *** /srv/nffs/oradata/ant12/data/NLWLDELFTFEWSModDat01_01.dbf 11 128 UTILITY01 *** /srv/nffs/oradata/ant12/data/utility01_01.dbf 12 500 XDB *** /srv/nffs/oradata/ant12/data/xdb01.dbf List of Temporary Files ======================= File Size(MB) Tablespace Maxsize(MB) Tempfile Name ---- -------- -------------------- ----------- -------------------- 1 200 TEMP 200 /srv/nffs/oradata/ant12/data/temp01.dbf I shall start simple, and destroy the TOOLS tablespace by deleting the file.\n1 2 3 RMAN\u0026gt; shutdown; database dismounted Oracle instance shut down 1 2 RMAN\u0026gt; host \u0026#34;mv /srv/nffs/oradata/ant12/data/tools01.dbf /srv/nffs/oradata/ant12/data/tools01.dbf.old\u0026#34;; host command complete 1 RMAN\u0026gt; startup 1 2 3 4 5 6 7 8 9 connected to target database (not started) Oracle instance started database mounted RMAN-00571: =========================================================== RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS =============== RMAN-00571: =========================================================== RMAN-03002: failure of startup command at 02/06/2012 16:14:41 ORA-01157: cannot identify/lock data file 5 - see DBWR trace file ORA-01110: data file 5: \u0026#39;/srv/nffs/oradata/ant12/data/tools01.dbf\u0026#39; You will note that the database didn\u0026rsquo;t open, but is stuck in a MOUNTed state. This is required to carry out a recovery. Lets restore the cold backup.\n1 2 RMAN\u0026gt; connect target / connected to target database: ANT12 (DBID=2799264292, not open) 1 RMAN\u0026gt; restore database; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Starting restore at 2012/02/06 16:15:49 using target database control file instead of recovery catalog allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=18 device type=DISK ... skipping datafile 1; already restored to file /srv/nffs/oradata/ant12/data/system01.dbf skipping datafile 2; already restored to file /srv/nffs/oradata/ant12/data/sysaux01.dbf skipping datafile 3; already restored to file /srv/nffs/oradata/ant12/data/undotbs01.dbf skipping datafile 4; already restored to file /srv/nffs/oradata/ant12/data/perfstat01_01.dbf skipping datafile 6; already restored to file /srv/nffs/oradata/ant12/data/users01.dbf skipping datafile 7; already restored to file /srv/nffs/oradata/ant12/data/audit01_01.dbf skipping datafile 8; already restored to file /srv/nffs/oradata/ant12/data/NLWLDELFTFEWSMCDat01_01.dbf skipping datafile 9; already restored to file /srv/nffs/oradata/ant12/data/NLWLDELFTFEWSMCIdx01_01.dbf skipping datafile 10; already restored to file /srv/nffs/oradata/ant12/data/NLWLDELFTFEWSModDat01_01.dbf skipping datafile 11; already restored to file /srv/nffs/oradata/ant12/data/utility01_01.dbf skipping datafile 12; already restored to file /srv/nffs/oradata/ant12/data/xdb01.dbf channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00005 to /srv/nffs/oradata/ant12/data/tools01.dbf channel ORA_DISK_1: reading from backup piece /media/oracle_backups/ant12/backupset_4.rman channel ORA_DISK_1: piece handle=/media/oracle_backups/ant12/backupset_4.rman tag=TAG20120206T154606 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:07 Finished restore at 2012/02/06 16:16:00 1 2 RMAN\u0026gt; alter database open; database opened How easy was that then?\nWhat happens if I\u0026rsquo;ve lost the SYSTEM tablespace then? Let\u0026rsquo;s see:\n1 2 3 4 RMAN\u0026gt; shutdown database closed database dismounted Oracle instance shut down 1 2 RMAN\u0026gt; host \u0026#34;mv /srv/nffs/oradata/ant12/data/system01.dbf /srv/nffs/oradata/ant12/data/system01.dbf.old\u0026#34;; host command complete 1 RMAN\u0026gt; startup 1 2 3 4 5 6 7 8 9 connected to target database (not started) Oracle instance started database mounted RMAN-00571: =========================================================== RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS =============== RMAN-00571: =========================================================== RMAN-03002: failure of startup command at 02/06/2012 16:20:45 ORA-01157: cannot identify/lock data file 1 - see DBWR trace file ORA-01110: data file 1: \u0026#39;/srv/nffs/oradata/ant12/data/system01.dbf\u0026#39; As before, we have \u0026ldquo;lost\u0026rdquo; a data file, this time making up the SYSTEM tablespace. Can we recover from this? Of course!\n1 RMAN\u0026gt; restore database; 1 2 3 4 5 6 7 8 9 10 11 12 13 Starting restore at 2012/02/06 16:39:43 using channel ORA_DISK_1 skipping datafile 2; already restored to file /srv/nffs/oradata/ant12/data/sysaux01.dbf ... channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00001 to /srv/nffs/oradata/ant12/data/system01.dbf channel ORA_DISK_1: reading from backup piece /media/oracle_backups/ant12/backupset_3.rman channel ORA_DISK_1: piece handle=/media/oracle_backups/ant12/backupset_3.rman tag=TAG20120206T153851 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:02:36 Finished restore at 2012/02/06 16:42:20 And now I can open up the database again for use, or can I?\n1 RMAN alter database open; 1 2 3 4 5 6 RMAN-00571: =========================================================== RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS =============== RMAN-00571: =========================================================== RMAN-03002: failure of startup command at 02/06/2012 16:43:49 ORA-01113: file 1 needs media recovery ORA-01110: data file 1: \u0026#39;/srv/nffs/oradata/ant12/data/system01.dbf\u0026#39; It appears not! Even though I\u0026rsquo;ve recovered the SYSTEM tablespace file(s) I still have potential updates etc in the online redo logs. A quick recover database command should resolve any issues:\n1 RMAN\u0026gt; recover database; 1 2 3 4 5 6 7 Starting recover at 2012/02/06 16:45:29 using channel ORA_DISK_1 starting media recovery media recovery complete, elapsed time: 00:00:02 Finished recover at 2012/02/06 16:45:32 Now, I should be able to open up the database:\n1 2 RMAN\u0026gt; alter database open; database opened Job done. All recovered and usable.\nIt is not possible to recover a single data file, or tablespace, from a cold backup if the database has been updated in any way since the cold backup was taken unless the database is running in ARCHIVELOG mode.\nIf the database is in NOARCHIVELOG mode, then all you can do is restore the complete database. The next part gives a demonstration of this in action.\nIn ARCHIVELOG mode, RMAN will notice if any of the files have been updated, as shown in the following example where changes have been made to the database since the cold backup and \u0026ldquo;suddenly\u0026rdquo; a data file goes missing.\nIn this example, the user norman created a new table named test in the users tablespace, and added a single row to it. Then the file making up the users tablespace was renamed.\n1 RMAN\u0026gt; startup; 1 2 3 4 5 6 7 8 9 connected to target database (not started) Oracle instance started database mounted RMAN-00571: =========================================================== RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS =============== RMAN-00571: =========================================================== RMAN-03002: failure of startup command at 02/06/2012 20:26:48 ORA-01157: cannot identify/lock data file 6 - see DBWR trace file ORA-01110: data file 6: \u0026#39;/srv/nffs/oradata/ant12/data/users01.dbf\u0026#39; As we are running in archivelog mode, we can recover only the users tablespace from the cold backup, and using the archive logs, recover the transactions.\n1 RMAN\u0026gt; restore tablespace users; 1 2 3 4 5 6 7 8 9 10 11 12 Starting restore at 2012/02/06 20:28:40 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=18 device type=DISK ... channel ORA_DISK_1: starting datafile backup set restore channel ORA_DISK_1: specifying datafile(s) to restore from backup set channel ORA_DISK_1: restoring datafile 00006 to /srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: reading from backup piece /media/oracle_backups/ant12/backupset_4.rman channel ORA_DISK_1: piece handle=/media/oracle_backups/ant12/backupset_4.rman tag=TAG20120206T154606 channel ORA_DISK_1: restored backup piece 1 channel ORA_DISK_1: restore complete, elapsed time: 00:00:07 Finished restore at 2012/02/06 20:28:49 1 RMAN\u0026gt; recover tablespace users; 1 2 3 4 5 6 7 Starting recover at 2012/02/06 20:29:16 using channel ORA_DISK_1 starting media recovery media recovery complete, elapsed time: 00:00:00 Finished recover at 2012/02/06 20:29:16 1 2 RMAN\u0026gt; alter database open; database opened That saves time as all we had to do was recover a single tablespace, which in this case consists of only one data file, and then reapply the committed transactions from the archivelogs which, handily, were still online. Had they not been online, RMAN would have found them in a backup and used those to recover the database.\nThis, of course, relies on a complete set of archived logs being found either online or in a backup. If any are missing, the database cannot be recovered fully.\nRestoring a Cold Backup With a Newer Controlfile Be aware that if your cold backup is \u0026ldquo;of some age\u0026rdquo; and the database has moved on since it was taken, restoring the database back to the cold backup will work fine, and most normally, give no errors. However, when you try to opoen the database, you will be told that one or other file wasn\u0026rsquo;t restored from an old enough backup.\nAn intriguing error message indeed, plus, the database will not open. In this situation, your contolfile is too new! All you have to do is as follows:\nFind the most recent backup of the controlfile that occurred prior to the cold backup that you have just restored. List backup of controlfile will suffice. Note the Handle - it will be something like \u0026ldquo;c-DBID-yyyymmdd-nn\u0026rdquo; where DBID is the database ID and yyyymmdd-nn is the date and a sequence number. Shut down the database, then startup nomount. Restore controlfile from \u0026ldquo;c-DBID-yyyymmdd-nn\u0026rdquo; You now have the database files restored and the controlfile suitable for use, also restored. You can now open the database with alter database open resetlogs.\n","description":"","id":130,"section":"posts","tags":null,"title":"Oracle RMAN for Beginners â Part 3","uri":"http://localhost:1313/RantsAndRaves/posts/2012/02/oracle-rman-for-beginners-part-3/"},{"content":"At the end of Oracle RMAN for Beginners - Part 1 I was ready to begin a backup of the ant12 database. Read on \u0026hellip;\nCold Backup - Backupset Type There are two different types of cold backup. The first uses RMAN\u0026rsquo;s own internal format for the dump files - a backupset - the other uses image copies of the individual database files. In this part I\u0026rsquo;ll concentrate on a backuset type of cold backup. I\u0026rsquo;ll look at image copies later.\nA cold backup is always a full backup. To take a cold backup the database needs to be MOUNTed. This is slightly different from a normal cold backup using the OS tools to copy files, as the database should be SHUTDOWN in that case.\nA cold backup is all that you can do when the database is running in NOARCHIVELOG mode, however, you can take a cold backup of a database running in ARCHIVELOG mode using the commands shown below.\n1 2 RMAN\u0026gt; connect target / connected to target database: ANT12 (DBID=2799264292) 1 2 3 RMAN\u0026gt; shutdown; database dismounted Oracle instance shut down 1 2 3 4 5 RMAN\u0026gt; startup mount; connected to target database (not started) Oracle instance started database mounted ... The following command creates a new backupset in the FRA. The files will be written to the FRA in a directory named backupset/SID.\n1 RMAN\u0026gt; backup full database; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Starting backup at 2012/02/06 09:58:40 allocated channel: ORA_DISK_1 channel ORA_DISK_1: SID=18 device type=DISK channel ORA_DISK_1: starting full datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set input datafile file number=00010 name=/srv/nffs/oradata/ant12/data/NLWLDELFTFEWSModDat01_01.dbf ... input datafile file number=00006 name=/srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: starting piece 1 at 2012/02/06 09:58:41 channel ORA_DISK_1: finished piece 1 at 2012/02/06 09:59:16 piece handle=/srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_06/o1_mf_nnndf_TAG20120206T095841_7lz92krk_.bkp tag=TAG20120206T095841 comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:35 Finished backup at 2012/02/06 09:59:16 Starting Control File and SPFILE Autobackup at 2012/02/06 09:59:16 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2012_02_06/o1_mf_s_774524380_7lz93qbg_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/06 09:59:23 And that\u0026rsquo;s all there is to it. The backup file(s) will be created in the FRA as detailed above by various piece handle messages. You can see the backup details using the list backup command:\n1 RMAN\u0026gt; list backup; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 List of Backup Sets =================== BS Key Type LV Size Device Type Elapsed Time Completion Time ------- ---- -- ---------- ----------- ------------ ------------------- 6 Full 365.34M DISK 00:00:35 2012/02/06 09:59:16 BP Key: 6 Status: AVAILABLE Compressed: NO Tag: TAG20120206T095841 Piece Name: /srv/nffs/flashback_area/ant12/ANT12/backupset/2012_02_06/o1_mf_nnndf_TAG20120206T095841_7lz92krk_.bkp List of Datafiles in backup set 6 File LV Type Ckp SCN Ckp Time Name ---- -- ---- ---------- ------------------- ---- 1 Full 847342 2012/02/06 09:39:40 /srv/nffs/oradata/ant12/data/system01.dbf ... 12 Full 847342 2012/02/06 09:39:40 /srv/nffs/oradata/ant12/data/xdb01.dbf BS Key Type LV Size Device Type Elapsed Time Completion Time ------- ---- -- ---------- ----------- ------------ ------------------- 7 Full 9.70M DISK 00:00:03 2012/02/06 09:59:20 BP Key: 7 Status: AVAILABLE Compressed: NO Tag: TAG20120206T095916 Piece Name: /srv/nffs/flashback_area/ant12/ANT12/autobackup/2012_02_06/o1_mf_s_774524380_7lz93qbg_.bkp SPFILE Included: Modification time: 2012/02/06 09:40:06 SPFILE db_unique_name: ANT12 Control File Included: Ckp SCN: 847342 Ckp time: 2012/02/06 09:39:40 The backup will be created in DB_RECOVERY_DEST/SID/backupset. A separate backup took place to copy the controlfile and spfile so that details of this current backup remain safe.\nIn most cases the SID will be used to determine the output directory in the FRA as specified by the database parameter DB_RECOVERY_DEST. However it is not ORACLE_SID that is actually used but the DB_UNIQUE_NAME initialisation parameter. I used SID as it\u0026rsquo;s easier to type! It is possible to avoid backing up to the FRA by specifying a FORMAT parameter:\n1 RMAN\u0026gt; backup format=\u0026#39;/media/oracle_backups/ant12/ant12_backupset.rman\u0026#39; full database; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Starting backup at 2012/02/06 12:30:20 using channel ORA_DISK_1 channel ORA_DISK_1: starting full datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set input datafile file number=00010 name=/srv/nffs/oradata/ant12/data/NLWLDELFTFEWSModDat01_01.dbf ... input datafile file number=00006 name=/srv/nffs/oradata/ant12/data/users01.dbf channel ORA_DISK_1: starting piece 1 at 2012/02/06 12:30:20 channel ORA_DISK_1: finished piece 1 at 2012/02/06 12:31:45 piece handle=/media/oracle_backups/ant12/ant12_backupset.rman tag=TAG20120206T123020 comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:01:25 Finished backup at 2012/02/06 12:31:45 Starting Control File and SPFILE Autobackup at 2012/02/06 12:31:46 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2012_02_06/o1_mf_s_774524380_7lzl1lcg_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/06 12:31:47 Unfortunately, if you look carefully at the last few lines of output, the controlfile and spfile autobackup have still gone to the FRA and not to the desired location. What to do?\nYou can temporarily turn off the controlfile autobackup, run a database backup followed by a manual controlfile (and spfile) backup and then turn controlfile autobackup back on, as follows. You don\u0026rsquo;t have to use a run {} block as RMAN will happily accept all the commands separately. (This is not always the case!)\n1 2 3 4 5 6 RMAN\u0026gt; run { 2\u0026gt; configure controlfile autobackup off; 3\u0026gt; backup format = \u0026#39;/media/oracle_backups/ant12/backupset_4.rman\u0026#39; full database; 4\u0026gt; backup format = \u0026#39;/media/oracle_backups/ant12/cf_backup_manual.f\u0026#39; current controlfile; 5\u0026gt; configure controlfile autobackup on; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 old RMAN configuration parameters: CONFIGURE CONTROLFILE AUTOBACKUP ON; new RMAN configuration parameters: CONFIGURE CONTROLFILE AUTOBACKUP OFF; new RMAN configuration parameters are successfully stored Starting backup at 2012/02/06 15:46:06 using channel ORA_DISK_1 ... piece handle=/media/oracle_backups/ant12/backupset_4.rman tag=TAG20120206T154606 comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:25 Finished backup at 2012/02/06 15:46:32 Starting backup at 2012/02/06 15:46:32 using channel ORA_DISK_1 channel ORA_DISK_1: starting full datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set including current control file in backup set channel ORA_DISK_1: starting piece 1 at 2012/02/06 15:46:33 channel ORA_DISK_1: finished piece 1 at 2012/02/06 15:46:40 piece handle=/media/oracle_backups/ant12/cf_backup_manual.f tag=TAG20120206T154632 comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:07 Finished backup at 2012/02/06 15:46:40 old RMAN configuration parameters: CONFIGURE CONTROLFILE AUTOBACKUP OFF; new RMAN configuration parameters: CONFIGURE CONTROLFILE AUTOBACKUP ON; new RMAN configuration parameters are successfully stored There has to be an easier way to do this doesn\u0026rsquo;t there? What about when you don\u0026rsquo;t know for sure that the configuration parameter is on or off? I\u0026rsquo;m sure there\u0026rsquo;s a way, and if there is, I\u0026rsquo;ll find it.\nUPDATE there is a way. Simply use the set command instead of configure. For example, the above should be replaced by:\n1 2 3 4 5 RMAN\u0026gt; run { 2\u0026gt; set controlfile autobackup off; 3\u0026gt; backup format = \u0026#39;/media/oracle_backups/ant12/backupset_4.rman\u0026#39; full database; 4\u0026gt; backup format = \u0026#39;/media/oracle_backups/ant12/cf_backup_manual.f\u0026#39; current controlfile; } The set remains in force until the end of the session, or, as in this case, until the end of the run block.\nAnyway, you now have a cold backup of the database and a safety copy of the controlfile. Unfortunately, perhaps, the backup of the controlfile didn\u0026rsquo;t take a backup of the spfile. To do that you need to add in a separate manual copy of the spfile similar to the following:\n1 RMAN\u0026gt; backup format = \u0026#39;/media/oracle_backups/ant12/sp_backup_manual.f\u0026#39; spfile; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Starting backup at 2012/02/06 15:58:31 using channel ORA_DISK_1 channel ORA_DISK_1: starting full datafile backup set channel ORA_DISK_1: specifying datafile(s) in backup set including current SPFILE in backup set channel ORA_DISK_1: starting piece 1 at 2012/02/06 15:58:31 channel ORA_DISK_1: finished piece 1 at 2012/02/06 15:58:32 piece handle=/media/oracle_backups/ant12/sp_backup_manual.f tag=TAG20120206T155831 comment=NONE channel ORA_DISK_1: backup set complete, elapsed time: 00:00:01 Finished backup at 2012/02/06 15:58:32 Starting Control File and SPFILE Autobackup at 2012/02/06 15:58:33 piece handle=/srv/nffs/flashback_area/ant12/ANT12/autobackup/2012_02_06/o1_mf_s_774524380_7lzy59go_.bkp comment=NONE Finished Control File and SPFILE Autobackup at 2012/02/06 15:58:34 Did you notice? The backup of the spfile includes a backup of the controlfile. Is nothing consistent with RMAN? ;-)\nComing soon, restoring and recovering a database from a cold backup.\n","description":"","id":131,"section":"posts","tags":null,"title":"Oracle RMAN for Beginners - Part 2","uri":"http://localhost:1313/RantsAndRaves/posts/2012/02/oracle-rman-for-beginners-part-2/"},{"content":"Introduction One of the things a DBA needs to be aware of, is RMAN. This has been around since Oracle 8 (or was it 8i?) and has been improving since then. It\u0026rsquo;s almost pretty good at 11.2!\nOne of the things that I, as a DBA, need to get to grips with is RMAN. Most of the work I\u0026rsquo;ve been doing for the last few years have not involved very much in the way of RMAN usage. It\u0026rsquo;s time I put an end to my almost complete ignorance.\nI have set myself up a small test system where I can do my own RMAN training. I\u0026rsquo;ll be writing up my results and findings as I go along. Feel free to correct me where I get things wrong - which I no doubt will!\nSensible Prerequisites I have found a few sensible prerequisites to using RMAN. For best results, set your environment up in a similar manner.\nI\u0026rsquo;ll be using a database named ant12 running on a server named hubble.\nThe first irritation I have with RMAN is it\u0026rsquo;s use of dates. They are never expanded to how I like them to be. So run the following command in a shell session:\n1 hubble\u0026gt; export NLS_DATE_FORMAT=\u0026#39;yyyy/mm/dd hh24:mi:ss\u0026#39; Next up, we have the problem that on some Linux/Unix systems the rman command is actually in two places, one in ORACLE_HOME, the other under the X11 installation. Usually, X11 is higher up $PATH than ORACLE_HOME, so we need to be sure we are using the right one.\n1 hubble\u0026gt; alias rman=\u0026#39;$ORACLE_HOME/bin/rman\u0026#39; Using single quotes in the command allows the evaluation of ORACLE_HOME to be carried out at run time - when the rman command is called - not at define time. That way, whatever ORACLE_HOME is current at the time of running RMAN will be the correct one for the database.\nNow we need to make sure that we have set the Oracle environment for the database to be backed up.\n1 2 3 4 # I\u0026#39;ll be using a database called ant12. hubble\u0026gt; . oraenv ORACLE_SID = [ant12] ? ant12 The Oracle base remains unchanged with value /srv/oracle Oracle advise that the database should be using a FLASH RECOVERY AREA (FRA), so let\u0026rsquo;s set the database up to use one:\n1 hubble\u0026gt; sqlplus / as sysdba 1 2 3 4 5 6 7 8 9 10 11 12 13 SQL\u0026gt; -- Commands must be typed in the following order .... SQL\u0026gt; alter system set log_archive_duplex_dest=\u0026#39;\u0026#39;; System altered. SQL\u0026gt; alter system set log_archive_dest=\u0026#39;\u0026#39;; System altered. SQL\u0026gt; alter system set db_recovery_size=19G; System altered. SQL\u0026gt; alter system set db_recovery_dest=\u0026#39;/srv/nffs/flashback_area/ant12\u0026#39;; System altered. There is a reason for not having the archived logs stored within the FRA, it becomes a single point of failure. However, there are a few reasons for doing so, the main one being that you have all files necessary for a recovery in the same place.\nIf you are using the FRA for archived logs, it is probably a good idea to be mirroring the FRA.\nFor my test system, I am putting the archived logs into the FRA.\n1 2 3 4 5 6 SQL\u0026gt; alter system set log_archive_dest_1=\u0026#39;LOCATION=USE_DB_RECOVERY_FILE_DEST\u0026#39;; System altered. SQL\u0026gt; alter system set log_archive_dest_state_1=enable; System altered. Check and Amend Configuration RMAN comes configured with reasonably sensible defaults. However, they are not as desired for my use, so the first thing to do is look at the default settings, and change them.\n1 2 3 # Run the correct version of RMAN for the database #the display the current default settings: hubble\u0026gt; rman target / 1 RMAN\u0026gt; show all; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 RMAN configuration parameters for database with db_unique_name ANT12 are: CONFIGURE RETENTION POLICY TO REDUNDANCY 1; # default CONFIGURE BACKUP OPTIMIZATION OFF; # default CONFIGURE DEFAULT DEVICE TYPE TO DISK; # default CONFIGURE CONTROLFILE AUTOBACKUP OFF; # default CONFIGURE CONTROLFILE AUTOBACKUP FORMAT FOR DEVICE TYPE DISK TO \u0026#39;%F\u0026#39;; # default CONFIGURE DEVICE TYPE DISK PARALLELISM 1 BACKUP TYPE TO BACKUPSET; # default CONFIGURE DATAFILE BACKUP COPIES FOR DEVICE TYPE DISK TO 1; # default CONFIGURE ARCHIVELOG BACKUP COPIES FOR DEVICE TYPE DISK TO 1; # default CONFIGURE MAXSETSIZE TO UNLIMITED; # default CONFIGURE ENCRYPTION FOR DATABASE OFF; # default CONFIGURE ENCRYPTION ALGORITHM \u0026#39;AES128\u0026#39;; # default CONFIGURE COMPRESSION ALGORITHM \u0026#39;BASIC\u0026#39; AS OF RELEASE \u0026#39;DEFAULT\u0026#39; OPTIMIZE FOR LOAD TRUE ; # default CONFIGURE ARCHIVELOG DELETION POLICY TO NONE; # default CONFIGURE SNAPSHOT CONTROLFILE NAME TO \u0026#39;/srv/oracle/product/11gR1/db/dbs/snapcf_ant12.f\u0026#39;; # default We are advised, by Oracle, that we should configure RMAN to always backup the control file (which includes the spfile - if one is in use) if we are not using an RMAN catalogue. This is because without a catalogue we store details of backups in the control file, so we don\u0026rsquo;t want to lose those.\nWe will also configure RMAN to prevent the copying of virgin blocks. These are blocks in data files which have never been used at all. (Being used and then emptied means a block is no longer virgin.) This can help to reduce backup times.\nWe will do all of them in one go, using a run {} block.\n1 2 3 4 RMAN\u0026gt; run { 2\u0026gt; configure controlfile autobackup on; 3\u0026gt; configure backup optimization on; 4\u0026gt; } 1 2 3 4 5 6 7 new RMAN configuration parameters: CONFIGURE CONTROLFILE AUTOBACKUP ON; new RMAN configuration parameters are successfully stored new RMAN configuration parameters: CONFIGURE BACKUP OPTIMIZATION ON; new RMAN configuration parameters are successfully stored We are ready to backup the ant12 database to the FRA.\n","description":"","id":132,"section":"posts","tags":null,"title":"Oracle RMAN for Beginners - Part 1","uri":"http://localhost:1313/RantsAndRaves/posts/2012/02/oracle-rman-for-beginners-part-1/"},{"content":"I\u0026rsquo;ve been doing a bit of Docbook work recently - converting a paper based manual into a Docbook one that can then be used to generate all kinds of different output from the same input file. Very useful.\nI needed to create an index that correctly reflected the contents of the new format rather than simply copying the old one - where the pages would no longer have matched up.\nI found a couple of good articles, namely:\nhttp://www.xml.com/pub/a/2004/07/14/dbndx.html\nand, of course, this one:\nhttp://www.sagehill.net/docbookxsl/GenerateIndex.html\nUPDATE: Of all the people you would expect to get it right, Bob Stayton is the one. Except he got it wrong. In the link above to www.sagehill.net, there is an example thus:\n1 2 3 4 5 6 7 \u0026lt;indexterm class=\u0026#34;startofrange\u0026#34; id=\u0026#34;makestuff\u0026#34;\\\u0026gt; \u0026lt;primary\u0026gt;Makefiles\u0026lt;/primary\u0026gt; \u0026lt;/indexterm\u0026gt; ... \u0026lt;indexterm class=\u0026#34;endofrange\u0026#34; startref=\u0026#34;makestuff\u0026#34;\\\u0026gt; \u0026lt;primary\u0026gt;Makefiles\u0026lt;/primary\u0026gt; \u0026lt;/indexterm\u0026gt; It should be as follows without the \u0026lt;primary\u0026gt; on the endofrange indexterm.\n1 2 3 4 5 6 \u0026lt;indexterm class=\u0026#34;startofrange\u0026#34; id=\u0026#34;makestuff\u0026#34;\\\u0026gt; \u0026lt;primary\u0026gt;Makefiles\u0026lt;/primary\u0026gt; \u0026lt;/indexterm\u0026gt; ... \u0026lt;indexterm class=\u0026#34;endofrange\u0026#34; startref=\u0026#34;makestuff\u0026#34;\\\u0026gt; \u0026lt;/indexterm\u0026gt; If you do it with a primary, you get the page range as expected, but with the final page duplicated, as in:\n1 Subject matter 123-127, 127 Remove the primary and it just works.\n","description":"","id":133,"section":"posts","tags":null,"title":"Docbook - Creating Indexes","uri":"http://localhost:1313/RantsAndRaves/posts/2012/01/docbook-creating-indexes/"},{"content":"It\u0026rsquo;s finally coming! Raspberry Pi has now officially started manufacture of the first production boards. See http://www.raspberrypi.org/archives/509 for details. (Update 25/02/2023: Sorry folks, dead link!)\nSadly, you can also see there why the tax situation in the UK (or possibly because of the EU) is preventing the charity from manufacturing these boards in the UK. (Update 25/02/2023: However, eventually, Sony in Wales started making the boards, and have done so ever since. Yippee!)\nAnd, equally sadly, it may also be true that certain UK manufacturers don\u0026rsquo;t seem to want the extra work either.\nHow sad are we in the UK?\nCheers.\n","description":"","id":134,"section":"posts","tags":null,"title":"Raspberry Pi Goes Into Manufacturing - Yippee!","uri":"http://localhost:1313/RantsAndRaves/posts/2012/01/raspberry-pi-goes-into-manufacturing-yippee/"},{"content":"I\u0026rsquo;m trapped on a plane for four and a bit hours and your bloody child has been screaming it\u0026rsquo;s head off for most of the way.\nThere\u0026rsquo;s little chance of me being able to sleep at all on this flight but somehow that\u0026rsquo;s ok with you because \u0026ldquo;she\u0026rsquo;s only 18 months old.\u0026rdquo;\nDo you think that I, and the other passengers around you, think that that is an acceptable reason for suffering 4 and a bit hours of continual screaming? The answer, if you give a toss, is a resounding \u0026ldquo;NO IT FUCKING ISN\u0026rsquo;T!\u0026rdquo;\nIf I make a noise around your bloody child, you will tell me to \u0026ldquo;be quiet or I will wake the baby\u0026rdquo;, so what makes you think that it\u0026rsquo;s perfectly ok the other way around?\nDon\u0026rsquo;t even think of telling me that she\u0026rsquo;s too young to know better. If that\u0026rsquo;s the case, she\u0026rsquo;s too bloody young to be keeping every passenger aboard this Boeing 737-800 awake - put the damned child in the bloody kennels next time you want a holiday you selfish and inconsiderate pair of idiots!\n(Can you tell I\u0026rsquo;m seriously pissed off?)\nCheers.\nPS. It\u0026rsquo;s 22:05 on new year\u0026rsquo;s eve 2011 and I need to get some sleep before the drive home from Manchester airport and your fucking child is getting right on my tits. Shut it the fuck up NOW!\nPPS. This post written on my Galaxy Tab while airbourne \u0026ldquo;somewhere over Europe\u0026rdquo;. Obviously I can\u0026rsquo;t post it from a plane, so you\u0026rsquo;ll get to see it some time after we have landed.\n","description":"","id":135,"section":"posts","tags":null,"title":"Why Do I Have To Suffer Your Children?","uri":"http://localhost:1313/RantsAndRaves/posts/2012/01/why-do-i-have-to-suffer-your-children/"},{"content":"I knew you could do this:\n1 2 3 4 5 SQL\u0026gt; select 1234567890 as abc from dual; ABC ---------- 1234567890 or\n1 2 3 4 5 SQL\u0026gt; select 1234567890 abc from dual; ABC ---------- 1234567890 But I didn\u0026rsquo;t know that this worked as well \u0026ndash; there\u0026rsquo;s no space between the value and the alias name:\n1 2 3 4 5 SQL\u0026gt; select 1234567890abc from dual; ABC ---------- 1234567890 So I did a bit of playing and discovered that there is a difference if the alias is D or F but no other single character:\n1 2 3 4 5 SQL\u0026gt; select 1234567890d, 1234567890f, 1234567890p from dual; 1234567890D 1234567890F P ----------- ----------- ---------- 1.235E+009 1.235E+009 1234567890 This shows the values in Scientific notation when D or F is used as an alias in this manner, but not if used in this manner:\n1 2 3 4 5 SQL\u0026gt; select 1234567890 d, 1234567890 f, 1234567890 p from dual D F P ---------- ---------- ---------- 1234567890 1234567890 1234567890 Then it gets stranger, note the alias names and the corresponding column names:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SQL\u0026gt; select 1234567890df from dual; F ---------- 1.235E+009 SQL\u0026gt; select 1234567890fd from dual D ---------- 1.235E+009 SQL\u0026gt; select 1234567890fa from dual A ---------- 1.235E+009 I get the impression that a trailing F or D on a number means \u0026ldquo;display as floating point or decimal\u0026rdquo; then the F/D is dropped and the A used as a label. I can\u0026rsquo;t find this in the docs though.\nWorks with strings as well but the F/D thing doesn\u0026rsquo;t appear with strings. Doesn\u0026rsquo;t work - for obvious reasons - with column names.\nUPDATE: Thanks to Maxim on the Oracle-L list, the answer is here (Sorry, another dead link!)\nUPDATE 2: Thanks to Jonathan Lewis also on the Oracle-L list, it seems that Tanel Poder has also come across this. On his blog here.\nCheers,\n","description":"","id":136,"section":"posts","tags":null,"title":"Slightly Weird Oracle Stuff","uri":"http://localhost:1313/RantsAndRaves/posts/2011/12/slightly-weird-oracle-stuff/"},{"content":"This \u0026ldquo;story\u0026rdquo; is doing the rounds on email at the moment, but it made me laugh. Grumpy old sod that I am! ;-)\nChecking out at the store, the young cashier suggested to the older woman that she should bring her own grocery bags because plastic bags weren\u0026rsquo;t good for the environment. The woman apologised and explained, \u0026ldquo;We didn\u0026rsquo;t have this green thing back in my earlier days.\u0026rdquo;\nThe clerk responded, \u0026ldquo;That\u0026rsquo;s our problem today. Your generation did not care enough to save our environment for future generations.\u0026rdquo;\nShe was right \u0026ndash; our generation didn\u0026rsquo;t have the green thing in its day.\nBack then, we returned milk bottles, lemonade and beer bottles to the store. The store sent them back to the plant to be washed and sterilised and refilled, so it could use the same bottles over and over. So they really were recycled. But we didn\u0026rsquo;t have the green thing back in our day.\nWe walked up stairs, because we didn\u0026rsquo;t have an escalator in every store and office building. We walked to the grocery store and didn\u0026rsquo;t climb into a 300-horsepower machine every time we had to go two blocks. But she was right. We didn\u0026rsquo;t have the green thing in our day.\nBack then, we washed the baby\u0026rsquo;s nappies because we didn\u0026rsquo;t have the throw-away kind. We dried clothes on a line, not in an energy gobbling machine burning up 3000 Watts every hour \u0026ndash; wind and solar power really did dry our clothes back in our early days. Kids got hand-me-down clothes from their brothers or sisters, not always brand-new clothing. But that young lady is right. We didn\u0026rsquo;t have the green thing back in our day.\nBack then, we had one TV, or radio, in the house \u0026ndash; not a TV in every room. And the TV had a small screen the size of a handkerchief (remember them?), not a screen the size of the state of Montana (See note 1) . In the kitchen, we blended and stirred by hand because we didn\u0026rsquo;t have electric machines to do everything for us. When we packaged a fragile item to send in the mail, we used wadded up old newspapers to cushion it, not Polystyrene or plastic bubble wrap. Back then, we didn\u0026rsquo;t fire up an engine and burn gasoline just to cut the lawn. We used a push mower that ran on human power. We exercised by working so we didn\u0026rsquo;t need to go to a health club to run on treadmills that operate on electricity. But she\u0026rsquo;s right. We didn\u0026rsquo;t have the green thing back then.\nWe drank from a fountain when we were thirsty instead of using a cup or a plastic bottle every time we had a drink of water. We refilled writing pens with ink instead of buying a new pen, and we replaced the razor blades in a razor instead of throwing away the whole razor just because the blade got dull. But we didn\u0026rsquo;t have the green thing back then.\nBack then, people took the tram or a bus, and kids rode their bikes to school or walked instead of turning their parents into a 24-hour taxi service. We had one electrical outlet in a room, not an entire bank of sockets to power a dozen appliances. And we didn\u0026rsquo;t need a computerised gadget to receive a signal beamed from satellites 2,000 miles out in space in order to find the nearest pizza joint.\nBut isn\u0026rsquo;t it sad the current generation laments how wasteful we old folks were just because we didn\u0026rsquo;t have the green thing back then?\nPlease forward this on to another selfish old person who needs a lesson in conservation from a smart-ass young person.\nRemember: Don\u0026rsquo;t make old people mad. We don\u0026rsquo;t like being old in the first place, so it doesn\u0026rsquo;t take much to make us cross. Note\nYes, tes, I know! TVs today use far less power than the old CRT versions of yesteryear - but this is humour ok? And why let facts get in the way\u0026gt; ;-) Cheers.\n","description":"","id":137,"section":"posts","tags":null,"title":"The Green Thing","uri":"http://localhost:1313/RantsAndRaves/posts/2011/12/the-green-thing/"},{"content":"TweetDeck for Linux, for example. Refused to start on me recently because it couldn\u0026rsquo;t find or access the location where I had saved my encrypted details. The problem is caused by Adobe Air not being able to find a running daemon for Gnome Keyring or KWallet, or a corrupted wallet database.\nFull details here http://kb2.adobe.com/cps/492/cpsid_49267.html - works for me!\nIn my case, simply restarting KWalletManager worked fine. And restarting TweetDeck of course - it\u0026rsquo;s unable to pick up the fact that the wallet is available.\nCheers,\nNorm.\n","description":"","id":138,"section":"posts","tags":null,"title":"Adobe Air Applications Can't Find Passwords etc?","uri":"http://localhost:1313/RantsAndRaves/posts/2011/12/adobe-air-applications-cant-find-passwords-etc/"},{"content":"You know the feeling, your 10g or 11g database displays a warning message about your use of deprecated parameters at startup, but it doesn\u0026rsquo;t say which parameters are deprecated?\nYou could look through the manuals to find the list of all deprecated parameters then go hunting in your spfile/pfile for those and remove them, or, you could simply look in the alert.log.\n1 2 3 4 5 6 7 8 ... Starting up ORACLE RDBMS Version: 10.2.0.4. ... Deprecated system parameters with specified values: log_archive_start End of deprecated system parameter listing PMON started with pid=2, OS id=11541 ... There are a few more at 11g:\n1 2 3 4 5 6 7 8 9 10 11 12 13 ... Starting up: Oracle Database 11g Enterprise Edition Release 11.2.0.2.0 ... ... Deprecated system parameters with specified values: log_archive_start hash_join_enabled max_enabled_roles background_dump_dest user_dump_dest End of deprecated system parameter listing PMON started with pid=2, OS id=5292 ... These databases were converted from 9.2.0.8 to 10g and 11g respectively. So, now you know what the deprecated parameters are, go fix them! ;-)\nCheers.\n","description":"","id":139,"section":"posts","tags":null,"title":"Deprecated Parameter Warning on Database Startup","uri":"http://localhost:1313/RantsAndRaves/posts/2011/11/deprecated-parameter-warning-on-database-startup/"},{"content":"I\u0026rsquo;ve been using KDE4 on OpenSuse (11.4 currently) for some time and never had any problems. After a recent patch fix to the system, the volume control buttons on my Dell Vostro stopped responding. Normally pressing volume up or volume down worked instantaneously. Stop and go still worked fine, but not volume.\nWhat would happen is that the on screen indicator would appear after about 45 seconds or so - but the volume wouldn\u0026rsquo;t change until it did, then, it would go to maximum or minimum - depending on which button I\u0026rsquo;d been hammering for 45 seconds! :-)\nA similar thing would happen if I tried to use the Volume control applet in the task bar. Hmmm.\nThe solution is fairly simple:\nStop the kmix process by right-clicking it in the task bar, and choosing the quit option. cd ~/.kde4/share/apps rm -R kmix* Restart Kmix by pressing ALT-F2 and typing kmix into the dialogue that appears. After a few seconds, you get your volume control icon back in the task bar and everything works fine. The problem appears to have been caused by a configuration change which left problems when old config was found in the assorted config files - but don\u0026rsquo;t quote me on that!\nCheers.\nNorm\n","description":"","id":140,"section":"posts","tags":null,"title":"Does Your Volume Control Freeze on KDE?","uri":"http://localhost:1313/RantsAndRaves/posts/2011/11/does-your-volume-control-freeze-on-kde/"},{"content":"Check this link (sorry, it\u0026rsquo;s no longer available, but this link is the product being described - the Philips Urban Beehive.) for details. I\u0026rsquo;m saying nothing!\nExcept, maybe, I really can\u0026rsquo;t see it taking off.\nWell, ok then. How do you stop the queen laying in the honey comb? There doesn\u0026rsquo;t appear to be a queen excluder - so your honey will have all sorts of stuff in it! Larvae, eggs, etc. It really won\u0026rsquo;t work!\nCheers.\n","description":"","id":141,"section":"posts","tags":null,"title":"Indoor Beekeeping? It Won't Catch on!","uri":"http://localhost:1313/RantsAndRaves/posts/2011/11/indoor-beekeeping-it-wont-catch-on/"},{"content":"Using NetBeans (version 7.0.1) to develop Android applications and for no apparent reason, a project that worked yesterday won\u0026rsquo;t run any more. It compiles without error but refuses to run - whether or not the emulator is running - and doesn\u0026rsquo;t produce an error. Hmmm.Â In the source code for the main Java class (yes, I said Java - anyone who knows me will be having a fit right now!) the very first line is flagged with a red (ok, pink!) dot. The line itself simply says:\npackage uk.co.dunbarit.myprojectname;\nRolling the mouse over the line pops up a helpful hint which states that the system cannot find java.lang in classpath or bootclasspath.\nNow, as my work colleagues will tell you, I\u0026rsquo;m not a Java developer by any means. But this was working yesterday, so why not today?\nRight-click the project name in the project tab, and select the option - near the bottom - to rebuild broken project. This pops up with a dialogue box upon which appears the message that I should \u0026ldquo;update the project using SDK tools\u0026rdquo;. Easy? What does that mean?\nWhat it means is this. I have to:\nClose the project in NetBeans and open a terminal session. Navigate to the Android SDK\u0026rsquo;s toolsÂ directory. In there you\u0026rsquo;ll find the android utility. ./android update project --name MyProjectName --path full_path_to_project Job done! Reopen the project in NetBeans and run it. Yippee! By the way, the --path parameter is set to the location where the AndroidManifest.xml file can be found.\nIt appears that the Android SDK updated itself thus rendering something that was legal as no longer legal. Nice one - thanks. I\u0026rsquo;m having enough trouble with Java without having silent updates bollox me as well! ;-)\nCheers.\n","description":"","id":142,"section":"posts","tags":null,"title":"NetBeans Complains That it Can't Find Java.Lang in your Android Project.","uri":"http://localhost:1313/RantsAndRaves/posts/2011/11/netbeans-complains-that-it-cant-find-java-lang-in-your-android-project/"},{"content":"If you install 10g and 11g on the same server, which one do you wish to supply the executables for \u0026ldquo;oraenv\u0026rdquo;?\nIf you install 10g first and allow 11g to overwrite the files in /usr/local/bin when you run \u0026ldquo;root.sh\u0026rdquo; then when you eventually call oraenv to set a 10g environment, you get a warning that \u0026ldquo;$ORACLE_HOME/bin/orabase\u0026rdquo; cannot be found.\nIf you allow the 10g files to overwrite the 11g ones, you don\u0026rsquo;t set ORACLE_BASE. I wonder what problems that might cause?\nCheers.\n","description":"","id":143,"section":"posts","tags":null,"title":"Can We Have 10g and 11g on the Same Server?","uri":"http://localhost:1313/RantsAndRaves/posts/2011/11/can-we-have-10g-and-11g-on-the-same-server/"},{"content":"Install 10.2.0.1 - the base release - of Oracle and deselect the various Enterprise Edition Options - these cost money and we don\u0026rsquo;t like that, especially if we don\u0026rsquo;t use them.\nPatching to 10.2.0.5 (in my installation - it could be different with previous versions) silently adds OLAP, Data Mining etc back into the mix. Not good - especially if/when Oracle decide to audit your licenses. You pay for what you install whether used or not.\nSo, to remedy the problem, do this:\n1 2 cd $ORACLE_HOME/rdbms/lib make -f ins_rdbms.mk olap_off rat_off rac_off dm_off sdo_off ioracle You may see an error telling you that SDO can\u0026rsquo;t be turned off. That\u0026rsquo;s fine, at least we tried! The various options are:\nolap_off = OLAP rat_off = Real Application Testing rac_off = Real Application Cluster dm_off = Data Mining sdo_off = Spatial There are others in the make file, but these are the ones I\u0026rsquo;m mostly interested in not having!\nCheers.\n","description":"","id":144,"section":"posts","tags":null,"title":"Removing \"Rogue\" Expensive Oracle Options After Upgrading to 10.2.0.x.","uri":"http://localhost:1313/RantsAndRaves/posts/2011/11/removing-rogue-expensive-oracle-options-after-upgrading-to-10-2-0-x/"},{"content":"Software efficiency halves every 18 months thus compensating for Moore\u0026rsquo;s Law.\nCheers.\n","description":"","id":145,"section":"posts","tags":null,"title":"Norm's Law of Computing","uri":"http://localhost:1313/RantsAndRaves/posts/2011/10/norms-law-of-computing/"},{"content":"Yesterday I had the pleasure of a day trip to London to attend the Mastering Oracle Trace Data session with Cary Millsap. The day got off to a great start as I arrived at the railway station in Leeds very early to find my train waiting for me at the platform. Excellent! I got on and 20 minutes before departure time, we headed on out of Leeds for London. WTF? The announcement that this was the 06:40 to London Kings Cross and not the 07:00 cleared things up a little.\nGuess what? My ticket was only valid on the 07:00. :-( However, I sought out the guard and she checked my ticket, her timetable and said that \u0026ldquo;as it\u0026rsquo;s only a difference of 6 minutes, don\u0026rsquo;t worry about it\u0026rdquo;. Result! Nice lady.\nAfter arriving in London, finding the location for the session wasn\u0026rsquo;t too bad, well if you exclude the Â£4 per trip to do two stations on the Tube, which was absolutely jam packed. I had to let one train go past as there was simply no room on it for anyone else. Luckily the next one wasn\u0026rsquo;t as packed. (Well, just!)\nThe session itself was an exceedingly interesting day. Cary is an excellent speaker and made what could have been a pretty dry day, very interesting indeed. There were 16 of us in total including one Doug Burns - who I would have thought already knew all this stuff!\nThe lunchtime chicken and mash was just that, chicken and mash. With a spot of gravy and a slice of lemon thrown in for good measure. I decided to get my jacket and nip out for something else, and a little walk. The room was locked.\nIn the afternoon session we looked at lots of ways to use the tools that we would be taking home with us. These are the MR Tools and MR Trace available from www.method-r.com - they appear exceedingly useful indeed. I\u0026rsquo;m currently waiting to obtain my license so that I can get hold of them and start trying them out on a few trace files from my past.\nMr Trace won\u0026rsquo;t be much use to me though, it\u0026rsquo;s an add on for SQL Developer which I don\u0026rsquo;t use, being a Toad man myself. However, I have passed Method R\u0026rsquo;s details on to Quest\u0026rsquo;s product manager for a potential inclusion in a future version of Toad, so maybe one day!\nI really think the course should have been over a couple of days rather than just one, as all too soon, we were all departing for home. But not before we had picked up our exclusive T shirts (not available in the shops!) and posed for a group photo. (Sorry, dead link these days!)\nCheers.\n","description":"","id":146,"section":"posts","tags":null,"title":"A Day in London with Cary Millsap","uri":"http://localhost:1313/RantsAndRaves/posts/2011/09/a-day-in-london-with-cary-millsap/"},{"content":"On Sunday past, I took 11 full \u0026amp; capped super frames of lovely honey from the original hive, and checked the new hive. They are a bit behind and have not yet capped anything off. Hmmm, what are they up to I wonder?\nThe original hive got another full set of super frames to draw out and fill up. Seeing as how it\u0026rsquo;s getting late on in the year, I\u0026rsquo;m not sure we\u0026rsquo;ll get much more, if anything off of them - but you never know.\nThe third hive (well, nucleus box) is going to Peter as soon as we can get the little b*ggers to stay inside so that I can block off the entrance and get them transported. They are up early every morning and they stay out much later that I do at night! Women! ;-)\n","description":"","id":147,"section":"posts","tags":null,"title":"2011 Honey Harvest - Let it Begin!","uri":"http://localhost:1313/RantsAndRaves/posts/2011/08/2011-honey-harvest-let-it-begin/"},{"content":"Great news! (At least for us Oracle types anyway.)\nJonathan Lewis has a new book coming out. The working title appears to be \u0026ldquo;A look at the internal mechanics of the important bits of Oracle for people who arenât planning to become rocket scientists but who do want to do a little more than just push buttons in OEM\u0026rdquo; but this will probably change by November when it is due to be published, all going well.\nJust in time for Christmas!\nCheck out the details here.\nCheers.\n","description":"","id":148,"section":"posts","tags":null,"title":"Jonathan Lewis Has A New Book Coming Soon ....","uri":"http://localhost:1313/RantsAndRaves/posts/2011/07/jonathan-lewis-has-a-new-book-coming-soon/"},{"content":"Except, it\u0026rsquo;s not!\nWTF is it all about? You open a report, or a book and see a page with no text on it except this page intentionally left blank. Except, because of that bit of text, it\u0026rsquo;s no longer blank is it?\nSo why do they bother? Does the author/publisher think we readers are too stupid to realise that a page is blank without being told? Is it because they think we might assume that somehow, the printing process has mysteriously failed for that one page in particular and we may be missing some highly important information? Or what?\nIf they really must tell us that the page was never intended to have anything useful on it, surely this page has no useful information on it could be a better idea. Except, pedants like me, might assume that because of what it is telling us, that it does indeed contain useful information.\nI give up. Just leave the blank pages as blanks and we\u0026rsquo;ll probably get the message that it has no content and is simply being used to ensure that the first page of the next section, part, chapter, whatever, starts on the generally accepted face of the page. It\u0026rsquo;s not rocket science!\nCheers,\nNorm.\n","description":"","id":149,"section":"posts","tags":null,"title":"This Page Intentionally Left Blank ...","uri":"http://localhost:1313/RantsAndRaves/posts/2011/06/this-page-intentionally-left-blank/"},{"content":"Ever wanted to parse /etc/oratab but ignore all the comments and blank lines? So did I. Here\u0026rsquo;s how \u0026hellip;\nI can\u0026rsquo;t claim all the credit for this, it is based on something I was doing plus a bit of \u0026ldquo;stolen\u0026rdquo; code from SLES.\n1 2 3 4 5 6 7 8 9 OLDIFS=$IFS IFS=: grep -v \u0026#39;^\\(#\\|$\\)\u0026#39; /etc/oratab |\\ while read ORASID ORAHOME AUTOSTART do ## Do what you like here with ## $ORASID, $ORAHOME and $AUTOSTART ## done IFS=$OLDIFS Cheers,\nNorm.\n","description":"","id":150,"section":"posts","tags":null,"title":"How To Extract Details From /etc/oratab on Linux","uri":"http://localhost:1313/RantsAndRaves/posts/2011/06/how-to-extract-details-from-etcoratab-on-linux/"},{"content":"Me too. Took ages to hit the \u0026ldquo;duh\u0026rdquo; moment, then it became pretty obvious! The file /etc/init.d/oracle also known as rcoracle to root users can be used to do a number of things such as starting the databases, starting the (default LISTENER) listener, CRS etc but, as I eventually found out, you have to configure it to do so!\nThe configuration file is /etc/sysconfig/oracle.\nMost of the options are defaulted to off, except for the setting of kernel parameters (SET_ORACLE_KERNEL_PARAMETERS=\u0026quot;yes\u0026quot;) which is very useful.\nCheers.\n","description":"","id":151,"section":"posts","tags":null,"title":"Wondering Why The Oracle Databases Won't Start With A SLES Reboot?","uri":"http://localhost:1313/RantsAndRaves/posts/2011/06/wondering-why-the-oracle-databases-wont-start-with-a-sles-reboot/"},{"content":"After a server reboot, your (windows) database stays down, even though you have it set to come up automatically - why? The services have started, just not the database.\nThe following is a clue:\n1 sqlplus \u0026#34;/ as sysdba\u0026#34; 1 2 3 4 5 SQL*Plus: Release 10.2.0.3.0 - Production on Tue Jun 21 07:29:08 2011 Copyright (c) 1982, 2006, Oracle. All Rights Reserved. ERROR: ORA-01031: insufficient privileges You now have two options to check, and both must be confirmed:\nIs the oracle user a member of the ora_dba group? Is sqlnet.ora correctly configured? To check the first, proceed as follows (on the database server!):\nStart-\u0026gt;Programs-\u0026gt;Administrative Tools-\u0026gt;Computer Management. Double-click Local Users and Groups. Double-click Groups. Double-click ora_dba (on the right) if it exists. If it doesn\u0026rsquo;t, it should - so have it added. The list of members of the ora_dba group should appear. Make sure that the oracle user is listed. If not, add it to the group. If you made changes, logout of the oracle user and back in again, then continue. To correctly configure sqlnet.ora, make sure that the following line appears:\nSQLNET.AUTHENTICATION_SERVICES=(NTS)\nIf it is not found, then the default is (NONE) which requires a username and password always be supplied when connecting as sysdba or sysoper.\nIf it is set to anything else, other than (ALL), then it should be ok - although some of the other settings may not use (NTS) - you need to test on your system to be sure.\nNow that you can connect \u0026quot;/ as sysdba\u0026quot;, you can check the oradim.log (%oracle_home%\\database\\oradim.log) to see if there are any of these errors corresponding to your server reboot times:\n1 2 3 oradim.exe -startup ... ORA-01017: invalid username/password; login denied You may also find entries in the application pages of the event viewer with similar messages:\n1 ORACLE. \u0026#34;CONNECT\u0026#34; DATABASE USER: \u0026#34;/\u0026#34; PRIVILEGE: NONE CLIENT_USER: oracle CLIENT_TERMINAL: STATUS: 1031 The alert log shows no errors whatsoever. It won\u0026rsquo;t, you didn\u0026rsquo;t get that far!\nIf the oradim.log shows no errors of the above kind, you may not have the database configured to autostart. So, delve into the registry.\nIf the two following keys exist, and have the value true, then all should be well on the next restart. Otherwise add/edit them manually. Setting both to true ensures that when you start and stop the services the database will also start and stop.\nWhen the server is rebooted, the services are told to stop, so they will stop the database cleanly before the server goes down. The reverse is true on a server startup, the services are told to start and they tell the database to come on up!\n1 2 HKLM/software/oracle/key_**ora_home_name**/ora_**oracle_sid**_autostart HKLM/software/oracle/key_**ora_home_name**/ora_**oracle_sid**_shutdown If all of the above pan out, next time you restart the server, the database will come up!\nYou can test without bouncing the server simply by shutting down and restarting the database service from control panel. On a restart of the service, the database should also be back up and running. If not, get into the oradim.log again, and fix the problem you find.\nCheers.\n","description":"","id":152,"section":"posts","tags":null,"title":"Does Your Windows Oracle Database Stay Down After A Server Reboot?","uri":"http://localhost:1313/RantsAndRaves/posts/2011/06/does-your-windows-oracle-database-stay-down-after-a-server-reboot/"},{"content":"Mine did, not any more though. Read on.\nI was just a tad annoyed at the sudden increase in s-l-o-o-o-o-w responses after starting OpenSuse 11.4 up. I noticed that the hard drive indicator LED was continually lit for about 5-10 minutes after bootup. Performance was abysmal during this time.\nTop showed almost no CPU being used, so I looks ad vmstat instead. This was the result, on an idle (and slow) system:\n1 2 3 4 5 vmstat -d disk ... -----IO------ ... cur sec sda8 ... 0 898 In the above, I\u0026rsquo;ve removed the individual reads and writes columns, to sort out the wheat from the chaff. I\u0026rsquo;m interested in I/Os overall. I\u0026rsquo;ve also removed the partitions that were showing 0 or 1 as the total I/O count. The figures showed me that partition sda8 was getting hit to the tune of almost 900 I/O operations per second. Who or what is partition sda8?\n1 2 3 df -h | grep -i sda8 /dev/sda8 251G 131G 108G 55% /data` Now I know, so what\u0026rsquo;s open on this mount point?\n1 2 3 4 5 lsof /data COMMAND PID USER NODE NAME preload 385 root /data/VirtualBox/ScientificLinuxEnterprise6.vdi preload 385 root /data/VirtualBox/LinuxMint64bit.vdi And having noted the above, why on earth was anything loading my virtual drives when I\u0026rsquo;m not even running VirtualBox at the moment?\n1 2 3 4 ps -ef | grep 385 root 385 1 1 11:15 ? 00:00:11 /sbin/preload /var/cache/preload/prepared A quick check of the internet showed that preload is a process to save boot time (how ironic) and that it can easily be disabled. So I disabled it. No more long boots now! To disable preload, run yast and select System Services (Runlevel).\nTick the Expert Mode radio button. Scroll down to find boot.startpreload At the bottom of the screen, uncheck the B box. Click OK. Cheers,\nNorm.\n","description":"","id":153,"section":"posts","tags":null,"title":"Does Your Hard Disc Lights Stay On For Ages After Booting Linux?","uri":"http://localhost:1313/RantsAndRaves/posts/2011/06/does-your-hard-disc-lights-stay-on-for-ages-after-booting-linux/"},{"content":"One of Alison\u0026rsquo;s ex work colleagues, Andy, keeps bees. So far this year he has had 4 swarms! We took advantage of his generosity and restocked our recently dead hive with one of his swarms, and then, a couple of days later we obtained another of his swarms in a (borrowed - thanks Peter) nucleus box. We now have two and a half hives.\nOur own hive, looks to have requeened itself - my last inspection showed no red queen, and what appeared to be an unmarked queen laying. Hmmm. There were, however, no signs of any queen related waxworks going on. I\u0026rsquo;m confused!\nCheers.\n","description":"","id":154,"section":"posts","tags":null,"title":"Bees Coming Out Of Our Ears!","uri":"http://localhost:1313/RantsAndRaves/posts/2011/05/bees-coming-out-of-our-ears/"},{"content":"How hard can it be? Well, as it turns out, not very. The steps are as follows:\nInstall, or check, that you have wine (latest version) installed. The command wine --version gives details, assuming that you already have it installed. Download \u0026ldquo;Kindle for PC\u0026rdquo; from Amazon\u0026rsquo;s web site. Double-click the Kindle exe file to install it under wine. If you see errors about \u0026ldquo;winesetup encountered an error\u0026rdquo; just ignore them. You will have a new entry in your applications menu for Kindle. How difficult was that? Here is Kindle for PC running on OpenSuse 11.4 with a KDE desktop:\nA couple of things don\u0026rsquo;t work as they do in Windows, for example, shopping in the Kindle Store opens up in your default browser, which is not really a problem - you get full screen browsing. As far as I can see, synchronising with your other Kindle(s) works fine - I\u0026rsquo;m running Kindle for Android as well as Kindle for PC and they happily sync with each other.\nI also see the following wine problem whenever I start Kindle, but this occurs on a couple of other wine programs as well, and did previously in OpenSuse 11.2 after a patch to wine. So far, nothing has been affected.\nCheers,\nNorm.\n","description":"","id":155,"section":"posts","tags":null,"title":"Getting Amazon \"Kindle For PC\" running on Linux.","uri":"http://localhost:1313/RantsAndRaves/posts/2011/05/getting-amazon-kindle-for-pc-running-on-linux/"},{"content":"I\u0026rsquo;ve been checking the girls today - the weather is lovely and warm and they are flying around all over the place, returning with lots of pollen and, hopefully, nectar.\nWe\u0026rsquo;ve been away for a couple of weeks and before we went, I fed them on about 5 pounds of honey and put a super on with last years frames - fresh (!) from extraction - in case the girls needed something to do while we were away. Unfortunately, due to being in a hurry and getting ever so slightly confused, I put the queen excluder above the super rather than below it. Sigh! Today, I rebuilt the hive correctly after checking that all was well in camp.\nIt seems that the queen has not moved up into the super anyway as there was no sign of laying in the super frames. This is good news, there is, however, a little honey in a couple of frames, so they seem to be getting on with things already!\nNow we have (or had?) a red marked queen in our original swarm and this makes her 4 years old this year which, I\u0026rsquo;m told, is past her best. However, I didn\u0026rsquo;t see her at all today when I was inspecting the hive but we do have larvae in various stages of development from tiny little newly hatched ones up to lots of fully developed and capped over cells - so someone is laying. There are a few drones too, but not too many. So far, there is very little drone comb around. So we definitely have a queen doing the laying - but who?\nI did see what I think was a queen bee - she looked different to the others and was certainly not a drone - wandering about over the comb, sticking her back end into vacant cells, pausing, then moving on - all of which is queen behaviour as far as I\u0026rsquo;m aware, but she wasn\u0026rsquo;t marked with a red spot.\nI\u0026rsquo;m wondering if the swarm decided to re-queen all by themselves while we were away, or even before we went. Today was the first physical inspection we\u0026rsquo;ve made since spring as the weather has been pretty chilly and damp. The only opening of the hive we\u0026rsquo;ve done up till now has been to feed the girls.\nI didn\u0026rsquo;t see anything that looked like an open queen cell on any of the comb or frames, so if they have re-queened, then I\u0026rsquo;ve got no idea where she came from! There was one large cell, open, on one frame, but it looked more like a drone cell than a queen cell. Confused? I certainly am!\nSo there you have it, bees don\u0026rsquo;t seem to need us humans as much as we seem to need them. We are still eating last years honey - Alison makes the finest honey flapjack in the world with nothing more than oats, honey and juicy sultanas - yum!\nVarroa counts are low, I saw only a few when I did my count today, and it worked out at less than one per day over the last couple of weeks. I\u0026rsquo;m extremely suspicious of this, but gave them a dose of icing sugar anyway - just to be on the safe side.\nCheers.\n","description":"","id":156,"section":"posts","tags":null,"title":"Do We Have A New Queen, Or What?","uri":"http://localhost:1313/RantsAndRaves/posts/2011/05/do-we-have-a-new-queen-or-what/"},{"content":"I\u0026rsquo;ve taken far too long to write this up, but I\u0026rsquo;ve been rather pissed off to be honest! We\u0026rsquo;ve lost one hive of bees since February. They were alive when I checked and I\u0026rsquo;ve been feeding them but I noticed that the feed wasn\u0026rsquo;t going down. Due to the weather, I couldn\u0026rsquo;t \u0026ldquo;go in\u0026rdquo; and see what was what in case I did them some harm with the cold and damp. I wish I had to be honest.\nWe had a warm day about three weeks ago, so I looked in, still food left, but no sound, no movement from within. Fearing the worst, I lifted the crown board a little to peek inside, nothing. Hmmm not good.\nGiven that it was reasonably warm, and that I had my cloth cover with me, I whipped off the crown board and I was met with a scene of carnage. Every single bee was dead and they were all lying in a heap on the floor. Why? They had starved to death.\nEven with the fact that there was lots of feed on hand and close by, it seems as if they refused to leave the cluster and eat. This happens (I\u0026rsquo;ve been told by Peter) and there\u0026rsquo;s not a lot we can do. The cold, damp weather may have kept them in the cluster and they would not have moved away to feed - starving themselves to death while there was plenty of food all around.\nEvery cell in each of the 5 frames that had drawn out last year was totally clean. A couple of bees had died \u0026ldquo;head down\u0026rdquo; in the cells looking for the last of the stores that they had accumulated. I find this all quite distressing - so much food, so many deaths.\nThis was our first swarmed hive too, with a new queen. We still have lots of bees in the original hive, but Queenie is a \u0026ldquo;red spot\u0026rdquo; and is now in her fourth year, so we may need to re-queen to keep that hive going strongly. We don\u0026rsquo;t want her to be laying drones all the time - that\u0026rsquo;s fatal for a colony.\nCheers.\n","description":"","id":157,"section":"posts","tags":null,"title":"Bee Tragedy!","uri":"http://localhost:1313/RantsAndRaves/posts/2011/04/bee-tragedy/"},{"content":"Domino, our California Kingsnake was put to sleep tonight at 19:10.\nShe\u0026rsquo;s been in the family for about 13 or 14 years with never a problem, well, except for what you can see below - she got me one night and had my finger down her throat before we could do anything about it. Alison decided to take a picture rather than get her off me!\nIt eventually took over an hour and a half, and judicious use of January ice cold water (as advised by the RSPCA) to get her off, safely.\nWhat you may not be able to make out in the picture above is the fact that she has effectively tied my wrists together - she had quite a grip, I can tell you. Mind you, she is a constrictor, so she would be strong in that department!\nRecently, she developed a lump on her tail. We took her to the vet but after a course of treatment, there was no response. She hasn\u0026rsquo;t eaten now for two weeks - not a problem in itself, she\u0026rsquo;s gone much longer without in the past, but she has lost a lot of her muscle tone and her spine was showing quite prominently. Even the vet was surprised at how much she had gone downhill since she last saw her.\nWe shall miss her. :-(\nCheers.\n","description":"","id":158,"section":"posts","tags":null,"title":"Domino is gone","uri":"http://localhost:1313/RantsAndRaves/posts/2011/03/domino-is-gone/"},{"content":"Earthquake, tsunami and nuclear problems. Like they don\u0026rsquo;t have enough trouble with just one of these?\nAnd as usual, the religious fuck-wits are coming out of the woodwork with their completely ridiculous claims that it\u0026rsquo;s all \u0026ldquo;someone\u0026rsquo;s\u0026rdquo; fault - be that someone the gays, women (it\u0026rsquo;s usually one of these groups to blame for everything according to the fundies!) or someone else.\nMind you, the Daily Mail isn\u0026rsquo;t much better, reporting on a astrologist (did you know, astrologist is an anagram of fuckwit?) who \u0026ldquo;predicted\u0026rdquo; the carnage. Read more, if you haven\u0026rsquo;t yet eaten your lunch yet, because it\u0026rsquo;s sickening, here.\n","description":"","id":159,"section":"posts","tags":null,"title":"Japanese Tragedy","uri":"http://localhost:1313/RantsAndRaves/posts/2011/03/japanese-tragedy/"},{"content":"Yesterday, 24th February 2011, was about the first reasonably warm day of the year. The pond and paths around our house are now filled with frogs. It must be spring!\n","description":"","id":160,"section":"posts","tags":null,"title":"The Frogs Are Back!","uri":"http://localhost:1313/RantsAndRaves/posts/2011/02/the-frogs-are-back/"},{"content":"On Sunday 6th February I hefted the hives as usual. The new hive felt a bit on the light side so I whipped the lid off (just the lid) to take a peek - it was quite mild or I wouldn\u0026rsquo;t have disturbed them.\nLifting the card I use to block the ventilation from passing through the cluster, I saw lots of live bees! Yippee! Things are looking good for a second successful winter, and this year was a cold one!\nAnyway, we have a pile of honey from the natural comb from last year so I decided to get some on as the girls felt a bit light to me (this is our first ever \u0026ldquo;home grown\u0026rdquo; swarm and as yet are still only on 5 brood frames). I\u0026rsquo;ve added a super and put a feeder in with some of their honey. A dribble went down the central hole to get them interested - just in case.\nHopefully the weather will stay on the mild side and they\u0026rsquo;ll be able to get up and feed. I\u0026rsquo;d rather feed them honey than fondant - that needs a lot of water to dilute just so that they can evaporate the water again! Doesn\u0026rsquo;t make sense to me. :-)\nWhile I was pouring honey into the feeder, a couple of bees flew out of our main hive, and took a look at me before nipping back inside the hive. Looks like we may have managed to get two hives through the winter.\nThis is our second year keeping bees, our second winter and it seems we still have beginner\u0026rsquo;s luck!\nWe may need to re-queen our main hive this year too, she\u0026rsquo;s red and now at least 3 years old. I need advice \u0026hellip;\nQueens are marked for the year they were born:\nWhite Yellow Red Green Blue - calendar years 1 and 6 are white, 2 and 7 are yellow and so on, up to 5 and 0 which are blue. This year, 2011, is a white year and our queen is red, so she\u0026rsquo;s most likely to be 2008. Apparently after 3 years there is a strong possibility that she is running out of stored sperm and may start laying nothing but drones. Fatal for a hive.\nCheers.\n","description":"","id":161,"section":"posts","tags":null,"title":"The Bees are alive!","uri":"http://localhost:1313/RantsAndRaves/posts/2011/02/the-bees-are-alive/"},{"content":"I was out this Sunday hefting the hive. It\u0026rsquo;s feeling a little lighter than I remember, so I suspect the girls are tucking in to the stores now. I heard from Peter, about a fortnight ago, that his girls have started laying already so it\u0026rsquo;s looking good for Spring.\nAs I was hefting, one solitary bee flew out from the mouse guard and buzzed around me to have a look, then vanished back into the hive. This is good because it means that we have at least one bee that survived the winter!\nI\u0026rsquo;ve got some honey from the Autumn crop all ready to feed them with, if they need it. Much better than sugar syrup if you ask me.\nCheers.\n","description":"","id":162,"section":"posts","tags":null,"title":"Signs Of Life?","uri":"http://localhost:1313/RantsAndRaves/posts/2011/01/signs-of-life/"},{"content":"Ever wondered how you can call a packaged procedure (or function) which resides at the far end of a database link? Wonder no more!\n1 2 3 begin package_name.Procedure_name@db_link_name(parameter, parameter, ...); end; Calling a function is just as easy:\n1 2 3 4 declare vResult number(10); begin vResult := package_name.Function_name@db_link_name(parameter, parameter, ...); end; Unfortunately, you don\u0026rsquo;t appear to be able to do this sort of thing via a synonym:\n1 2 3 4 5 create or replace synonym X for package_name@db_link_name; begin X.Procedure_name(parameter, parameter, ...); end; You get ORA-00904: \u0026ldquo;SQLTEST\u0026rdquo;.\u0026ldquo;TEST\u0026rdquo;: invalid identifier instead of a result. :-(\nUpdate: 20 January 2011: If you prefix the remote object name with the remote object owner, regardless of the fact that the database link is connecting to that schema anyway, then you can subsequently call the function, procedure or packaged code using the synonym. Result - thanks to Oracle Support.\n1 2 3 4 5 create or replace synonym X for owner.package_name@db_link_name; begin X.Procedure_name(parameter, parameter, ...); end; It \u0026ldquo;just\u0026rdquo; works. :-)\nCheers,\nNorm.\n","description":"","id":163,"section":"posts","tags":null,"title":"ExecutingPackaged Code Over a Database Link","uri":"http://localhost:1313/RantsAndRaves/posts/2011/01/executingpackaged-code-over-a-database-link/"},{"content":"The (Tory) government\u0026rsquo;s wonderful new system for the NHS starts tomorrow. GP\u0026rsquo;s will take over running things where the Primary Care Trusts used to and Primary Care Trusts are to vanish. If any hospital fails under the new system, there will be no tax-payer funded bale outs.\nStrange that, they baled out the banks and have been shafted in return as bankers returned to their old ways - the ways that all political parties said would never happen again - and stuck two fingers up at the tax-payer and the government. But there\u0026rsquo;s no such luxury for a hospital. Hmm.\nNice to know where their priorities lie then!\nAs I always say, voting should be mandatory and there must be an option on every ballot paper entitled \u0026ldquo;none of the above\u0026rdquo;. Mind you, I also have said for years don\u0026rsquo;t vote, the government will get in!\nCheers,\nNorm.\n","description":"","id":164,"section":"posts","tags":null,"title":"What Do We Need, Banks or Hospitals?","uri":"http://localhost:1313/RantsAndRaves/posts/2011/01/what-do-we-need-banks-or-hospitals/"},{"content":"Today, Alison and I took our lives in our hands and extracted the honey from 20 full frames. In all, we have now got a kitchen full of about 70 lbs of fresh garden honey courtesy of our hard working lady friends.Â The day started with the de-capping of the combs, four of, and then putting those into the centrifuge - a giant salad spinner of sorts. After 60 turns of the handle, we open up and turn the combs around and give the other side 60 turns as well. repeat for the other 16 frames.\nOf course, as soon as you get a good extraction, you have to empty the centrifuge through a double mesh filter and into a bucket (sterile of course). This filtering gets rid of various lumps of bees wax, assorted bee parts - legs, heads, the odd wing etc and leaves you with fresh, clean and very tasty honey ready for bottling.\nIt\u0026rsquo;s hard work, but not as hard as cleaning up afterwards and getting tiny lumps of sticky bees wax off the floor of the kitchen! The empty frames have all bee put back into a super and put on top of the two hives. While the centrifuge gets most of the honey out, it leaves everything sticky. The bees, on the other hand, will clean up everything and take it down into the brood box for winter stores. In return for this bounty, we get the combs back clean and dry and ready for storage.\nWe give the girls back the honey because it\u0026rsquo;s theirs after all, it would be cruel not to let them have some. We have kept back about 10 lbs of honey - in raw natural comb - for the girls to feed on over the preparation for winter and in early parts of the year when they need feeding ready to being. No sugar syrup this year for our girls!\nCheers.\n","description":"","id":165,"section":"posts","tags":null,"title":"Very Sticky Sunday!","uri":"http://localhost:1313/RantsAndRaves/posts/2010/10/very-sticky-sunday/"},{"content":"It seems that this new government is off it\u0026rsquo;s bloody head. A new law has been introduced that means, in essence, that if someone tells a joke in the office, and someone else is offended by it, they can sue, even (get this) if the offended person didn\u0026rsquo;t actually hear the joke in question!\nWTF is the government doing for crying out loud? Have they not learned from the mistakes of the last Tory government (aka selfish b\u0026rsquo;stards) - led by the \u0026ldquo;lovely\u0026rdquo; Maggie T. - who were the ones that introduced the American style \u0026ldquo;sue everyone, I\u0026rsquo;m not to blame\u0026rdquo; culture, that it just leads to complete nonsense? It appears not.\nI\u0026rsquo;m seriously offended by this new rule - who do I sue?\nCheers.\n","description":"","id":166,"section":"posts","tags":null,"title":"I'm Seriously Offended ...","uri":"http://localhost:1313/RantsAndRaves/posts/2010/10/im-seriously-offended/"},{"content":"Saturday 2nd October was a fairly nice day given the weather we\u0026rsquo;ve been having recently, so we managed - at long last - to get the final harvest of honey done. We managed to take another 9 full frames and a bucket full of raw natural comb also completely full and capped off. This was the space I inadvertently left when I removed the initial three frames. The girls went mad and filled the gaps.\nThe moral learned was never ever give them any free space because they will fill it up!\nAnyway, we have now got the supers off, the queen excluder off, the eke on, the apiguard (anti varroa) on and we are just about almost nearly ready for winter - just the mouse guards to go.\nWe intend to keep the honey in the bucket to use as winter feed rather than supplying them with syrup that they have to work on to convert into \u0026ldquo;honey\u0026rdquo;. I suspect giving them back their own honey is far batter for them in the long run.\nPeter\u0026rsquo;s extractor was delivered by Helen last night (Sunday) so next Sunday will hopefully be the day that the kitchen gets extremely sticky as we attempt to extract honey from all 19 frames. Watch this space for details.\nNext time, I need to make sure I get hold of a couple of Porter Bee Escapes to keep the girls from the capped off honey stores - it\u0026rsquo;s a lot easier to extract the filled frames when you don\u0026rsquo;t have to deal with a swarm of slightly irritated bees!\nCheers.\n","description":"","id":167,"section":"posts","tags":null,"title":"The Final Harvest Is In...","uri":"http://localhost:1313/RantsAndRaves/posts/2010/10/the-final-harvest-is-in/"},{"content":"The pope arrives in the UK today. Personally, I think he should be arrested. As the head of an organisation that is riven with child molesting staff he should be arrested. As the man in charge of covering up the atrocities (while a Cardinal) he should be arrested. As God\u0026rsquo;s disciple on Earth - he\u0026rsquo;s a fraud!\nWhy are the British Tax Payers, at a time when cut-backs are \u0026ldquo;required\u0026rdquo; paying for the head of theÂ Vatican \u0026ldquo;state\u0026rdquo; to pay a \u0026ldquo;state\u0026rdquo; visit to the UK, forced to pay huge sums, running into multi-millions of pounds to provide securityÂ for this man?\nIf he really is god\u0026rsquo;s disciple on Earth then god will prevent harm coming to him, surely? If someone does harm him, then surely it was god\u0026rsquo;s will, and we lowly Earthlings shouldn\u0026rsquo;t even begin to attempt to subvert god\u0026rsquo;s will? So, regardless, we don\u0026rsquo;t need and shouldn\u0026rsquo;t have to pay a single penny for securityÂ on this visit. Just a thought!\nAnyway, you can tell how much real faith all churches and mosques etc have by the fact that they all have lightning conductors on the highest points. Go figure. :-D\nAnd finally, as the pope is arriving in the UK via Scotland, here\u0026rsquo;s a question:\nWhy is it, in public toilets in Glasgow, that people write \u0026ldquo;Fuck the pope\u0026rdquo; on the walls? Because they can\u0026rsquo;t be bothered to write \u0026ldquo;Fuck the Moderator of the General Assembly of the Church of Scotland\u0026rdquo; (Billy Connolly)\nCheers.\n","description":"","id":168,"section":"posts","tags":null,"title":"The Pope Is Coming - Lock Up Your Kids!","uri":"http://localhost:1313/RantsAndRaves/posts/2010/09/the-pope-is-coming-lock-up-your-kids/"},{"content":"Oracle have made subtle changes to the way that they advise you to patch an installation from the release of 11.2.0.2. The full gory details can be found here (My Oracle Support link - support contract required) but in brief:\nYou are now advised to perform an out-of-line patch, however, you can do an in-line patch if you are short of space. However, this will delete the existing installation. Beware \u0026amp; take copious backups! The patch kit is a full installation - so you getÂ the latest patch kit and install it instead of gettingÂ the base release and then patching it. Grid components must be patched out-of-line. Cheers.\n","description":"","id":169,"section":"posts","tags":null,"title":"Oracle Change The Patching Process From 11.2.0.2","uri":"http://localhost:1313/RantsAndRaves/posts/2010/09/oracle-change-the-patching-process-from-11-2-0-2/"},{"content":"The girls were checked again today and all is well. Varroa counts is nil on both hives - that worries me a tad!Â But I did give them a good dusting of icing sugar last week - so maybe it works after all!Anyway, I managed to swap out 5 old brood frames from the new hive - the bees were not using those frames and on inspection, it\u0026rsquo;s easy to see why - old dirty wax, some dead brood in the combs so they have gone, destroyed. I thought about cleaning them up and re-waxing them, but you never know what has been in there.\nThese were frames from Peter\u0026rsquo;s hive out in the woods near Leeds and he did say that the swarm in there originally had died for some unknown reason. Before he got a chance to clean up, a new swarm moved in and we were the lucky recipients of said new swarm last year.\nThey are starting to store quite a lot of honey on the brood frames, but there are still larvae so the new queen is doing her stuff - I still have not found her yet, but when I do, she will have to be marked - something new for us to learn!\nThe original hive is also doing well. They are out and about quite a lot at the moment making the most of the end of season sunshine and, of course, the honey flow from Himalayan Balsam.\nAfter we took a super of frames off this hive last week, they started to fill in every available space with comb and honey, so I put a new super on today with 9 brand spanking new frames. I\u0026rsquo;m not expecting them to draw out and fill all nine, but at least they have somewhere to store all that lovely nectar!\nAll being well, we will be able to feed them honey, of their own making, to keep them happy over winter.\nTalking of winter, it doesn\u0026rsquo;t seem like it was this time last year when we were just getting ready for our first over wintering of the bees. Here we are again, getting ready to over winter two hives, one strong and the other, not as large, but doing quite well so far. Other than in the stores department!\nCheers.\n","description":"","id":170,"section":"posts","tags":null,"title":"Bee Latest.","uri":"http://localhost:1313/RantsAndRaves/posts/2010/09/bee-latest/"},{"content":"The girls have been busy and today, armed with instincts analogous to that of Defence Lawyers Perth, we combed the trees for honeycombs, and we also have harvested another 9 frames of fully capped honeycomb to add to the three we took a couple of weeks back. Unfortunately \u0026hellip; \u0026hellip; I neglected to replace the three full frames I took out last time with new ones. The picture below shows what the bees will do if left to their own devices and when they find a big space in the hive. The entire space has been filled with natural comb and filled and capped all in a couple of weeks.\nBusy or what? Anyway, here\u0026rsquo;s the picture.\nUnfortunately, as you can tell from the picture above, the natural comb has been broken when we split the super off the top (there\u0026rsquo;s another super underneath - that\u0026rsquo;s what you are looking at) so there\u0026rsquo;s a bit of loose honey that the girls will have to tidy away quickly now - in case of robbers, and bloody wasps, of which there are many this year.\nWe still have six frames, capped off, to harvest plus the mess above. I have a feeling I may be using the natural comb as feed for the winter.\nAt present, the girls are heavily into Himalayan Balsam which is good for them, but not so good for our native environment. This stuff is an alien and takes over quite rapidly when it arrives - but the bees love it and it flowers late into the season keeping them on good natural honey which, with a bit of luck, will be more than enough to keep them through winter without having to be fed on sugar syrup.\nWe may have to let them have their heads, as it were, and give them a few new super frames to fill up with winter stores. I\u0026rsquo;d much rather my bees fed on their natural food than on sugary substitutes that we supply. It makes sense to me.\nVarroa counts were up a little - we have about 16 on the tray today, so while not bad, it\u0026rsquo;s not good to have any, so the hive got a good dusting of icing sugar. With the honey still on the hive, we can\u0026rsquo;t use any of the chemical (thymol) based stuff, but once we get the last few frames off the hive, there will be some going in. We need to get the girls healthy in time for winter.\nThe other hive is still getting established. The girls in there have managed to use about 5 full frames to get the queen laying and they have pollen and honey - capped off now - aplenty. Not enough for the winter though, not by a long way. The remainder of the frames have not bee touched. They don\u0026rsquo;t seem to want to do much wax works inside the hive this lot.\nThey haven\u0026rsquo;t begun to create or lay, any drone comb either, but with it being September (tomorrow) I doubt we\u0026rsquo;ll see any of that from them now, it\u0026rsquo;s getting on for drone expulsion time, so any boys in the hive better enjoy life while they can, it\u0026rsquo;s soon to be all over!\nAs an aside to the main posting here, it\u0026rsquo;s a hard life being a boy bee - in case you didn\u0026rsquo;t know, drones are the boys and they exist for one thing and one thing only, sex! (How much does that resemble us Humans?) They sit around in the hive all day doing nothing, providing nothing and eating stores, waiting for a virgin queen.\nThey also hang about in gangs at a drone meeting place, where a virgin queen will come and mate with as many of the boys as she can. This is her only chance of mating so she takes full advantage!\nFor the boys this is it! Sex, then death, by having your bits ripped off! Still they seem to like it! If the boys manage to avoid being killed off in this way, come September, the girls will either evict them from the hive, or sting them to death. There\u0026rsquo;s no place for boy bees in the hive over winter.\nAt least the wax moth problem I mentioned last time has been sorted, no more signs of any, which is a good thing. And even better, the varroa count was one single solitary mite.\nThis new hive is going to need quite a bit of feeding before winter. And I still haven\u0026rsquo;t found our new queen yet! I know she\u0026rsquo;s in there because there are lots of lovely larvae in the brood frames, but she\u0026rsquo;s keeping her head down and hiding from us.\nPeter, if you are reading this, thanks! We couldn\u0026rsquo;t have managed without your help and advice.\nCheers,\nNorm.\n","description":"","id":171,"section":"posts","tags":null,"title":"It's Harvest Time!","uri":"http://localhost:1313/RantsAndRaves/posts/2010/08/its-harvest-time/"},{"content":"John Hallas has an interesting blog post here about how a system running flashback can become a system not running flashback - and without you knowing!\nEnjoy.\nCheers.\n","description":"","id":172,"section":"posts","tags":null,"title":"To Flashback, Or Not To Flashback. Are You Sure About That?","uri":"http://localhost:1313/RantsAndRaves/posts/2010/08/to-flashback-or-not-to-flashback-are-you-sure-about-that/"},{"content":"Greg Rahn wrote a pretty good article back in 2008 - it still applies today.\nCheers.\n","description":"","id":173,"section":"posts","tags":null,"title":"How To Gather Table Stats Effectively","uri":"http://localhost:1313/RantsAndRaves/posts/2010/08/how-to-gather-table-stats-effectively/"},{"content":"My wife recently changed from her old insurance company to the AA as they gave her a much better quote for herself and me as named driver. However, it soon turned out to be not quite as attractive as it first seemed. Shortly after sending in the forms and money Alison received a call from the AA saying that there was a problem with her application and could she call a certain number to discuss. She duly did so and was told that as I had a claim against me, that they were increasingÂ the premium by approximately Â£60 per year.\nThis was interesting as I don\u0026rsquo;t have any claims against me at all, however, they assured my wife that this claim against me took place in 2007. I was puzzled and decided to check.\nI spoke first with the AA who assured me that there was indeed a claim on the database and so Alison had no choice but to pay up the additional premium on her police (or drop me as named driver). I got as much information as I could from the telephone operator - who was very helpful just powerless to deviate from the rules she had to work under - and eventually asked for her manager. S/he wasn\u0026rsquo;t there but promised to call me back. So far,Â no call back!\nI then spoke to my own insurance company, Sainsburys who I highly recommend and they explained everything about this claim. I was informed that, on their database it is listed as \u0026ldquo;no fault, no claim\u0026rdquo; as it was simply noting that I had advised my insurance company of an accident in which I had been hit, while stationery, by another car. The other driver immediately admitted full liability and the whole matter was resolved with no claims being made against me and my own insurance company merely being informed of the facts.\nSo, a non-claim against me turned into a claim against me on the AA\u0026rsquo;s database and they flatly refuse to change it, or even speak to my own insurance company for clarification.\nNext time you think you need the AA, try here or here instead - the AA\u0026rsquo;s own claims are probably as dubious as the one they think has been logged against me.\nIt\u0026rsquo;s notÂ the first time that I\u0026rsquo;ve had trouble with this organisation, they gave me a load of grief years back when I was a motorcyclist and I promised myself never to use them. I was happy with the RAC for years - until Alison had some grief with them once and changed our \u0026ldquo;contract\u0026rdquo;.\nI stand by my previous decision never to become a customer of the AA. (Unless they sort their act out pretty sharpish and refund our additional expense.)\nWould I recommend anyone take out insurance with the AA? In a word, no. (Other opinions are available, but they are wrong! :-))\nHaving said all that, I do appreciate the fact that the AA are not actually an insurance company, just a broker (or are they?) for a real insurance company located somewhere in Gibraltar and that the AA probably have to follow the rules laid down by said off-shore insurance company but does anyone actually think about these rules? I mean, how on Earth can a \u0026ldquo;no claim\u0026rdquo; be a \u0026ldquo;claim against\u0026rdquo;? Sigh.\nCheers.\n","description":"","id":174,"section":"posts","tags":null,"title":"The AA Insurance Company \"Scam\".","uri":"http://localhost:1313/RantsAndRaves/posts/2010/08/the-aa-insurance-company-scam/"},{"content":"Been a while since I wrote, so here\u0026rsquo;s a brief update on what\u0026rsquo;s happened since last time. The artificial swarm was carried out and we watched hopefully for a hatching new queen. We didn\u0026rsquo;t see her, but we have evidence that she is indeed there and laying. The hive is filling with bees and we have larvae in the cells. The vast majority are worker cells and there are very few drone cells - so it\u0026rsquo;s looking good for a new laying queen as opposed to a laying worker.\nWe were in Sardinia for two weeks and on our return, it was impossible to separate the top (two) supers from the main hive. The girls had glued everything together. It didn\u0026rsquo;t help that I\u0026rsquo;d lost the hive tool somewhere and it simply seems to have vanished from the planet. I suspect it was beamed up at some point between the hive and the garage!\nAnyway, a new hive tool was purchased, and I went in to the hive last night when the weather was quite mild. The new tool was an immense help in separating the two supers and there is currently 16 frames of honey. Three were fully capped off, so I\u0026rsquo;ve removed those for later. I\u0026rsquo;ll be putting another three empty frames in to keep the girls busy.\nBetween the supers, they had built comb from the top of the lower super to the bottom of the frames in the upper one, that\u0026rsquo;s what made it difficult to split the two. After last night when I scraped off what was bridging the two supers, we have a bowl of wax + honey to enjoy. A bit more work needs doing on this matter, but the girls were getting grumpy and I\u0026rsquo;d refilled the smoker three times by then, so I decided to let them have some peace and quiet.\nThe second hive had a plague of wax moth caterpillar, well two of the b*ggers, so I got rid of those. One brood frame was pretty much trashed but it was an old one I fancied replacing as the girls weren\u0026rsquo;t using it yet - so it\u0026rsquo;s now gone. I ditched the wax as well it was looking pretty black. (It was one of the original frames we inherited when we got our first swarm from Peter.)\nWhen I checked for varroa, that nasty little mite, the counts were still low. About 4 weeks ago (I know, far too long!) I had 16 on the second hive, but after a fairly liberal sprinkling of icing sugar, we are down to about 3 mites over the 4 week period. The main hive had 1 mite over the same period. I\u0026rsquo;ve also done a drone cull in the second hive (where the frame came from the main hive - don\u0026rsquo;t get confused like me!) and there were a few mites present in the drone comb, but nothing spectacular. Yet!\nI\u0026rsquo;ll be keeping on top of the mites on a weekly basis now that the holidays are over.\nCheers.\n","description":"","id":175,"section":"posts","tags":null,"title":"The First Honey Crop","uri":"http://localhost:1313/RantsAndRaves/posts/2010/08/the-first-honey-crop/"},{"content":"It was been checking time again yesterday and of the three queen cups I found last week, one has a grub in it. Looks like we need to consider doing an artificial swarm for real this time.\nThe first super we put on is now in the process of being capped off. The honey is almost ready. I\u0026rsquo;m leaving it for the girls to completely cap off and then, Alison gets to do what she\u0026rsquo;s been looking forward to since we started on the Bee Keeping course, extracting her own honey! I foresee a very sticky kitchen any time soon.\nThe second super is starting to be filled as well. They have drawn out 7 of the 8 frames and have simply left the last one untouched. Not bad for a week though.\nQueenie is still around, I saw her yesterday. Wandering about and laying eggs as usual.\n","description":"","id":176,"section":"posts","tags":null,"title":"We Have A New Queen Coming!","uri":"http://localhost:1313/RantsAndRaves/posts/2010/05/we-have-a-new-queen-coming/"},{"content":"On Saturday I went to check on the girls again. In the seven days since my last check, they\u0026rsquo;ve been busy. How busy? Well, last week I gave them a pair of brand new super frames to fill up the super. They had 6 already (or was it 7? I must go and count next time I see them!) and they had just started to draw out the comb. Well, 7 days later, we not only have a complete set of super frames, fully drawn out, but the whole lot is just about full (on both sides!) with brand new honey. Once they get it evaporated down to the correct water content, they will cap it off.\nI suspected something was up when I lifted off the super to do my weekly checks, I could hardly lift it! Anyway, I\u0026rsquo;ve kept them busy by giving them a new super on top of the existing one, with a compete set of empty frames. Wonder what I\u0026rsquo;ll find next weekend?\nDownstairs in the brood box, Queenie has been busy and there are \u0026ldquo;millions\u0026rdquo; of bees milling about. Plenty of stores - honey and pollen - stashed away in the brood frames. The drone comb that we cut out last week (to try and prevent varroa infestation) has been completely rebuilt with new, fresh beeswax. No eggs yet.\nWe also have three potential queen cups. No eggs (not that I can see them with my eyesight - glasses on or not!) or larva yet, but I\u0026rsquo;ll be keeping an eye out, just in case.\nAnd the varroa count. Well, after 7 days, I counted what was on the varroa tray under the hive - a bit of chalk-brood, and one, single, solitary, all by itself, mite. One!\nThis is good news, but worrying at the same time. I can\u0026rsquo;t believe that our hive doesn\u0026rsquo;t seem to be a hot bed of varroa like almost all the others. There were some in the drone comb we removed last week, but not many. I haven\u0026rsquo;t see any of the bees with mites, but then again, it\u0026rsquo;s difficult to tell with so many of them knocking about.\nHow many times have I been stung when I dismantle their house? So far this year, none! They are very docile at the moment, I\u0026rsquo;d personally be going off my head if someone kept breaking into my house and moving the furniture all the time!\nCheers.\n","description":"","id":177,"section":"posts","tags":null,"title":"Just How Busy Can a Bee Get?","uri":"http://localhost:1313/RantsAndRaves/posts/2010/05/just-how-busy-can-a-bee-get/"},{"content":"Ted Hooper has died at the age of 91. He was most famous for the Bee Keeping book Guide To Bees And Honey.\nWikipedia link to Ted:\nWikipedia\n","description":"","id":178,"section":"posts","tags":null,"title":"Ted Hooper - RIP.","uri":"http://localhost:1313/RantsAndRaves/posts/2010/05/ted-hooper-rip/"},{"content":"On Saturday, Peter came over because we (Alison and I) thought we had noticed a few queen cells in the hive. Drastic action could have been required if this was the case with an artificial swarm being uppermost on the list. As it turned out, we were wrong and what we had seen was a number of drone cells, not queen cells. Ah well, at least we know now!\nWhile inside the hive, we found the queen - and she was marked, with a red dot. This makes her either 2003 (highly unlikely) or 2008 (most likely) which does imply that our swarm of bees used to belong to someone else!\nAnyway, a full investigation showed lots of new brood, plenty drones and workers (phew!) and while we were inspecting, the queen laid an egg right in front of us.\nThey have also drawn out about 6 super frames from bare foundation since the start of the year. There is even honey in some of the super combs already.\nThey are still surprisingly docile, we basically destroyed their house, and they were not bothered at all.\nWe now have to check them every 7 days to make sure that they don\u0026rsquo;t start getting ideas about leaving home. Mind you, we have a new hive sitting right next door to the existing one, so there\u0026rsquo;s always a chance that they will move in there.\nCheers.\n","description":"","id":179,"section":"posts","tags":null,"title":"We Have A Queen - And She's Red!","uri":"http://localhost:1313/RantsAndRaves/posts/2010/05/we-have-a-queen-and-shes-red/"},{"content":"Oh joy! Politics. How lovely. Ok, here\u0026rsquo;s what I think of the election:\nThey don\u0026rsquo;t give a shit about keeping promises. Regardless of what your local MP (or wannabe MP) tells you he will do for you, once (if) s/he gets elected, s/he will have to follow the party whip - so why do they bother telling us lies all the time. They still have an expenses system that gives them the right to excess. If I took a job in London, for example, I would not be getting second homes allowances, nor any of the rest of the stuff that they provide themselves with. It\u0026rsquo;s a con. They know what the job entails and where they have to work from, so deal with it. if you don\u0026rsquo;t like working to the same expenses rules as everyone else in the country, don\u0026rsquo;t apply for a job as an MP. Simple. Voting should be mandatory in the UK. However, see next bullet point. Every voting paper should have one additional \u0026lsquo;candidate\u0026rsquo; at the bottom - None Of The Above - that would give the politicians a better idea of exactly what we think of them. (None Of The Above courtesy of Brewster\u0026rsquo;s Millions an old film with Richard Prior.) And finally - for now - remember this if nothing else. You can always tell when a politician is lying, because his lips move! Update 28/02/2023: Not much has changed since I ranted about this!\nCheers,\nNorm.\n","description":"","id":180,"section":"posts","tags":null,"title":"The Election Is Coming!","uri":"http://localhost:1313/RantsAndRaves/posts/2010/04/the-election-is-coming/"},{"content":"On Saturday the 10th of April, at 20:30 our journey home from Sharm El Sheikh began on a Monarch Airlines Airbus A300. We were sitting in row 15, seats A and B - if it makes a difference. 15A being the window seat and 15B being the middle of three. For someone over about 5'6\u0026quot; legroom is a problem and as Alison is 5'8\u0026quot; and I\u0026rsquo;m 6\u0026rsquo; we both were in trouble before we left! I had to request that the people in front of me didn\u0026rsquo;t attempt to put their seats back as they would crush my knees if they did - thankfully, they obliged even though they had to have their 2 year old on their knees. Not an ideal way to start a long (approx 6 hours) flight back to the UK.\nThe window seat has a big metal bracket screwed to the floor restricting the available space for feet under the chair in front. The aisle seat has a big \u0026ldquo;box\u0026rdquo; of some sort that also performs the same task. Only the middle seat - where Alison was - has any decent amount of space for your feet - although the middle seat is always seemingly narrower than the others.\nSo, an auspicious start to the nightmare ahead - no leg room and nowhere to put your feet either.\nDid I mention also that the flight was short staffed as two Cabin Crew had failed to arrive for work? Well, all compliments to the girls who had arrived - they did a great job. Credit where it is due.\nOn the ground, the interior of the plane was far too hot. We were told that after take off the air conditioning would come on and reduce the temperature. Take off couldn\u0026rsquo;t come too soon!\nAnyway, we soon set off, on time (ok, 5 minutes late!) and immediately we hit a problem. The temperature inside the cabin soared into the heights. People all around us were fanning themselves with the in-flight magazine, sick bags or anything else they could find. The captain was asked to turn it down. It made no difference.\nDid I mention that we had been given details in the safety briefing on where to find the exercises we really should do to keep ourselves from the scourge of Deep Vein Thrombosis (DVT)? Well, reading about the exercises was about all we could manage - there\u0026rsquo;s no room for your feet under the chair when sitting still so you have absolutely no chance of \u0026ldquo;extending your leg and waggling your foot around and around\u0026rdquo; - once sat down in a Monarch Plane, you simply cannot move.\nOnce safely at height, the cabin crew sold us drinks. We again asked to have the heating turned off and they asked the Captain to do so. He did. It made no difference. Maybe third time lucky?\nIf I remember correctly, two tiny bottles of Brandy, two tiny tins of Ginger, and two tiny bottles of white wine (unchilled for crying out loud!) and a tiny box of Pringles cost us about Â£17.00. Ouch! Still anything to take the mind away from the uncomfortable situation we find ourselves in has to be worth that, doesn\u0026rsquo;t it?\nShortly afterwards, and it\u0026rsquo;s still far too hot, it\u0026rsquo;s dinner time. Sausage, mash and two vegetables. Sounds good doesn\u0026rsquo;t it? Unfortunately, airline food all smells the same - it must be the altitude and reduced cabin pressure - and it smells (to me) like vomit. I cannot eat airline food. Dinner for me was a brandy \u0026amp; ginger and a bottle of wine. I tried to chill it in a cup with two ice cubes in it - I failed. The heat in the cabin rendered the ice cubes into water in about 5 minutes. Sigh.\nWe ask the crew to turn the heating down, or perchance, off completely. They promise to do so. It doesn\u0026rsquo;t work. Aaargh!\nThe entertainment is a rerun of some old crappy TV programs, which if memory serves me, started with something dire, was followed by a program about the Beatles, that then Progressed into a Tribute To Heath Leger followed by some other ancient drivel from the TV and got worse after that. It\u0026rsquo;s still too bloody hot!\nIt makes no difference anyway, the headphones supplied (excuse me, sold to us for Â£2.50 each) don\u0026rsquo;t work. Well, they do work, but the holes in the seat to plug them in are so close to your thighs that you can either have the headphones on or you can sit in the seat - but not both!\nI suspect that after a few \u0026ldquo;passenger plus headphones plugged in\u0026rdquo; interactions, the sockets are trashed. No sound. It was the same on the way out as well. Thankfully, the TVs are small and so far away from where I\u0026rsquo;m sitting that watching is almost impossible anyway.\nLater that same day \u0026hellip;. it\u0026rsquo;s almost midnight. Guess what, it\u0026rsquo;s cooled down a lot now. The Captain finally managed to turn the heating off/down and we are almost comfortable - temperature wise I mean. The cabin comfort levels are non-existent I\u0026rsquo;m afraid. Everyone (it seems) is stiff and unable to move. I didn\u0026rsquo;t remember anyone putting their seats back to try and get a bit more comfortable - maybe the sound of cracking knee caps from behind put them off?\nThe cabin is full of people wanting desperately to go to sleep. The staff will sell you \u0026ldquo;comfort kits\u0026rdquo; consisting of a blanket, a blow up neck pillow and a mask. Hmm.\nYou will need the mask because even though it\u0026rsquo;s midnight (Egyptian Time) the Cabin staff are now selling Duty Free crap goods from the trolley at prices that appear way too high to be anything but Duty Free. (Other opinions are available!) The Cabin lights are turned up to full and other than take-off, they haven\u0026rsquo;t been anything but.\nNow, the blow up pillow are a great idea. In principle that is. The idea is that you blow them up and place around your neck so that you can lean back in your chair (yeah right!) and have a nice comfortable place to rest your weary head as you drift off into a deep and comfortable sleep.\nReality has a nasty habit of changing things! The seats that we are supplied with are way too low in the head rest department, and any attempt to sleep with your head back results in your head falling over the back of the seat and irritating the people behind you! There is nothing at all to rest your head on! The seats in my car have more height to them than the ones in this plane! Monarch are the only airline I know that have such low seats.\nTrying to rest your nice comfy pillow on the seat simply won\u0026rsquo;t work - you\u0026rsquo;ve been sold a pup - as the people in front of me discovered when they tried! At least the mask comes in handy as the Duty Free trolley is still slowly doing the rounds and the cabin is lit up at full brightness.\nTrying to get to sleep I rest my head on the back of the chair in front of me. It doesn\u0026rsquo;t work. Any time anyone in any of the three seats in front of me moves - or breathes heavily - the vibrations transfer straight into my head.\nI\u0026rsquo;m in the window seat (15A) and I think I could be able to lean my head against the window or the wall. How sadly mistaken was I? I start to nod off and my head falls backwards into the window recess, twisting my neck at an unnatural angle and wakes me up. I try again. Same result. I swear! Twice.\nI suppose I forgot to mention the fact that the seats and the windows don\u0026rsquo;t line up? Well, they don\u0026rsquo;t. Some people can see out of two windows (ok, one and a bit) while some people can\u0026rsquo;t see out at all - even though they have a \u0026ldquo;window\u0026rdquo; seat. Hmm.\nOk, sleep is going to be difficult I can tell. But I try again, this time I must have drifted off for a while as I wake up to find the lights in the cabin have been dimmed. I look at my watch - why am I awake? Oh yes, I fell into the window again - we still have hours and hours to go yet.\nI give up on sleep. I look out the window - there\u0026rsquo;s nothing on TV and anyway, my earphones don\u0026rsquo;t bloody work! (See above!) The view down to ground is quite pleasant - it\u0026rsquo;s a clear night and you can see for miles. Looking up, I can see even further as I can see stars! Oh well, that\u0026rsquo;s the good bits done then, back to the flight!\nI attempt another attack on my complete exhaustion by leaning my head on the seat in front again. Alison - herself knackered - tries to sleep by leaning on my back. I think I drop off but the liquid lunch seems to be causing weird dreams (or it could just be the surreal reality of this flight!) and I\u0026rsquo;m not sure if I\u0026rsquo;m awake or asleep or what. Anyway, it doesn\u0026rsquo;t work and Alison gets off of me pretty soon. I\u0026rsquo;m awake again.\nAlison has given up on sleeping now and has been trying to do Sudoku on my Nintendo DS but she soon gives up on that and starts chatting to the man in 15C. He turns out to be on permanent holiday since being laid off from his job as a civil engineering something or other! He was responsible for there Leeds City Tram project that the government canned. They chat for ages while I doze - my head still jammed onto the seat in front. I\u0026rsquo;ll have a textured forehead when we eventually get back to Manchester!\nThere\u0026rsquo;s a famous person on this flight as well. David someone or other. He plays the caretaker in the Harry Potter movies. I didn\u0026rsquo;t recognise him when he was chatting to us on our bus on the way out - or on the way back. Nice man. He\u0026rsquo;s knackered as well and seems to have given up on sleep and sitting down. He stood up for the best part of the flight.\nMany people, when I look around, are doing the same. Standing. It\u0026rsquo;s the only thing to do. I suspect the number of numb bums on this flight is approaching 100% - mine certainly is.\nI try to sleep. I doze mostly and I seem to remember when I could actually feel my feet. Thanks to that metal bracket (and the sloping curved sides of the plane) I\u0026rsquo;m sitting with my legs and feet at what can only be called \u0026ldquo;an unnatural angle\u0026rdquo; and I\u0026rsquo;m really uncomfortable. The man in front is in a similar situation and is complaining about his back giving him grief. I suspect others have a similar mindset.\nSuddenly I awake from my doze. The plane has throttled back and dumped it\u0026rsquo;s nose into a downward direction. We must be descending into Manchester. I look at my watch and I can\u0026rsquo;t figure it out, we are early!\nHalf an hour later we are down and off the plan. What a relief. That flight was a nightmare and I\u0026rsquo;m surprised that Monarch are allowed to put passengers in a situation where they have no leg room and no foot room while sitting on a seat with no ability to lean back and rest their head. I\u0026rsquo;m not surprised people get DVT when flying.\nSometimes the only thing that spoils the joy of flight is being on a plane.\nUPDATE: Monarch are no more, they went bust. I wonder why!\n","description":"","id":181,"section":"posts","tags":null,"title":"Fly Monarch? I Don't Think So!","uri":"http://localhost:1313/RantsAndRaves/posts/2010/04/fly-monarch-i-dont-think-so/"},{"content":"There\u0026rsquo;s a mass orgy taking place in our pond this weekend. The frogs are hard at it again - it must be springtime!\nThe first spawn I found was actually in the filtration housing, one batch of spawn plus 38 frogs - 36 coupling and two single males looking for action.\nI hefted all the above into the main pond itself, and left them to get on with it. Yesterday afternoon, the spawning had begun in earnest. This morning they are still laying.\nIt\u0026rsquo;s a shame that we have fewer frogs in the pond than we had on the last two years, plus those damned Koi haven\u0026rsquo;t died yet so any tadpoles that do hatch, will eventually be eaten. I saved some last year by a transfer to the small pond in the back garden - but it\u0026rsquo;s far too small to cope with the expected numbers of tadpoles. Sigh!\nCheers.\n","description":"","id":182,"section":"posts","tags":null,"title":"Shag Fest!","uri":"http://localhost:1313/RantsAndRaves/posts/2010/03/shag-fest/"},{"content":"What more can I say?\nI was leaving home, for work, this morning at 06:55 and when I opened the garage door (from the inside) a frog was sittingÂ there, right slap bang in the middle of the doorway. She, for it was a girl frog, just sat looking at me.\nObviously, if I was to drive over her, she wouldn\u0026rsquo;t be too happy (neither would I, I like frogs) so I picked her up to take her to the pond.\nShe wasn\u0026rsquo;t happy about that at all. She peed all over my hands. Thanks girlie! Frog pee is cold by the way, in case you are interested!\nAnyway, I took her up to the pond where rather a lot of other frogs are waiting for the right moment for a frog orgy and popped her in.\nI didn\u0026rsquo;t kiss her though! :D\nCheers.\n","description":"","id":183,"section":"posts","tags":null,"title":"I Was Peed On By A Frog This Morning!","uri":"http://localhost:1313/RantsAndRaves/posts/2010/03/i-was-peed-on-by-a-frog-this-morning/"},{"content":"On Sunday (14th March) I went to check on the bees. Hefting the hive was a tad tricky as they were all out and about flying around and generally being bees. The weather had taken a bit of a warm turn (relative to recent months that is) and they took advantage.\nA closer inspection showed a good few had been \u0026ldquo;hopping\u0026rdquo; over the hedge to a crocus \u0026lsquo;plantation\u0026rsquo; not 10 feet from the hive and were covered in shocking yellow pollen. This is a good sign.\nI decided to take the mouse guard off and let them clear out the bodies of those bees that didn\u0026rsquo;t make it through the winter. They do it so much better than me after all!\nIt looks like we may have managed to keep our bees alive through their first winter. Thankfully!\nCheers.\n","description":"","id":184,"section":"posts","tags":null,"title":"The Bees are Flying Again!","uri":"http://localhost:1313/RantsAndRaves/posts/2010/03/the-bees-are-flying-again/"},{"content":"Â£6.5 million per year it would appear.\nA new Parliamentary Standards Authority has been set up to monitor our beloved politicians and to keep them honest. According to Radio 4 today, this has an annual budget of Â£6.5 million and will employ 80 people.\nIs it just me, or is it a tad strange that the people we elect to run (ruin?) the country should need so much watching to keep them honest?\nSomeone mentioned on the news last week, when discussing Grodon Brown\u0026rsquo;s proposal for the new voting system, that we should have an option that says \u0026ldquo;none of the above\u0026rdquo; - well, I\u0026rsquo;ve been saying that ever since I first watched Brewster\u0026rsquo;s Millions many years ago.\nIn my opinion, voting should be mandatory - like Australia - but every ballot paper should have the final option of \u0026ldquo;none of the above\u0026rdquo; and that way we can show politicians exactly what we think. Gordon\u0026rsquo;s new system doesn\u0026rsquo;t let us show our disapproval - funny that eh?\nI suppose, looking on the bright side, 80 people will have a new job.\nCheers.\n","description":"","id":185,"section":"posts","tags":null,"title":"How Much Does it Cost to Keep Politicians Honest?","uri":"http://localhost:1313/RantsAndRaves/posts/2010/02/how-much-does-it-cost-to-keep-politicians-honest/"},{"content":"Subject to unannounced changes of course, but \u0026hellip;\nAll supplied scripts will be QA\u0026rsquo;d to ensure adherence to the following standards. Any script that fails on one or more of the following rules will not be applied to any system. Instead it will be returned to the vendor for correction.\nAll vendor or internally supplied scripts must be able to be run - without error - in SQL*Plus. We need this as a standard because of all the wonderful GUI tools out there in vendor land, Toad etc are fine - if you can guarantee that the person running the script has the same tool.Â SQL*Plus should be available in all Oracle installations. I much prefer Toad myself, but this is how it is.\nI have seen (and run!) scripts in the past where semi-colons were missing off the end of statements. These had been tested and proved to work under some GUI or other that allowed this sort of thing - but come the day of reckoning, they failed in SQL*Plus.\nAll scripts must spool to .log. This is so that we can see what errors occurred rather than hoping to spot them all as they zoom off the top of the screen at high speed!\nThe use of WHENEVER is not permitted. It\u0026rsquo;s a pain when a script blows up at the first hurdle and then carries on, however, scripts which blow up and immediately vanish are even worse.\nAll scripts will explicitly set linesize, pagesize, trimspool etc. No assumptions about how the client is set up shall be made. The vendor must explicitly specify all required options.\nCommit and/or rollback is not permitted in any script. Obvious really. If the vendor tests on a 6 row table and it all works, that\u0026rsquo;s fine. What if I have a 3.6 billion row table (and I do!) that, if updated, could blow away my UNDO tablespace? I don\u0026rsquo;t want some developer telling me it\u0026rsquo;s ok to commit in such an event, I\u0026rsquo;m the DBA so I decide.\nScripts must instead prompt the DBA to commit or rollback as appropriate after checking the log file (see above) as required.\nMixing DDL and DML is forbidden. It is surprising how many vendors don\u0026rsquo;t know how Oracle works. The simply do not realise that DDL implicitly commits any outstanding transactions before it starts to CREATE or ALTER or DROP or whatever. It then commits again when done (if successful).\nIf you do see scripts from vendors that mix and match DML and DDL (and invariable compound the error by having a commit at the end!) then you should be on your guard for other problems - they don\u0026rsquo;t know what they are doing!\nIf it is impossible (!) to separate DML and DDL then it is imperative that all the DDL is executed first and all the DML executed last. This prevents the implicit commits from affecting the DML statements. However, it is advised that separate scripts are supplied for the DDL and DML parts.\nScripts that create stored procedure code must SHOW ERRORS. It\u0026rsquo;s always nice when that well tested piece of code that creates a new package or whatever, fails to compile on my databases, but it doesn\u0026rsquo;t tell me why. Slipping a SHOW ERRORS in at the end of the CREATE OR REPLACE \u0026hellip; is a nice little touch and keeps the DBA happy. The DBA is especially happy as the errors are logged in the spool file (see above) and can be sent straight back to the vendor for correction!\nDo not drop temporary tables in the same script that created/used them. This shouldn\u0026rsquo;t ever occur based on not mixing DDL and DML, but I have seen scripts which:\nCreate temp_table as select * from live_table; DML the temp table to massage the data. Drop live_table; Create new live_table as select * from temp_table; Drop temp_table; Now, the obvious problem is this, it always fails at step 1. Steps 2 through 4 obviously also fail resulting in the loss of the live table and all it\u0026rsquo;s data. (10g recycle bin not withstanding!) and finally, we drop the temp table at step 5 losing all the data completely.\nAt this point, it\u0026rsquo;s back to the backups - you did take a backup didn\u0026rsquo;t you?\n","description":"","id":186,"section":"posts","tags":null,"title":"My Oracle Scripting Standards","uri":"http://localhost:1313/RantsAndRaves/posts/2010/02/my-oracle-scripting-standards/"},{"content":"I hefted the hive the other evening - checking for the remaining supplies - it\u0026rsquo;s still heavy, so the girls should be fine for a while yet.\nUnfortunately, when I peeked in through the mouse guard, there was a pile of dead bees. Not good. There are still lots of live ones, clustering on the combs to stay warm (and alive!) so that\u0026rsquo;s good. It\u0026rsquo;s just a shame that winter kills off so many of them.\nThe other thing is, being on our first season of actual bee keeping, we don\u0026rsquo;t know what to expect in winter. The last couple have been wet and miserable and that\u0026rsquo;s what kills bees, damp, not cold.\nWe shall see what happens soon. The queen should be up and laying shortly, then the stores will start going down and we\u0026rsquo;ll need to keep a close eye on them then. We don\u0026rsquo;t want them to starve after surviving all winter.\nCheers.\n","description":"","id":187,"section":"posts","tags":null,"title":"Dead Bees :-(","uri":"http://localhost:1313/RantsAndRaves/posts/2010/01/dead-bees/"},{"content":"Well, the snows are here and the temperatures have dropped dramatically. The bees are settled in for winter. Hopefully they will make it through to next year (only eight days away!) and be ready to start hunting and gathering pollen, nectar and allÂ the other stuff that they seem to like so much.\nWe only had them for a brief period at the end of 2009, we got them in late August/Early September but so far they seem to have been fine. Not too much varroa even with ApiGuard in the hive and I\u0026rsquo;ve just bought them a Christmas present of some Oxalic acid. When I get a decent enough day, I shall apply it and that should hopefully reduce any chance of getting a load of nasty mites in the new year when the brood comes on.\nThe mouse guard is on and secure, the bees can be seen in the brood box, clustered together for heat and warmth. With the temperatures below zero at the moment, they need all the heat that they can get.\nI have 12Kg of baker\u0026rsquo;s fondant in the garage all ready in case they haven\u0026rsquo;t got enough stores to make it through. Regular hefting of the hive seems ok at present, but January is when queenie should start laying again, and the stores will take a severe hit when she does.\nHere\u0026rsquo;s to next year.\nCheers.\n","description":"","id":188,"section":"posts","tags":null,"title":"The bees are clustering for winter","uri":"http://localhost:1313/RantsAndRaves/posts/2009/12/the-bees-are-clustering-for-winter/"},{"content":"Check out this link first.\nSpeaking as a ginger myself, I have no idea what that woman is complaining about. So what if a card says something funny about gingers (ok, funny is relative, some things I find funny others don\u0026rsquo;t and vice versa) - it\u0026rsquo;s called humour and it doesn\u0026rsquo;t hurt!\nI wonder if this woman was simply \u0026lsquo;offended\u0026rsquo; because she was after a payout form Tesco for \u0026ldquo;mental distress\u0026rdquo; or some other such nonsense. I find that being offended is usually an indicator that you have nothing to say. It\u0026rsquo;s what the religious fundies are all using as an excuse these days to complain about people saying or doing something that their particular imaginary friend would disapprove of.\nThe fact that this woman got TV News time and an interview on North Tonight on the BBC is laughable.\nCheers.\n","description":"","id":189,"section":"posts","tags":null,"title":"Get a bloody life!","uri":"http://localhost:1313/RantsAndRaves/posts/2009/12/get-a-bloody-life/"},{"content":"Well, it has happened - granted, a few weeks ago - but we have bees. Peter, our mentor was as good as his word and he supplied us with a hive of bees. They were moved by us from a location outside Leeds, West Yorkshire, to our own location. I got stung twice - once on the arm and once on the end of my nose! That certainly makes your eyes water, I can tell you!\nSince then, the bees have been transferred from Peter\u0026rsquo;s hive into our own and they have settled in nicely. After transferring them, Peter, Alison and I spent a happy evening extracting honey from them. Considering the bees had only found Peter\u0026rsquo;s hive about 4 weeks before we extracted, the fact that we got about 20 pounds (UK) of honey from them was a good haul indeed.\nSince then, they have been feeding almost exclusively - or as exclusively as we can tell - on Himalayan Balsam which in the UK is a weed and should be eradicated. This, of course, leaves DEFRA and the Environment Agency with a huge problem. Bees are suffering tremendously at the moment with all sorts of problems and Himalayan Balsam keeps flowering late into the season - it\u0026rsquo;s still flowering now at the beginning of October - and keeps the bees alive. The other nice thing about HB is that the honey it produces (by way of the bees of course!) is the finest you can taste. What a problem.\nAs bee keepers (and we have certificates to prove it) Alison and I prefer the HB to stay around for a bit. The bees love it, the flowers are extremely nice - they look like orchids to us - but it\u0026rsquo;s an invasive weed. Who said life was easy.\nAnyway, we are feeding the bees on a sugar syrup at the moment, but not for much longer, as we have robbed them of a lot of honey. The recipe is simple, 2 KG of sugar to one litre of water, heat slowly stirring all the time until dissolved. Allow to cool. If it starts to crystalise add some more hot water and stir again. Easy!\nLets hope we can get these bees through the winter and into next year where they will be able to make as much honey as possible and we will leave them with plenty for themselves. I think it\u0026rsquo;s better for the bees to feed on their own food rather than the sugar syrup we give them - and every little helps. Bees, at the moment, need lots of help.\n","description":"","id":190,"section":"posts","tags":null,"title":"We Have Bees!","uri":"http://localhost:1313/RantsAndRaves/posts/2009/10/we-have-bees/"},{"content":"Tomorrow night (Tuesday 18th August 2009), Alison and I are off out to a secret location near where we live to colect our very first honey bee swarm. Our Mentor, Peter, has been looking after then for us while we were on holiday, and tomorrow we get to take them home.\nWe\u0026rsquo;ve been on the Harrogate \u0026amp; Ripon Beekeepers Association course and are, unfortunately, the last people to get some bees. Some people have simply placed a hive outside and had bees come and live there by themselves, others have purchased bees for themselves (I\u0026rsquo;m Scottish and Alison is Yorkshire - so that was never going to be an option was it? ;-) ) while others have managed to obtain a swarm from their mentors - we just happened to be last!\nWe are very excited - as you can imagine - and looking forward to the imminent arrival. Hopefully, we can remember all that we were taught by HRBKA and we will make good \u0026lsquo;parents\u0026rsquo; to our swarm.\nWith winter coming soon (well, it is August you know!) we will most likely need to keep them fed throughout the remainder of this year - it is quite late in the season for a swarm. Hopefully this winter will not be as miserable as the last two which caused havoc with the bee populations all over the country.\nWish us luck!\nCheers.\n","description":"","id":191,"section":"posts","tags":null,"title":"The Bees Are Coming!","uri":"http://localhost:1313/RantsAndRaves/posts/2009/08/the-bees-are-coming/"},{"content":"From time to time an Oracle based application will encounter a deadlock. This happens when two (or more) sessions are holding onto a resource and waiting for another one before it can relinquish the one(s) it holds.\nThere is quite a lot of misunderstanding about deadlocks and what happens within Oracle to relieve the situation. Hopefully this blog entry will help to sort it all out.\nFirst of all, a brief demonstration.\nBackground.\n1 2 3 4 CREATE TABLE DEADLOCK (A NUMBER); INSERT INTO DEADLOCK VALUES (1); INSERT INTO DEADLOCK VALUES (2); COMMIT; Session One.\n1 DELETE FROM DEADLOCK WHERE A=1; Nothing wrong so far, the row will be deleted, but not (yet) COMMITed.\nSession Two.\n1 DELETE FROM DEADLOCK WHERE A=2; Again, nothing wrong, the row will be deleted, but not (yet) COMMITed.\nSession One.\n1 DELETE FROM DEADLOCK WHERE A=2; This session will hang waiting for Session Two to commit (in which case the delete will fail) or rollback.\nSession Two.\n1 DELETE FROM DEADLOCK WHERE A=1; This session will hang waiting for Session One to commit (in which case the delete will fail) or rollback. In addition, we now have a deadlock condition.\nAfter a few seconds, Oracle will detect the deadlock and pick one of the sessions and \u0026lsquo;rollback\u0026rsquo; the statement. This is where we see our first misunderstanding about deadlocks.\nOracle does not kill the session. Oracle does not kill the transaction. Oracle only kills the statement. Oracle does rollback the failing statement, but Oracle does not rollback the entire transaction that the failing statement is part of. (Correction by Mark Bobak.) PMON (Process Monitor) does not clear out the locks. It is the responsibility of the session that detects the \u0026ldquo;ORA-00060 deadlock detected while waiting for resource\u0026rdquo; error to trap and handle the error by issuing a rollback (or a commit) command. Only once this has been done will the other session be able to continue.\nIf, like me, you run a test of the above using two separate SQL*Plus sessions (or TOAD sessions, whatever you like) you will find that the session that detects the deadlock will return to a prompt and allow you to enter new commands. The other session remains hung until such time as the other session releases the locks it took out on the rows it had deleted.\nThis demonstration uses DELETEs but UPDATE will show similar results.\nCheers.\n","description":"","id":192,"section":"posts","tags":null,"title":"Oracle deadlocks - what happens?","uri":"http://localhost:1313/RantsAndRaves/posts/2009/08/oracle-deadlocks-what-happens/"},{"content":"I\u0026rsquo;m not a religious person - I think and check the evidence rather than accepting the word of a misogynistic bloke in a frock.\nHowever when confronted by a religious zealot I find the following conversation goes down rather well! But then, I\u0026rsquo;m a right b\u0026rsquo;stard!\nMe: Do you believe in Santa Claus?\nReligious Person: No, don\u0026rsquo;t be stupid.\nMe: But you believe in god?\nRP: Yes, of course.\nSo what\u0026rsquo;s the difference? They are both \u0026lsquo;invisible\u0026rsquo;, both live in \u0026lsquo;far away\u0026rsquo; places, both know if you are good or bad, both will reward you for being good or punish you for being bad \u0026hellip;\nThe other thing about RPs in general (ok, the so called Christians) is that when you point out that something in the Bible - all characters and events in this book are fictitious, any resemblance to persons living or dead is entirely coincidental - they always say that I shouldn\u0026rsquo;t take any notice of that bit.\nExcuse me? Is this the word of god or not? Who decides what we pay attention to or otherwise then? If you are picking and choosing which bits of the \u0026lsquo;rule book\u0026rsquo; to take note of and which to ignore, then surely you are making it all up as you go along and choosing your own customised version of the bible.\nOne god? I rather suspect not! The Hindu religion alone has 33 milion gods and godesses, Most of them reside in a cow, apart from the Good Luck godess, who didn\u0026rsquo;t want to live in a cow, but then changed her mind and the only room left was in the cow dung! That\u0026rsquo;s why Hindus burn cow dung for good luck. You couldn\u0026rsquo;t make this stuff up! (Oh!\u0026hellip;..)\nAnd one last thing, why exactly do they put lightning conductors on church spires then? Have they no faith? :-D\n","description":"","id":193,"section":"posts","tags":null,"title":"God or Santa?","uri":"http://localhost:1313/RantsAndRaves/posts/2009/07/god-or-santa/"},{"content":"All organisations need Change Control to make sure that changes are applied correctly and so on. However, have Change Control departments become the new \u0026ldquo;Health \u0026amp; Safety\u0026rdquo; and gone beyond being useful to being a disruptive and obstructive unit interested only in getting in the way rather than getting work done?\nI\u0026rsquo;m all in favour of change control - it serves a purpose and helps prevent invalid changes being applied to production (or test) systems without them having been checked, tested and applied correctly with full cooperation from the system\u0026rsquo;s users.\nHowever, I\u0026rsquo;m seeing a trend develop whereby the cascade or new rules and regulations, redesigned forms, consultation matrices and all the attendant palaver have simply become a joke. Change Control are rapidly hurtling down the road to becoming the new Health \u0026amp; Safety with all the ludicrous diktats that they produce.\nThese people are left in control of whether a change is applied or not; but as to the quality of the code being applied, they care not a jot. However, get a date wrong on a form (filled in in multiple copies of course!) and all hell breaks loose with arrogant emails winging their way around everyone in the entire universe to let them know that you failed to fill in a date correctly.\nFinished the task? All went ok? Now fill in a Post Implementation review form or else. Even when there are no actual changes taking place - which they will know from reading the myriad of forms we have sent them prior to getting approval for the \u0026lsquo;change\u0026rsquo; - they insist on knowing what was changed and how well it went!\nIf a change is raised as an emergency, I need to apply it now and right this minute. I do not need or want some numpty who doesn\u0026rsquo;t know what the change is actually doing or for telling me that it\u0026rsquo;s not really an emergency because I put a date on to start the work in 5 days time - which is what I have to do in order to get a change approved.\nThen, equally, when we agree that perhaps maybe it might actually be an emergency, they then spend the next three days trying to get me to update the work plan to insert the correct end dates and times for the various parts of the change.\nMy advice is this, change hap[pens, it needs to be controlled, and this is a good thing. However, Change Control departments need to butt out and start doing their job and stop telling the rest of us how to do ours - after all, we don\u0026rsquo;t go to their office and tell them how to sweep up do we! ;-)\n","description":"","id":194,"section":"posts","tags":null,"title":"Have Change Control become the new Health \u0026 Safety?","uri":"http://localhost:1313/RantsAndRaves/posts/2009/07/have-change-control-become-the-new-health-safety/"},{"content":"The following is a pretty nice expression evaluator for Oracle\u0026rsquo;s PL/SQL language. You pass it a string containing an expression that would return a numeric value when evaluated and it will evaluate the entire expression and return the number.\nThe passed expression must be in valid Oracle syntax or you will get a NULL instead of a number.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 CREATE FUNCTION expression (iExpression IN varchar2) RETURN number AS vResult number; BEGIN ----------------------------------------------------------- -- Expression evaluator for PL/SQL. -- Pass in a string containing the expression you want to -- evaluate using correct Oracle syntax and the result will -- be returned. -- -- WARNING: Causes a parse for every expression and will -- soon fill your cache with similar statements. ----------------------------------------------------------- -- Norman DunbarÂ 02 July 2009Â Created new function. ----------------------------------------------------------- execute immediate \u0026#39;begin :r := \u0026#39; || iExpression || \u0026#39; ; end;\u0026#39; using OUT vResult; RETURN vResult; EXCEPTION WHEN others THEN RETURN NULL;Â -- It all went horribly wrong! END; / Of course, it has its drawbacks, the biggest one being that it will, if called repeatedly with different expressions, fill up your cache with similar statements.\nSo my challenge is to come up with a similar and equally short PL/SQL routine that will not age potentially useful SQL out of the cache while leaving multiple copies of similar SQL statements such as:\n1 begin :r := 2+2; end; and so on lying around taking up valuable cache space.\nCheers.\n","description":"","id":195,"section":"posts","tags":null,"title":"PL/SQL expression evaluator","uri":"http://localhost:1313/RantsAndRaves/posts/2009/07/plsql-expression-evaluator/"},{"content":"The recent revelations about the inhuman brutality and abuse heaped on innocent children by the so called Sisters Of Mercy and/or the Christian Brothers over 60 odd years in Ireland is an abomination.\nSummary of the report\nReactions\nBut what is worse is the fact that before the report came out damning the Catholic Church for covering up and/or ignoring the abuse (physical, mental and sexual) a couple of things happened.\nThe Christian Brothers successfully sued in 2004 (the report was 9 years in the making)Â banning the report from naming names and so no-one can be prosecuted.\nThe Catholic Church negotiated with the Irish Government with the end result that it is the Irish Tax Payers who are paying the vast majority of the compensation for the crimes committed by the Catholic Church.\nSomething here doesn\u0026rsquo;t make sense to me.\nEven better, when the Irish citizens discovered this fact and created a row about it (as reported on the BBC news this morning - Tuesday 26th May 2009) the Church simply announced that it would not be renegotiating the deal.\nI fail to see how or why the Catholic Church - one of the richest institutions in the world - should be allowed to commit heinous crimes over 60 odd years and then get off by having the people that the crimes were committed against, pay the compensation. Despicable.\nMy advice to people worldwide who wish to have children is simple:\nDon\u0026rsquo;t. There are far too many people on this planet already and we do not need any more. If you ignore my advice above, don\u0026rsquo;t forget to keep your children well away from the Catholics. Actually, keeping your children well away from any form or religion sounds like excellent advice to me. Live in the real world and not the one favoured by these people who believe in an invisible sky fairy who rules over us all.\nCheers.\n","description":"","id":196,"section":"posts","tags":null,"title":"If I committed a crime, would the government pay for me?","uri":"http://localhost:1313/RantsAndRaves/posts/2009/05/if-i-committed-a-crime-would-the-government-pay-for-me/"},{"content":"MP\u0026rsquo;s expense claims seem to be causing all sorts of trouble at the moment.\nIt\u0026rsquo;s extremely funny how The Daily Mail suddenly went all quiet on the subject after it was discovered that the Tories had been just as dishonest as Labour. Funny that.\nI work for a living and if I get a job in London and have to live there, I can claim a limited amount of my overall expense back. This is fine.\nWhy is it that MPs, who are doing nothing more than a job - like the rest of us - can claim all sorts of additional expenses over and above what we lowly workers can claim?\nI can\u0026rsquo;t get a load of scatter cushions on expenses for my flat in London, not that I would allow any scatter cushions within the four walls mind you, so why the f*ck should MPs be allowed to? At our expense as well?\nWhen they put themselves up for election they know what the job entails, London, and if they don\u0026rsquo;t want to have the same overheads of working away from home as the rest of the population, then tough, don\u0026rsquo;t apply for the bloody job then.\nIs it just me that thinks these people have some sort of superiority complex over the rest of us? Being am MP is doing a job, nothing more, nothing less. They get paid for it and incur expenses like we all do. Deal with it in the same way that we have to.\nIt\u0026rsquo;s not bloody rocket science after all. (But if it was, then you can bet that the rocket scientists wouldn\u0026rsquo;t be getting tax payers to fund their scatter cushion fund!)\nCheers.\n","description":"","id":197,"section":"posts","tags":null,"title":"Might as well jump on the bandwagon","uri":"http://localhost:1313/RantsAndRaves/posts/2009/05/might-as-well-jump-on-the-bandwagon/"},{"content":"Driving to work this morning when I hit the Kirkstall Road area where a perfectly good dual carriageway has been split into a bus (and cyclist) lane only and a lane for the rest of us. at the end of this section of the road, there is a set of traffic lights that only affect the non-bus (and cyclist) lane.\nThe purpose of these lights is to stop the normal people from getting to work on time, while allowing those on a bus the ability to get about 20 yards further up the road than they would normally if there were no lights there. Go figure, this is Leeds after all.\nThe lights are operated - in other words, they turn red - when a bus or coach passes over a sensor in the road about 300 yards back from the lights themselves. This allows the bus to trigger said sensor and travel at breakneck speed - as they do - towards the lights which will have turned red by the time it gets there.\nUnfortunately, this morning, all the bloody cyclists were triggering the sensor as well. So every bloody cyclist passing over the section of road where the sensor was buried, turned the lights red. We all have to stop while no buses are coming and the cyclists are so far away from the lights that they will be green by the time they get there.\nWhat a crock of shit!\n","description":"","id":198,"section":"posts","tags":null,"title":"Bloody cyclists have a new weapon!","uri":"http://localhost:1313/RantsAndRaves/posts/2009/05/bloody-cyclists-have-a-new-weapon/"},{"content":"Alison and I went to Harrogate on Saturday and spent the day building a couple of beehives. Alison hasn\u0026rsquo;t done any form of wood workery before and she did a really good job of putting her hive together, although her hammering technique was a tad amusing, but then again, as she\u0026rsquo;s never had to use a hammer before why should she know?\nMe? Well, I\u0026rsquo;m a professional (mechanic and DBA) and I managed to get a bit of mine glued and nailed into the wrong location and, I think, upside down as well! Duh! A quick bash with a hammer and chisel and it was retrieved ready to be fixed onto the right place.\nAt present the hives are store in the garage and are exuding a very pleasant \u0026ldquo;woody\u0026rdquo; smell. Hopefully soon we will get them into the garden and full of bees.\nFor details of bee keeping in the West Yorkshire area, check out http://www.hrbka.org.uk/ which is, if I do say so myself, a rather fine website.\nWe also prepared for the arrival of our new workers by purchasing a smoker, a hive tool, some frames and foundation (for the brood box) from Claro Bees and ordering a couple of rather natty jackets with a built in veil. Both in bridal white as well so we shall look very dashing indeed!\nClaro Bees is an intersting shop. It looks like a portakabin!\nCheers.\n","description":"","id":199,"section":"posts","tags":null,"title":"A different way to spend a Saturday ...","uri":"http://localhost:1313/RantsAndRaves/posts/2009/05/a-different-way-to-spend-a-saturday/"},{"content":"On the QL we have traditionally had a 36 character limit on file names after the initial (5 character) device name. Something like this:\nflp1_source_code_myfile_c\nIn the above, flp1_ is the device name and is not included in the 36 character limit. There can be a network device tagged on the front as well, and this too is not included in the filename limit.\nTechnically, in the above, source and code are both directories (the default directory separator is the underscore character) while myfile_c is the filename and extension.\nIn the original days of the QL, all we had was a pair of built in microdrives and so the file name limit wasn\u0026rsquo;t too much of a hassle because we only had about 100KB (yes, KB) to play with and there were no actual directories as such.\nThen we moved on to floppies and even a 40 MB (yes, MB) hard drive - and suddenly, this limit began to look rather silly, especially as we now had proper directories.\nThe problem, as I see it is simple. Inside the directory entry for a filename, a space of 36 characters has been reserved. Unfortunately, given the above filename as an example, the following will happen:\nThe root directory on the floppy disc will contain an entry for a directory named source. The source directory will have an entry for a directory named source_code. The source_code directory will have an entry for a file named source_code_myfile_c. You can see the problem, the entire directory structure, less the device name, is replicated all the way down the tree. Surely it makes sense, even with a 36 character limit, to only have the appropriate parts of the full path in each directory. For example:\nThe root directory on the floppy disc will contain an entry for a directory named source. The source directory will have an entry for a directory named code. The code directory will have an entry for a file named myfile_c. With this system, we would be able to have almost unlimited depth to our disc structures - possibly not wise - and each part of a file\u0026rsquo;s full path would be limited to 36 characters, not the entire path itself.\nWell, I think it makes sense anyway.\nCheers.\n","description":"","id":200,"section":"posts","tags":null,"title":"What's wrong with this file system?","uri":"http://localhost:1313/RantsAndRaves/posts/2009/05/whats-wrong-with-this-file-system/"},{"content":"As mentioned previously on this blog, we have had a large number of breeding frogs in our pond.Well the tadpoles have hatched and are growing well. At least, the ones that manage to stay well hidden are. :-(\nThose damned Koi are eating machines and there is wholesale slaughter going on in the pond at the moment. The volume of tadpoles is dwindling quite rapidly as those fat, greedy bastards (have I mentioned that I don\u0026rsquo;t like them?) gorge themselves on anything that moves.\nWhy can\u0026rsquo;t they eat the cats that we get in the garden I wonder?\nI\u0026rsquo;d love to don my scuba gear and go spear fishing in the pond. Only kidding! You can\u0026rsquo;t use scuba gear when spear fishing in the UK - so I\u0026rsquo;ll have to use my snorkel instead!\nAt least I managed to save afew lumps of spawn and move them to the little ponds in the back garden for safety. No fish in there - just how I like it. Those little ones are doing well so far.\nCheers.\n","description":"","id":201,"section":"posts","tags":null,"title":"I hate those damned koi carp!","uri":"http://localhost:1313/RantsAndRaves/posts/2009/04/i-hate-those-damned-koi-carp/"},{"content":"Walking through Leeds today, traffic doing its thing as usual. Then, in the distance, it could be heard\u0026hellip;\u0026hellip;\u0026hellip;. thump thump thump getting louder and louder and louder. Soon, it appeared, yet another bloody moron in a beat up old Corsa (or similar) with a dustbin exhaust - take note, that changes the engine\u0026rsquo;s breathing and will stuff your engine a lot quicker than you think. Intake and exhaust are balanced for best results by professional engineers, and you are not one of those.\nThe noise, for it could hardly be described as music, is pumping out with maximum base applied and the volume turned up as loud as it will go (all the wat to 11 for Spinal Tap fans!).\nNow, I love my music, but I listen to it, not everyone else on the bloody planet! I am smart enough to realise that not everyone has my taste in music, so I don\u0026rsquo;t force everyone to listen to it. Why can\u0026rsquo;t these driving morons have similar consideration for the rest of us?\nThere is an additional problems too, not just the noise. The volume is so high that whatever music was intended to be played has no chance thanks to the total harmonic distortion of about 100% introduced by the crap quality speakers! You can usually hear them trying desperately to escape from their mountings.\nThe \u0026lsquo;C\u0026rsquo; is silent - as in Rap Music. ;-)\n","description":"","id":202,"section":"posts","tags":null,"title":"Why is it that the people with the worst taste in music, play it loudest?","uri":"http://localhost:1313/RantsAndRaves/posts/2009/04/why-is-it-that-the-people-with-the-worst-taste-in-music-play-it-loudest/"},{"content":"As I came into work this morning (21st April 2009) I was listening to the man from the Driving Test \u0026lsquo;agency\u0026rsquo; on Radio 4.\nThe person was asked about the proposals for adding a new section to the test where the examiner gives the trainee driver a set of instructions similar to directions obtained from a passer by,Â and whether a trainee would be failed for getting the instructions wrong.\nThe man from the agency replied , with a straight face, that \u0026ldquo;we do not fail people. We have people who pass and those who don\u0026rsquo;t pass\u0026rdquo;. The Radio 4 interviewer practically wet himself laughing.\nMe? I was screaming at the radio again! What the f*ck are we doing to this country with all this crap about political correctness and not hurting people\u0026rsquo;s feelings? It is all lies I tell you, damned lies.\nThere are failures in exams, driving tests and so on, no matter what you call it, they are failures and it is a fact of life. Stop trying to kid on that people don\u0026rsquo;t fail, they effing well do - I should know, I failed my maths Higher (A Scottish Educational exam) where you get a grade between A and F, I got nothing, not a mention, zilch, bugger all in fact.\nI failed it so miserably indeed.\nAm I bitter? No.\nHas it affected my life since them? Not yet.\nDo I care that I failed, well yes I do, but I know for a fact that I did fail, I didn\u0026rsquo;t not pass!\nCheers.\n","description":"","id":203,"section":"posts","tags":null,"title":"Political correctness my arse!","uri":"http://localhost:1313/RantsAndRaves/posts/2009/04/political-correctness-my-arse/"},{"content":"I was shouting at a nun on TV the other night. The silly woman had [thankfully] survived the Italian earthquake and in retelling her story she said words along the lines of \u0026ldquo;I woke up when the quake started and the walls began to fall down. I got up and rushed outside. I only escaped because of god.\u0026rdquo;\nDoes the woman not realise that it was her \u0026lsquo;god\u0026rsquo; who was responsible for the damned earthquake in the first place? Or does that not count? Funny how \u0026lsquo;god\u0026rsquo; is only ever mentioned for the good stuff.\nWhy do they put lightning conductors on church spires then?\n","description":"","id":204,"section":"posts","tags":null,"title":"God and the Italian earthquake","uri":"http://localhost:1313/RantsAndRaves/posts/2009/04/god-and-the-italian-earthquake/"},{"content":"Well, the frogs have almost gone. After a mass orgy of croaking all night and day (do they ever sleep?) and sex all day and night (lucky b*ggers!) they have started leaving. The pond is returning to the domain of the Koi carp (which I hate almost as much as I hate cats) and is full of spawn. This morning there were only about a dozen frogs left, a couple still paired up, but most of the rest had gone. Where to I wonder?\nNo doubt that damned cats will be having a field day killing and maiming the poor b*ggers as they wander back to whence they came. Wish I could do the same for the cats - send them back to wherever they came from I mean. (Ok, I admit it, in a box if necessary!)\nFunnily enough, this year we have no frogs in the ponds at the back of the house, only in the one at the front. Last year we had lots at the back as well - but that was where the cats used to hang out - until I put the cat zapper in. I just wonder if those damned murdering b\u0026rsquo;stards managed to kill off all the frogs at the rear?\nAnyway, we seem to have a leak in the smallest pond at the rear, so I\u0026rsquo;ll have to get patching soon.\nCheers.\n","description":"","id":205,"section":"posts","tags":null,"title":"The frogs are gone ...","uri":"http://localhost:1313/RantsAndRaves/posts/2009/03/the-frogs-are-gone/"},{"content":"Coming home from our bee keeping course on Tuesday night (10th March 2009) Alison and I spotted a ghostly shape on the grass verge in the dark. Turns out to have been a live badger. We stopped (yes, it was safe to stop!) and it wandered away a few yards, paused and watched us watching it. We remained thus for about 10 seconds until it lost interest in us and wandered off into a field.\nAt least it wandered away from the road!\nI\u0026rsquo;ve only ever seen wild badgers as corpses until now! Usually at the side of the road where so many end up. I have to say that they are pretty hard to see in the gloom. Must be something to do with their colouring and us humans being pretty blind at night as well!\nFoxes are even more difficult to see at night - unfortunately, it\u0026rsquo;s usually too late when you do spot them!\nCheers.\n","description":"","id":206,"section":"posts","tags":null,"title":"Live badger spotted in Yorkshire!","uri":"http://localhost:1313/RantsAndRaves/posts/2009/03/live-badger-spotted-in-yorkshire/"},{"content":"Just like last year, the mass orgy of frog sex is taking place on Alison\u0026rsquo;s birthday! We have more mating frogs that you can shake a stick at in the pond right now. Most are spawning - as they do - some already have. It\u0026rsquo;s interesting to see how the males lose interest in the females once sex has been had!\nOne problem I have this year, lots of the frogs are getting up the pump outlet pipe and into the filtration system. I\u0026rsquo;ve extracted about 20 pairs of frogs from the filter bed this weekend already. I don\u0026rsquo;t want them going up there and spawning as they will clog up the works and, most likely, die as they cannot get out and there\u0026rsquo;s nothing to eat in there either.\nAll I need now is some method of stopping them getting up the pipe and, much more importantly, some method of preventing those damned Koi from eating all the tadpoles. I have suggested a harpoon but I have been over-ruled by Alison on the matter as she loves her fish.\nCheers.\n","description":"","id":207,"section":"posts","tags":null,"title":"The frogs are spawning again","uri":"http://localhost:1313/RantsAndRaves/posts/2009/03/the-frogs-are-spawning-again/"},{"content":"Tanel Poder has started what looks to be an excellent series of posts on his blog on the subject of Oracle diagnostic events (eg 10046 traces) at this location.\n","description":"","id":208,"section":"posts","tags":null,"title":"Oracle diagnostic events","uri":"http://localhost:1313/RantsAndRaves/posts/2009/03/oracle-diagnostic-events/"},{"content":"I ranted previously about cyclists. Tonight, coming home from work, I saw the biggest moron on two wheels yet.So there we were at a junction. I was at the head of the queue on a dual carriageway. There was traffic crossing from our left and a cyclist arrived up the inside of a large van and cycled to the head of the queue and pulled in front of the van!\nThe cyclist had no protective clothing or head gear at all. She, for it was a woman, was dressed in her sunday best by the look of things! She was now at the front of the queue, however, her position on the road put her slap bang in the middle of the other side of the dual carriageway and right in the path of the oncoming vehicles.\nThis woman was completely unaware of the carnage heading her way as she started to pull out into the road without looking in any direction what so ever! She was missed - very closely - by at least 4 cars. None of them much further away from destroying her than 6 inches. I kid you not.\nOnce she had managed to avoid all the oncoming traffic, she wobbled directly into the path of the van she had just queue jumped. Still not looking and still, miraculously, breathing and unhurt. As the van took avoiding action to avoid this lunatic, the driver tooted his horn. His reward for not killing the cyclist?\nShe gave him the finger.\nI suspect this woman is not long for the world. Some people should not be allowed to breed!\nCheers.\n","description":"","id":209,"section":"posts","tags":null,"title":"Another cycling moron","uri":"http://localhost:1313/RantsAndRaves/posts/2009/02/another-cycling-moron/"},{"content":"Last night Alison found the first frog of the year in our driveway. The snow that has been around for a couple of weeks now had vanished as the temperature raised itself to a heady 6 degrees C and out she came, (the frog, not Alison!) heading for the big pond.\nLast year, on March the 9th (Alison\u0026rsquo;s birthday), I counted over one hundred froggy heads in our pond by the front door. It was very pleasant sitting in our lounge listening to them sing the evening away. About a week or so later, they were gone butÂ their spawn filled the pond. When it had all hatched, the water was black with tadpoles. Nice!\nUnfortunately, when we bought the house, the pond we inherited was full of fish too. Some Koi Carp and some yellow things and a couple of black versions of the yellow ones - can you tell I know what I\u0026rsquo;m talking about - and I\u0026rsquo;m sad to say that they completely \u0026lsquo;dysoned\u0026rsquo; the tadpoles up leaving nothing at all. I never saw a single young frog from the big pond last year. :-(\nWe did get a few froglets from the smaller ponds round the back of the houseÂ as there are no fish there. Unfortunately, all the neighbourhood cats used to play around there and hunt the frogs, squirrelsÂ and birds. Murdering bastards!\n\u0026ldquo;They are only doing what comes naturally\u0026rdquo; cat owners are fond of saying and this is true, but if I had a python and it did what they do naturally, to their cats, I\u0026rsquo;m sure they would change their tune! Keep your effing cats out of my garden! I\u0026rsquo;m pretty sure that if I took all the cat shit back to the owners they wouldn\u0026rsquo;t be too enamoured either - so why do I have to put up with it?\nI hate cats by the way, can you tell?\nI\u0026rsquo;m happy for you to own a cat but keep the effing things inside where they can do no harm otherwise, don\u0026rsquo;t be surprised if they don\u0026rsquo;t come home one night - if only I could set up a cat killing laser defence system around my garden life would be grand! For the moment, I have to be satisfied with the RSPB approved cat deterrent device which emits ultrasonic sound that cats, dogs and foxes find uncomfortable. It seems to work!\nI\u0026rsquo;m also not all that fond of the fish either, but I do like frogs. Unfortunately, Alison likes the fish and indeed, the cats, so a mass slaughter of these killer alien creatures is somewhat forbidden at the moment I\u0026rsquo;m afraid to say. She won\u0026rsquo;t let me buy a crossbow from Amazon either. Is there no justice in the world?\nWith a bit of luck, I\u0026rsquo;m hoping to help boost the frog population this year by grabbing a load of spawn from the big pond and using it to fill the smaller ponds at the back of the house. They should be safer there this year as the cats now populate the front garden instead.\nI\u0026rsquo;d much rather I just slaughter the fish (foul creatures that they are) and the cats too. Still, there\u0026rsquo;s time!\nCheers.\n","description":"","id":210,"section":"posts","tags":null,"title":"The frogs are coming!!!","uri":"http://localhost:1313/RantsAndRaves/posts/2009/02/the-frogs-are-coming/"},{"content":"For many years, various big guns - and a lot of smaller ones - in the Oracle world have been advocating, nay demanding, that we [almost] always use bind variables in our SQL code. The reason is simple, it\u0026rsquo;s shareable, efficient,Â reduces parsing and allows the application to scale up to more and more users.\nOver the years I have spent fixing and tuning Oracle databases, I have noticed a trend of developers moving away from hard coding everything to using binds more and more in their code. How refreshing, the performance must also be improving, mustn\u0026rsquo;t it?\nNot quite! :-(\nThe problem is becoming apparent that a lot of developers don\u0026rsquo;t know anything/much about Oracle and also don\u0026rsquo;t know anything/much about their own chosen language either.\nI\u0026rsquo;m pleasantly surprised when I see code like the following in the cache on my databases :\n1 2 SELECT stuff FROM some_table WHERE some_column = :a_bind_variable; It\u0026rsquo;s a start, but it is still broken. How can this be?\nWhile theÂ developer is indeed using binds, s/he has not considered how that statement is created within the application. As there are many application coding languages, I shall resort to my own pseudo-code to demonstrate the problem.\n1 2 3 4 5 6 7 8 9 Function GetEmpForID(int empId, Connection conn) begin Statement Stmt(conn, \u0026#34;Select stuff from some_table where some_column = :some_value\u0026#34;); Stmt.Parse(); Stmt.Bind(\u0026#34;:some_value\u0026#34;, empId); ResultSet Rslt =Â Stmt.Execute(); //Process Results here end Looks great doesn\u0026rsquo;t it? Well, no, it doesn\u0026rsquo;t. The major problem is that fact that the statement will be parsed each and every time that this function is called. Within a function (in most languages) the statement is created on the stack and on exit from the function, deleted.\nIn reality, we don\u0026rsquo;t get much in the way of efficiency improvements because we still have a 1:1 parse:execute ratio, not what we want at all. What we need to do is make the statement external to the function (global or whatever the language supports) similar to the following Java-ish pseudo-code.\n1 2 3 4 5 6 7 8 9 10 11 12 Connection conn; Statement StmtEmpForId(conn, \u0026#34;Select stuff from some_table where some_column = :some_value\u0026#34;); Function GetEmpForID(int empId) begin external Statement StmtEmpForId; if not StmtEmpForId.Parsed() then StmtEmpForId.Parse(); StmtEmpForId.Bind(\u0026#34;:some_value\u0026#34;, empId); ResultSet Rslt =Â StmtEmpForId.Execute(); //Process Results here end Now we are getting somewhere! The statement is external to the function itself, so exists even when the function has exited. If this is the first time that we call the function, the statement will be found to be unparsed, so we can parse it - this avoids the application parsing statements at startup and possibly parsing statements that never get used.\nOnce parsed, and on every subsequent call of the function, all we have to do is bind the variable and execute the statement before processing the results.\nAs long as the statement (and connection)Â remain in scope (hence being globals) then we only parse the statements once regardless of the number of times that we execute it. So, if we execute our one statement a million times, we have only carried out one single [hard] parse.\nIn C++ you would, I suppose, create an appConnection object with members for the Oracle (OCCI?) Connection object and each of the Statements you wish to use in the application, after all, the statements should remain in the same scope as the connection shouldn\u0026rsquo;t they?\nIn Java, well, I have no idea, all the Java stuff I\u0026rsquo;ve looked at seems to be reinventing the wheel over and over again - who knows what those guys get up to! ;-)\nCheers.\n","description":"","id":211,"section":"posts","tags":null,"title":"It must be efficient, I'm using bind variables!","uri":"http://localhost:1313/RantsAndRaves/posts/2009/02/it-must-be-efficient-im-using-bind-variables/"},{"content":"In a previous posting here on the subject of Lazy Developer Syndrome, I showed a small fragment of code where I SELECTed the ROWID in addition to all the other data I wanted, then UPDATEd the same row using the ROWID I had stored rather then using the Primary Key index that I used to SELECT the row in the first place. Why did I do this?\nSelecting the ROWID is always a good idea if you intend to UPDATE or indeed, DELETE, it afterwards.Â The ROWID is after all the fastest way to get to a row in your table. It\u0026rsquo;s how the indexes get you to the data after all.\nIf you have a primary key constraint and the index supporting it has a height of 2, then to get from your supplied primary key value to the data requires 3 single block accesses - the index root and the correct leaf block. From the leaf block we get the ROWID for the requested primary key value, and a further single block read takes place to fetch back the block containing the required row.\nSome of these blocks may be in the cache but then again, they might allÂ not be - so 3 physical reads would be required as the worst case and 3 logical reads as a best case.\nAn UPDATE specifying the primary key value again will also have to perform 3 more single block reads, hopefully from the cache. However, an UPDATE using the ROWID already fetched will only have to perform one block read, again hopefully from the cache, and that block will be the actual data block to be amended. We don\u0026rsquo;t need to use the index and we save 2 single block reads. Good eh?\nA DELETE using a ROWID or a primary key value will also take differing numbers of block reads. If you delete using the ROWID, the index has to be read and updated by removing the entry for the row in question and if you DELETE using the Primary Key value, the index will be read again (in consistent mode) and the data block will be read in current mode in order to make the DELETE work.\nMy own testing, so far, has shown that the difference is a single block read saved when using the ROWID rather than the Primary key value however, that figure will change if the height of the index itself changes.\nUpdate by Primary Key. 1 2 3 4 SQL\u0026gt; update indextest set name = name where id = 86057; 1 row updated. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Execution Plan ---------------------------------------------------------- 0 UPDATE STATEMENT Optimizer=ALL_ROWS (Cost=2 Card=1 Bytes=79) 1 0 UPDATE OF \u0026#39;INDEXTEST\u0026#39; 2 1 INDEX (UNIQUE SCAN) OF \u0026#39;INDEXTEST_PK\u0026#39; (INDEX (UNIQUE)) (Cost=1 Card=1 Bytes=79) Statistics ----------------- 1 db block gets 2 consistent gets 0 physical reads 1 rows processed Update by ROWID. 1 2 3 4 SQL\u0026gt; update indextest set name=name where rowid = \u0026#39;AAAXktAAEAAAET8ABI\u0026#39;; 1 row updated. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Execution Plan ---------------------------------------------------------- 0 UPDATE STATEMENT Optimizer=ALL_ROWS (Cost=1 Card=1 Bytes=78) 1 0 UPDATE OF \u0026#39;INDEXTEST\u0026#39; 2 1 TABLE ACCESS (BY USER ROWID) OF \u0026#39;INDEXTEST\u0026#39; (TABLE) (Cost=1 Card=1 Bytes=78) Statistics ----------------- 1 db block gets 0 consistent gets 0 physical reads 1 rows processed Delete by Primary Key. 1 2 3 SQL\u0026gt; delete from indextest where id = 86057; 1 row deleted. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Execution Plan ---------------------------------------------------------- 0 DELETE STATEMENT Optimizer=ALL_ROWS (Cost=1 Card=1 Bytes=13) 1 0 DELETE OF \u0026#39;INDEXTEST\u0026#39; 2 1 INDEX (UNIQUE SCAN) OF \u0026#39;INDEXTEST_PK\u0026#39; (INDEX (UNIQUE)) (Cost=1 Card=1 Bytes=13) Statistics ----------------- 5 db block gets 2 consistent gets 0 physical reads 1 rows processed 1 2 SQL\u0026gt; rollback; Rollback complete. Delete by ROWID. 1 2 3 SQL\u0026gt; delete from indextest where rowid = \u0026#39;AAAXktAAEAAAET8ABI\u0026#39;; 1 row deleted. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Execution Plan ---------------------------------------------------------- 0 DELETE STATEMENT Optimizer=ALL_ROWS (Cost=1 Card=1 Bytes=25) 1 0 DELETE OF \u0026#39;INDEXTEST\u0026#39; 2 1 TABLE ACCESS (BY USER ROWID) OF \u0026#39;INDEXTEST\u0026#39; (TABLE) (Cost=1 Card=1 Bytes=25) Statistics ----------------- 5 db block gets 1 consistent gets 0 physical reads 1 rows processed 1 2 3 4 SQL\u0026gt; rollback; Rollback complete. SQL\u0026gt; exit So, when creating your application, the idea is to build in performance from the start (ie, you do not add it in after the fact!) so when reading data that will be updated or deleted in a subsequent operation, always grab hold of the ROWID and use that to make your DELETE or UPDATE as efficient as you can.\nBy the way, never (can we say \u0026rsquo;never\u0026rsquo; in these politically correct times?) store the location of a row in another table as a ROWID - if you do, and you ever export the table and reimport it, the ROWIDs you carefully saved will no longer point to the row that you thought they did!\nDid you never wonder why indexes are stored in the export file as a CREATE INDEX statement, and not as the pure index data itself?\nCheers.\n","description":"","id":212,"section":"posts","tags":null,"title":"ROWIDs are fun","uri":"http://localhost:1313/RantsAndRaves/posts/2009/01/rowids-are-fun/"},{"content":"To those who don\u0026rsquo;t know me, I\u0026rsquo;m an Oracle DBA and I also can develop as well. I detest having to work on applications which have the following construct in the code:\n1 2 3 4 SELECT \u0026lt;stuff\u0026gt; FROM TABLE WHERE \u0026lt;something\u0026gt; FOR UPDATE; In most cases, the above is a sign of developer laziness. They cannot be bothered to write correct code to handle a situation where a row that the user has been working on (unlocked) has been amended by another user in the meantime.\nIn Oracle you should lock late and lock short (as Dave Ensor says). Only lock a row as you are about to update it. This is optimistic locking.\nDo not lock it when you start working on it and keep it locked until you finish the update. This is pessimistic locking. When you write an application that does this, you end up with rows locked out all over the place as people get half way through amending some details and then have to answer the phone, or pop outside for a ciggie, or go to the loo.\nPessimistic locking - just say NO!\nIt isn\u0026rsquo;t difficult, for example, to add a column to the table to hold an update counter (or similar) and to have a trigger update that counter by one each time an UPDATE is performed. The developer would read in the current value along with the data he wishes to display for amendment and the ROWID for the row in question.Â Once the user has made the changes and clicked OK (or whatever) what\u0026rsquo;s wrong with running the following statement :\n1 2 3 4 UPDATE \u0026lt;table\u0026gt; SET \u0026lt;stuff\u0026gt;=\u0026lt;new_stuff\u0026gt; WHERE ROWID=:stored_rowid AND upd_counter=:stored_upd_counter; If it works, job done, slip in a COMMIT while no-one is looking, that will release the locks and will save the data permanently to the table with the changes made and the trigger\u0026rsquo;s change to the update counter carried out as well.\nIf, on the other hand, it returns no rows updated, we know some other user got there before us and changed the value in the update counter column. Now, we can handle that by displaying a message to the user and perhaps switching to a new screen showing the user\u0026rsquo;s own changes and the other user\u0026rsquo;s changes and then request that our user chooses to either force through his own changes,Â accept the other users changes aborting his own or to redisplay the previous screen with the new data loaded for further amendments.\nIt\u0026rsquo;s much more friendly for all users of the application but it takes a little more thought on the developer\u0026rsquo;s side of things. It\u0026rsquo;s easier for a lazy developer to simply choose not to think and blindly carry on with his SELECT ... FOR UPDATE nonsense.\nCheers.\n","description":"","id":213,"section":"posts","tags":null,"title":"Lazy developer syndrome","uri":"http://localhost:1313/RantsAndRaves/posts/2009/01/lazy-developer-syndrome-and-rowids/"},{"content":"I suspect not.\nhttp://www.newscientist.com/article/mg20026873.100-how-kangaroo-burgers-could-save-the-planet.html\nThe gist of the above article is this, worldwide, livestock creates more greenhouse gasses than all forms of transport combined. Eating meat is bad for the planet - can you hear all those veggies crowing now? :-) Never mind, Kangaroos taste good and can help save the planet. Check out the link above for the full story.\nCheers.\n","description":"","id":214,"section":"posts","tags":null,"title":"So transport is killing the planet is it?","uri":"http://localhost:1313/RantsAndRaves/posts/2009/01/so-transport-is-killing-the-planet-is-it/"},{"content":"I hate cyclists. They are so full of themselves! They sit there, two or three abreast on narrow roads, dawdling along having a nice chat, completely oblivious to the queue of traffic behind them. Give them a happy toot (ie, not a nasty long drawn out blast) on the horn to let them know you are there and you get a load of abuse. Nice \u0026amp; friendly!\nI was once run over by a cyclist. I was on a pedestrian crossing, with the green man telling me I could cross. The cyclist [must have] overtaken a queue of stationary traffic to get me, but he managed it. He was apparently watching some scantily clad women on the other side of the road, his ipod (other personal stereos are available ;-) ) was turned up as loud as it would go and he hit me.\nWhen he saw what had happened, his excuse was that \u0026ldquo;the lights were green!\u0026rdquo; which was highly amusing to the taxi driver at the front of the queue (he said as much out of his window while calling the cyclist a few choice names too) as he had overtaken the taxi in order to get me!\nI have also been narrowly missed on numerous occasions when the lights were red (for traffic) by cyclists (usually clad in those tight shiny suits) who apparently do not have to stop at the red lights. When I comment as they pass, that \u0026ldquo;the lights are red\u0026rdquo; - they all seem to have serious doubts about the marital status of my parents at the time of my conception! Hang on, I\u0026rsquo;m the one with right of way so why am I the \u0026lsquo;bastard\u0026rsquo;?\nDon\u0026rsquo;t bother to mention one way streets either, they are apparently not applicable to cyclists.\nOk, I\u0026rsquo;m not tarring all cyclists with the same brush, there are many who do have consideration for other road users and indeed the laws of the road and Highway Code too. But so far, not many!\nThey pay no road tax, no insurance and demand that we give up half of our roads to enable them to have their own dedicated cycle path. Excuse me? In my opinion, cyclists shouldn\u0026rsquo;t be allowed on the road until they have taken a test and passed it (I at least passed a cycling proficiency test before I was allowed out on the road by my parents)Â and you should pay at least third party insurance as well. Then you can consider yourselves \u0026lsquo;allowed\u0026rsquo; on the public roads (ok, I get the irony of that last bit!) .\nIf I am driving my car along the road and a cyclist ignores the lights and gets hit by me, I have no come back on him for the damage to my car, caused by him/her and if s/he is dead, how do I claim back from the corpse? Try it and all you will get is a tabloid style attack from the various red tops about how a callous car driver killed an innocent cyclist and then billed his parents for the damage. Best you get insurance then!\nTo all you human, considerate, and law abiding cyclists out there - well done and thank you. You have my sincereÂ sympathies at the treatment you get from other road users purely because of the insane and selfish behaviour of a number of your peers.\nTo the rest of you, and you know who you are, get stuffed!\nBy the way, two bits of black tarmac in the pub having a chat. A red bit of tramac walked in. The bar went quiet. One of the black bits turned to the other and said \u0026ldquo;do not say anything to him, he will kill you!\u0026rdquo;. The other replied \u0026ldquo;Why?\u0026rdquo;. \u0026ldquo;Because he\u0026rsquo;s a cycle-path!\u0026rdquo; :-D\nAre cyclists really friendly to the environment? I don\u0026rsquo;t think so myself. All that puffing and panting as they cycle slowly up hills must surely be contributing extra CO2 to the atmosphere? (That wasn\u0026rsquo;t a serious question by the way!)\nCheers.\n","description":"","id":215,"section":"posts","tags":null,"title":"Cyclists - I hate them!","uri":"http://localhost:1313/RantsAndRaves/posts/2009/01/cyclists-i-hate-them/"},{"content":"Welcome to my blog.\nDon\u0026rsquo;t expect me to be sensible or politically correct here, it\u0026rsquo;s my space and I\u0026rsquo;ll say what I like! So there! :-)\nOf course, anything I do say will be able to be commented on by you, my faithful reader (singular!), assuming that you care of course.\nOk, seriously, I\u0026rsquo;ll be ranting and raving about all sorts of stuff here. I\u0026rsquo;ll try to categorise them so you can sift the interesting from the less interesting. Who knows, it might be amusing.\n","description":"","id":216,"section":"posts","tags":null,"title":"Hello world!","uri":"http://localhost:1313/RantsAndRaves/posts/2009/01/hello-world/"},{"content":" This blog is simply a way for me to keep useful information, have a rant or rave at times, and so on.\nOk, this is the original about page from my Wordpress blog, created back in 2009. I\u0026rsquo;ve now converted this to Hugo, a static website generator, using Markdown, as I was running out of disc space on my host, so I had to reduce costs somehow after retirement. There\u0026rsquo;s nothing all that special here, but you know, everyone else has a blog these days, why not me? The chances are, I won\u0026rsquo;t have much to say (always a blessing as my wife Alison would agree!) most of the time anyway!\nLet\u0026rsquo;s see how it pans out then \u0026hellip;.\nBy the way, unless I\u0026rsquo;ve changed it, the header picture you can see on this page was taken by a friend of mine - Scott Russell - in Belgium. The scene is one of utter peacefulness.\nIn reality, the pond you see, in the middle of a quiet woodland glade, is actually a First World War bomb crater. The scene of utter peacefulness has been created from an utter nightmare. Ironic or what?\nCheers,\nNorm.\n","description":"About Rants and Raves, the Blog!","id":217,"section":"","tags":null,"title":"About","uri":"http://localhost:1313/RantsAndRaves/about/"}]